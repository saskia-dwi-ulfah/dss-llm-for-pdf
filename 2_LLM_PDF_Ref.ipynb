{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSS May 2024: Efficient Information Extraction: Q&A and Summarization over PDF Documents using LLM\n",
    "\n",
    "* Instruktur: [Saskia Dwi Ulfah](https://www.linkedin.com/in/saskia-dwi-ulfah/).\n",
    "* Last updated: May 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📕 Dokumen yang disimpan dalam format **Portable Document Format (PDF)** merupakan salah satu bentuk dokumen yang sering digunakan untuk bertukar informasi melalui internet dan perangkat digital seperti handphone, laptop, dan komputer. PDF juga dimanfaatkan di berbagai sektor. Di bidang pendidikan, mahasiswa dapat mengakses jurnal penelitian dengan format PDF melalui website seperti Elsevier dan IEEE. Di bidang finansial, perusahaan menampilkan laporan keuangan tahunan dalam format PDF pada website perusahaan.\n",
    "\n",
    "\n",
    "⛳ Format PDF merupakan format yang universal. Artinya, dokumen yang disimpan dalam format PDF dapat diakses secara mudah pada perangkat yang berbeda. Selain itu, format PDF lebih disukai karena tampilan dokumen yang lebih rapi dibandingkan format dokumen lainnya. PDF juga men-support dokumen yang terdiri lebih dari satu halaman. Hal ini memungkinkan pengguna untuk menuliskan informasi yang lengkap dan komprehensif dalam sebuah dokumen PDF. Akan tetapi, hal ini membuat **dokumen PDF cenderung tebal dan kompleks** sehingga menyebabkan pembaca **kesulitan untuk menemukan informasi yang spesifik**.\n",
    "\n",
    "💡 Dengan perkembangan teknologi artificial intelligence  (AI) dan machine learning (ML), kita dapat menggunakan **Large Language Model (LLM)** untuk pencarian informasi pada dokumen PDF. Pada workshow ini, Anda akan belajar bagaimana kita dapat memperluas kemampuan LLM untuk pencarian informasi secara efisien dari dokumen PDF. Dimulai dengan dokumen PDF biasa, Anda akan belajar cara memproses dokumen ini dan menyajikannya sebagai konteks tambahanuntuk LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎯 Setelah menyelesaikan workshop ini, Anda diharapkan dapat:\n",
    "\n",
    "* Memahami konsep dasar dari LLM.\n",
    "* Mengimplementasikan penggunaan LLM dengan framework LangChain.\n",
    "* Memahami workflow yang digunakan dalam menyediakan additional context untuk LLM.\n",
    "* Mengembangkan skill di bidang AI dan data science dengan menguasai teknik information retrieval dari dokumen PDF menggunakan LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Syllabus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Python Programming Basics** \n",
    "    - Introduction to Python for Data Science.\n",
    "    - Working with Python Environment.\n",
    "    - Working with Notebook.\n",
    "    - Python Fundamental Data Types and Data Structures.\n",
    "    - Understanding Looping Concept in Python.\n",
    "    - Understanding The Creation of Python Function.\n",
    "    - Understanding The Usage of Python Libraries.\n",
    "* **The Fundamentals of LLM**\n",
    "    - The Concept of Generative AI.\n",
    "    - LLM as Generative AI.\n",
    "    - Transformer Architecture in a Nutshell\n",
    "    - LLM Capability, Limitation, and Consideration\n",
    "* **Introduction to LangChain**\n",
    "    - The Big Picture of LangChain Concept and Component\n",
    "    - API Concept and Setting for LangChain Usage\n",
    "    - Demonstration of LLM Usage with LangChain\n",
    "* **Case Study: Q&A and Summarization for PDF Document**\n",
    "    - The Concept of RAG (Retrieval Augmented Generation)\n",
    "    - Loading PDF Documents using LangChain\n",
    "    - The Concept of Embedding for PDF Documents\n",
    "    - Storing The Embedding using a Vector Database\n",
    "    - Prompt Creation for Q&A and Summarization Cases\n",
    "    - Employing LLM for Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fundamentals of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concept of Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/gen_ai_hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚀 **Apa itu Generative AI?**\n",
    "\n",
    "**Generative AI** merupakan bagian dari artificial intelligence yang bertujuan untuk **menghasilkan (generate) data**, seperti teks, gambar, video, dan musik. Untuk dapat menghasilkan data, generative AI dilatih/belajar dari data yang jumlahnya sangat besar. Proses belajar ini sering disebut training. Generative AI akan menghasilkan data berdasarkan pola yang sudah dipelari selama proses training.\n",
    "\n",
    "🚀 **Jenis-Jenis Generative AI dan Aplikasinya**\n",
    "\n",
    "* **Generative Text Model**\n",
    "    * Input: teks.\n",
    "    * Output → teks (text-to-text generator).\n",
    "      * Contoh: translation, **summarization**, **question-answering**, grammar correction.\n",
    "    * Output → image (text-to-image generator).\n",
    "      * Contoh: image generation, video generation.\n",
    "    * Output → audio (text-to-speech generator).\n",
    "      * Contoh: music generation.\n",
    "* **Generative Image Model**\n",
    "    * Input: gambar.\n",
    "    * Output: teks (image-to-text generator).\n",
    "      * Contoh: image captioning, visual question-answering, image search.\n",
    "    * Output: gambar (image-to-image generator).\n",
    "      * Contoh: image completion.\n",
    "    * Output: video (image-to-video generator).\n",
    "      * Contoh: animation generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM as Generative AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💬 **LLM** merupakan generative AI yang telah dilatih menggunakan data teks dalam jumlah yang sangat besar. LLM mampu untuk memahami dan menghasilkan teks dengan gaya bahasa yang mirip dengan manusia. Selain menggambarkan besarnya data yang digunakan untuk melatih LLM, istilah \"large\" mengacu kepada tingkat kompleksitas, ukuran, dan banyaknya parameter pada LLM.\n",
    "\n",
    "> ℹ️ Sebagai contoh, GPT-4, varian yang paling baru dari LLM yang dirilis oleh OpenAI, memiliki 1,3 triliun parameter. Sumber data yang digunakan untuk melatih model ini meliputi buku, website, jurnal ilmiah, artikel, posting-an media sosial, dan repositori kode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Platforms\n",
    "\n",
    "💬 Beberapa contoh platform LLM yang cukup populer: \n",
    "\n",
    "* 🤖**GPT**🤖\n",
    "\n",
    "    🤖 **Generative Pre-Trained Transformer (GPT)** merupakan large language model yang dikembangkan oleh Open AI. Sama seperti model generative AI lainnya, GPT di-training dengan data teks dalam jumlah yang besar. GPT dirancang untuk memahami dan menghasilkan teks dengan cara yang mirip dengan manusia, sehingga dapat digunakan dalam berbagai aplikasi seperti chatbot, language translation, dan content creation. Baru-baru ini, OpenAI meluncurkan varian terbaru dari GPT-4, yakni GPT-4o.\n",
    "\n",
    "    Segmentasi produk GPT dapat dibedakan menjadi 2: chat-based platform dan Application Programming Interface (API)-based platform. Produk GPT berbasis chat dikenal sebagai [**ChatGPT**](https://chatgpt.com/). Semenjak dirilis pada November 2022 dengan versi GPT-2, saat ini ChatGPT telah merilis varian GPT yang lebih powerful: ChatGPT dengan GPT-3.5 dan ChatGPT dengan GPT-4. Untuk ChatGPT-3.5, pengguna dapat mengakses secara free. Untuk mendaptkan fitur yang lebih advanced pada ChatGPT dengan GPT-4, pengguna perlu melakukan subscription terlebih dahulu.\n",
    "\n",
    "    Varian produk kedua adalah produk berbasis [**Application Programming Interface (API)**](https://platform.openai.com/). API memungkinkan pengguna untuk meng-embbed kemampuan GPT pada aplikasi yang dapat disesuakaikan dengan kebutuhan mereka. Penggunaan API ini sangat fleksibel karena dapat diintegrasikan dengan berbagai platform dan sistem yang sudah ada. \n",
    "\n",
    "* ♊**Gemini**♊\n",
    "\n",
    "    ♊ **Gemini** merupakan model generative AI yang dikembangkan oleh Google. Gemini dilatih menggunakan data teks, gambar, suara, dan video secara bersamaan dalam jumlah yang sangat besar. Hal ini membuat Gemini mampu untuk menerima berbagai macam input. Kelebihan ini menjadikan Gemini unggul dari segi generalisasinya terhadap data baru. Saat ini, versi terbaru dari Gemini adalah Gemini 1.5. \n",
    "\n",
    "    Sama seperti GPT, secara umum, fitur-fitur pada Gemini dikemas dalam 2 bentuk produk. Produk yang pertama merupakan [aplikasi yang berbasis chat bernama **Gemini app (sebelumnya Bard)**](https://gemini.google.com/app). Penggunaan aplikasi ini ditujukan untuk siapa saja secara umum tanpa perlu menuliskan kode tertentu. Produk lainnya merupakan [**API**](https://ai.google.dev/gemini-api) yang dapat digunakan pada use case yang lebih custom. Penggunaan API ini lebih ditujukan untuk pengguna dengan aspek pekerjaan teknis yang lebih tinggi, seperti software developer.\n",
    "\n",
    "* 🤗**HuggingFace**🤗\n",
    "\n",
    "    🤗**HuggingFace** platform yang dikenal luas dalam komunitas machine learning dan AI, khususnya dalam bidang Natural Language Processing (NLP). Platform ini menyediakan model-model pre-trained yang dapat digunakan untuk berbagai task yang berkaitan dengan NLP, seperti language modeling dan sentiment analysis.\n",
    "\n",
    "    Beberapa contoh LLM open source yang dapat diakses melalui HuggingFace:\n",
    "    \n",
    "    ---add the list---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt pada Language Model\n",
    "\n",
    "Untuk menghasilkan output yang sesuai, LLM memerlukan **prompt**. Prompt dapat dianalogikan sebagai sebuah petunjuk untuk LLM. Secara eksplisit, prompt merujuk kepada instruksi atau detail yang diberikan kepada LLM terkait output seperti apa yang diharapkan. Prompt juga bisa berupa konteks atau latar belakang dari topik yang ingin diketahui.\n",
    "\n",
    "💡 Beberapa tips awal untuk merancang sebuah prompt:\n",
    "\n",
    "* **Buat Prompt yang Sederhana**\n",
    "\n",
    "    Pembuatan prompt merupakan proses iteratif. Selalu mulai dengan sebuah prompt sederhana. Jika masih belum mendapatkan output yang diinginkan dari LLM, kita bisa me-refine prompt yang sudah dibuat sebelumnya.\n",
    "\n",
    "    Hindari pembuatan prompt untuk task yang kompleks. Kita dapat mem-break down task yang kompleks tersebut menjadi subtask yang lebih kecil dan membuat prompt yang berbeda untuk masing-masing subtask.\n",
    "\n",
    "* **Berikan Perintah yang Jelas: What to Do and What Not to Do?**\n",
    "\n",
    "    Saat membuat sebuah prompt, pastikan prompt memiliki instruksi yang jelas. Misalnya, kita menginginkan ringkasan dari sebuah kalimat, maka prompt yang efektif mengandung instruksi untuk membuat ringkasan tersebut. Kita juga dapat mendefinisikan detail apa yang tidak perlu dimasukkan dalam ringkasan yang dihasilkan LLM.\n",
    "\n",
    "\n",
    "* **Be Speficic and Detail**\n",
    "\n",
    "    Selain memberikan perintah yang jelas, kita dapat membuat prompt menjadi lebih spesifik dan detail. Misalnya, kita dapat mendefinisikan detail seperti gaya bahasa dan panjang output yang diharapkan.\n",
    "\n",
    "Contoh prompt:\n",
    "\n",
    "```\n",
    "\n",
    "Summarize the following article into approximately 150 words, highlighting the main points and key information discussed. Do not include minor details, personal opinions, or repetitive information.\n",
    "\n",
    "```\n",
    "\n",
    "> 🔑 Kunci prompt yang efektif: sederhana, jelas, dan spesifik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤖 Arsitektur **transformer** merupakan salah satu fondasi dibalik keandalan LLM. Arsitektur ini pertama kali diperkenalkan pada paper yang berjudul [Attention is All You Need](https://arxiv.org/abs/1706.03762) oleh Vaswani dkk. pada tahun 2017. \n",
    "\n",
    "Kemunculan arsitektur transformer bertolak dari keterbatasan dari arsitektur yang sebelumnya sering digunakan untuk memodelkan data berupa sequence (contoh: bahasa), seperti Recurrent Neural Network (RNN) dan Long Short-Term Memory (LSTM). Keterbatasan ini di antaranya: ketidakefisienan dari segi daya komputasi dan waktu serta ketersediaan memori yang terbatas untuk sequence yang cukup panjang. Transformer mengatasi keterbatasan ini dengan fitur spesial yang disebut dengan **self-attention mechanism**. \n",
    "\n",
    "Secara sederhana, Transformer terdiri dari 2 komponen utama:\n",
    "\n",
    "![](assets/transformer_simplified_3.png)\n",
    "\n",
    "\n",
    "▶️ **Encoder**\n",
    "\n",
    "**Encoder** merupakan bagian dari transformer yang bertujuan untuk memproses input sequence ke dalam representasi tertentu. Representasi ini merupakan bentuk numerik dari input sequence berikut dengan informasi urutannya. **Encoder** terdiri dari beberapa lapisan yang disebut **encoder layer**. Setiap encoder layer memiliki dua bagian utama:\n",
    "\n",
    "- **Multi-Head Self-Attention Mechanism**\n",
    "\n",
    "    Bagian ini membantu model untuk memperhatikan berbagai kata dalam input sekaligus, sehingga memahami hubungan antar kata yang berbeda.\n",
    "\n",
    "- **Feed-Forward Neural Network**\n",
    "    \n",
    "    Setelah bagian self-attention, representasi dari input sequence diproses oleh neural network untuk memperkuat pemahaman model tentang konteks.\n",
    "\n",
    "▶️ **Decoder**\n",
    "\n",
    "**Decoder** merupakan bagian dari transformer yang bertujuan untuk menghasilkan output. Output ini mempertimbangkan input dari encoder. **Decoder** juga terdiri dari beberapa lapisan yang disebut **decoder layer**. Setiap decoder layer memiliki tiga bagian utama:\n",
    "\n",
    "- **Masked Multi-Head Self-Attention Mechanism**\n",
    "\n",
    "    Bagian ini memastikan model hanya memperhatikan kata-kata yang sudah dihasilkan sebelumnya, bukan kata yang belum dihasilkan. Ini penting untuk menjaga urutan kata yang dihasilkan.\n",
    "- **Multi-Head Attention Mechanism**\n",
    "\n",
    "    Bagian ini membantu decoder memperhatikan informasi dari encoder, sehingga output yang dihasilkan mempertimbangkan seluruh input.\n",
    "- **Feed-Forward Neural Network**\n",
    "\n",
    "    Setelah melalui kedua bagian attention, representasi dari sequence diproses oleh neural network untuk memperkuat pemahaman model tentang konteks.\n",
    "\n",
    "Berikut merupakan ilustrasi sederhana bagaimana transformer bekerja.\n",
    "\n",
    "![](assets/transformer_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Capability, Limitation, and Consideration\n",
    "\n",
    "Saat ini, LLM banyak dipilih karena keandalan dan kemampuannya untuk memahami konteks yang rumit. Selain itu, LLM juga mampu untuk menghasilkan teks dan narasi yang hampir menyamai produk bahasa yang ditulis oleh manusia. \n",
    "\n",
    "Berikut ini beberapa contoh task yang dapat diaplikasikan oleh LLM.\n",
    "\n",
    "* **Text Generation**: menghasilkan narasi dengan gaya bahasa yang hampir menyamai kemampuan manusia.\n",
    "* **Translation**: menerjemahkan teks ke dalam bahasa yang berbeda secara akurat.\n",
    "* **Summarization**: membuat ringkasan singkat dari teks yang panjang.\n",
    "* **Question-Answering**: menjawab sebuah pertanyaan berdasarkan konteks yang diberikan.\n",
    "\n",
    "Akan tetapi, perlu diperhatikan bahwa saat ini LLM masih menjadi topik riset yang aktif. Para peneliti masih mengembangkan metode yang lebih advanced untuk mengurangi beberapa limitasi dari LLM, seperti:\n",
    "\n",
    "* **Misleading Output** \n",
    "    \n",
    "    Sebagai produk AI yang sangat powerful, output yang dihasilkan oleh LLM dapat terlihat sangat meyakinkan. Akan tetapi, sebagai pengguna, kita perlu bijak dalam mengonsumsi konten yang dibuat oleh AI. Secara aktif, kita perlu melakukan verifikasi fakta melalui sumber-sumber yang kredibel.\n",
    "\n",
    "* **Technical Limitation**\n",
    "\n",
    "    Di balik kapabilitas LLM dalam task yang melibatkan natural language, masih terdapat concern terkait limitasi teknis. Terdapat situasi di mana AI tidak dapat memahami konteks secara mendalam atau menghasilkan respons yang sesuai dengan harapan pengguna. Selain itu, LLM memerlukan sumber daya komputasi yang sangat tinggi untuk berfungsi secara optimal, yang dapat menjadi hambatan bagi pengguna dengan keterbatasan infrastruktur teknologi.\n",
    "\n",
    "* **Laws and Rights-Related Issue**\n",
    "\n",
    "    Sebagai teknologi yang terus berkembang, penggunaan LLM juga dihadapkan pada isu-isu hukum dan hak. Misalnya, hak cipta, privasi, dan perlindungan data adalah aspek-aspek penting yang perlu dipertimbangkan saat menggunakan AI. \n",
    "    \n",
    "    Pengguna harus memastikan bahwa mereka mematuhi semua regulasi yang relevan dan menghormati hak-hak individu dan entitas lain dalam penggunaan dan penyebaran konten yang dihasilkan oleh AI.\n",
    "\n",
    "Mempertimbangkan kelebihan dan limitasi yang ada, sebagai end-user, kita perlu cermat, selektif, dan aware dalam mempergunakan LLM. Kemampuan LLM yang powerful akan semakin maksimal selama dibersamai dengan konsiderasi berikut:\n",
    "\n",
    "* **Tidak Menganggap Output LLM sebagai Final Result** \n",
    "    \n",
    "    Output dari LLM tidak dijadikan sebagai hasil akhir, melainkan hanya sebagai pendukung tersier. Kita perlu tetap melakukan verifikasi dan cross-check dengan sumber lain yang terpercaya untuk memastikan akurasi dan relevansi informasi.\n",
    "\n",
    "* **Keputusan Kompleks dan Terkait Makhluk Hidup**\n",
    "    \n",
    "    Keputusan yang diambil dari output LLM tidak digunakan dalam pertimbangan yang kompleks dan menyangkut keperluan yang berkaitan dengan makhluk hidup, seperti keputusan medis, hukum, atau keuangan, tanpa konsultasi dengan ahli yang kompeten di bidang tersebut.\n",
    "\n",
    "* **Evaluasi Kritis dan Penyesuaian Konteks**\n",
    "    \n",
    "    Selalu melakukan evaluasi kritis terhadap output yang dihasilkan oleh LLM, termasuk memahami konteks dan bias yang mungkin ada dalam data pelatihannya. Penyesuaian dan interpretasi yang bijak diperlukan untuk memastikan bahwa informasi tersebut sesuai dengan konteks yang dibutuhkan.\n",
    "\n",
    "* **Keamanan dan Privasi Data**\n",
    "\n",
    "    Memperhatikan aspek keamanan dan privasi data saat menggunakan LLM. Jangan memasukkan informasi pribadi atau sensitif yang dapat disalahgunakan, dan pastikan bahwa penggunaan LLM mematuhi kebijakan privasi dan regulasi yang berlaku.\n",
    "\n",
    "> 📌 More info: [Responsible AI](https://ai.google/responsibility/responsible-ai-practices/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain🦜🔗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Concept and Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🦜🔗**LangChain** merupakan sebuah framework yang dapat digunakan untuk membuat aplikasi berbasis LLM. Dengan LangChain, pengguna dapat mengakses berbagai jenis LLM yang disediakan oleh provider seperti OpenAI, Google, dan HuggingFace. Selain itu, framework ini juga memungkinkan kita untuk membuat aplikasi custom dengan memanfaatkan kemampuan LLM. Fitur- fitur ini menjadikan LangChain sebagai package yang cukup powerful. \n",
    "\n",
    "> 📌[LangChain Official Documentation](https://python.langchain.com/v0.1/docs/get_started/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 Seperti namanya, ide dasar dari LangChain adalah men-chaining beberapa komponen menjadi satu kesatuan untuk menjalankan fungsi tertentu. Dengan menggabungkan berbagai komponen ini, kita bisa membuat alur kerja yang kompleks dan dinamis.\n",
    "\n",
    "Secara umum, berikut adalah pengelompokan komponen LangChain berdasarkan fungsinya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1️⃣ **Model I/O**\n",
    "\n",
    "* Mengatur input dan output dari sebuah LLM.\n",
    "* Komponen:\n",
    "    * **Prompt**: memberikan instruksi terkait output seperti apa yang harus dihasilkan.\n",
    "    * **LLM dan Chat Model**: memberikan output berdasarkan prompt.\n",
    "\n",
    "2️⃣ **Retrievers**\n",
    "\n",
    "* Mengambil data yang relevan untuk digunakan dalam LLM (biasanya melibatkan dokumen eksternal).\n",
    "* Komponen:\n",
    "    * **Document Loader**: untuk memuat dokumen eksternal.\n",
    "    * **Text Splitter**: untuk pemrosesan dokumen.\n",
    "    * **Embedding Model**: untuk merepresentasikan dokumen dalam bentuk numerik (embedding).\n",
    "    * **Vectorstore**: untuk menyimpan embedding\n",
    "    * **Retrievers**: mencari dan mengembalikan data yang paling relevan dari vectorstores berdasarkan query pengguna.\n",
    "\n",
    "3️⃣ **Composition**\n",
    "\n",
    "* Gabungan dari komponen basic di atas.\n",
    "* Komponen: tools, agents, chains.\n",
    "\n",
    "4️⃣ **Additional**\n",
    "\n",
    "* Komponen: memory, callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Concept and Setting for LangChain Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/api_analogy.png)\n",
    "\n",
    "Provider LLM seperti OpenAI dan Google menyediakan fungsionalitas LLM mereka dalam bentuk **Application Programming Interface (API)**. API merupakan sebuah protokol atau program yang memungkinkan beberapa komputer untuk saling berkomunikasi.\n",
    "\n",
    "> 💡 Apabila dianalogikan, API merupakan pelayan di sebuah restoran. Kita sebagai pelanggan memesan makanan melalui pelayan. Selanjutnya, pelayan akan menyampaikan pesanan kita kepada chef (provider LLM) di dapur dan chef akan membuatkan pesanan tersebut. Pesanan yang sudah jadi akan diantarkan kembali melalui pelayan.\n",
    "\n",
    "Untuk dapat berinteraksi dengan API dari provider LLM, kita memerlukan sebuah **API key** (ID untuk pemesanan). API key ini digunakan setiap kali kita berinteraksi dengan LLM yang disediakan oleh provider. Pada workshop ini, kita akan menggunakan **Gemini sebagai LLM**.\n",
    "\n",
    "> 🔑 Find your [Gemini API key here](https://ai.google.dev/gemini-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena API key bersifat rahasia, API key jarang dimasukkan langsung ke dalam kode program. Salah satu cara terbaik untuk memuat API key secara otomatis adalah dengan membuat file `.env` dan menggunakan fungsi `load_dotenv()` dari package `dotenv` untuk memanggil file tersebut. \n",
    "\n",
    "Berikut adalah contoh isi dari file `.env`.\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY = \"your_GOOGLE_API_key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk meng-import fungsi load_dotenv() dari package dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file .env secara otomatis\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of LLM Usage with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sejauh ini, kita sudah mengetahui beberapa dasar terkait LLM, konsep dasar LangChain, dan komponen dari LangChain. Selanjutnya, mari kita lihat bagaimana implementasi sederhana penggunaan LLM dengan LangChain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Your First Call to Gemini ♊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan pertama ini, kita akan:\n",
    "\n",
    "* Membuat objek LLM.\n",
    "* Mencoba varian Gemini yang tersedia.\n",
    "* Menanyakan pertanyaan dengan method `.invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat objek LLM dengan beberapa varian yang ada\n",
    "bison = GoogleGenerativeAI(model=\"models/text-bison-001\")\n",
    "gemini_10_pro = GoogleGenerativeAI(model=\"models/gemini-pro\") # also refer to gemini-1.0-pro\n",
    "gemini_15_pro = GoogleGenerativeAI(model=\"models/gemini-1.5-pro-latest\") # beware of 2 RPM limit\n",
    "# gemini_15_flash = GoogleGenerativeAI(model=\"gemini-1.5-flash\") --> still not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bison.invoke('Who is the president of Indonesia?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_10_pro.invoke('Who is the president of Indonesia?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_15_pro.invoke('Who is the president of Indonesia?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Add Prompt to Boost Gemini's Response 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan kedua ini, kita akan menambahkan prompt untuk membuat implementasi yang lebih dinamis dan custom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita akan membuat sebuah template untuk menerjemahkan sebuah kalimat ke dalam bahasa Inggris dengan gaya bahasa yang informal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template = '''\n",
    "                        Translate the following sentence into informal English:\n",
    "                        {sentence}\n",
    "                        ''',\n",
    "                        input_variables = [\"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita menggunakan operator `|` untuk membuat sebuah chain. Chain tersebut terbentuk dari komponen prompt dan LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain =  prompt | gemini_10_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terakhir, kita akan mencoba untuk memberikan sebuah pertanyaan melalui method `.invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke({\n",
    "    'sentence' : 'Saya akan kembali 10 menit lagi'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Prompt + Gemini via `LLMChain()` ⛓️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan sebelumnya, kita menggunakan dictionary `{ }` pada method `.invoke()` untuk memberikan pertanyaan kepada LLM. Pendekatan tersebut masih kurang fleksibel di mana pengguna masih perlu secara manual mendefinisikan dictionary yang berisi pertanyaan.\n",
    "\n",
    "Pada eksperimen ketiga ini, kita akan mengenal `LLMChain()`. `LLMChain()` memudahkan kita untuk bertanya dengan LLM dengan hanya mendifinisikan:\n",
    "\n",
    "* LLM apa yang akan digunakan.\n",
    "* Prompt seperti apa yang diinginkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm = gemini_10_pro,\n",
    "                     prompt = prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan kode di atas, kita tidak perlu mendefinisikan pertanyaan dengan dictionary `{ }`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke('Di mana terakhir kali kita bertemu?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Q&A and Summarization for PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔 Coba tanyakan pertanyaan berikut kepada LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bison.invoke('Who is the winner of presidential election in Indonesia for 2024-2029?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_10_pro.invoke('Siapa pemenang Pemilu 2024 di Indonesia?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_15_pro.invoke('Siapa pemenang Pemilu 2024 di Indonesia?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meskipun memiliki kemampuan yang powerful untuk mengerjakan task yang berkaitan dengan bahasa, LLM memiliki **knowledge cutoff**. Knowledge cutoff merupakan batas pengetahuan yang dimiliki oleh LLM karena resource untuk men-training LLM berhenti pada titik waktu tertentu. \n",
    "\n",
    "> ℹ️ Untuk Gemini, knowledge cutoff-nya adalah awal 2023.\n",
    "\n",
    "Dengan demikian, pengetahuan LLM tentang peristiwa dan informasi setelah knowledge cutoff tersebut mungkin terbatas atau tidak ada sama sekali.\n",
    "\n",
    "Untuk mengatasi permasalahan tersebut, kita dapat menggunakan **Retrieval Augmented Generation** (RAG). RAG bekerja dengan cara menyediakan konteks tambahan untuk LLM, seperti dokumen, website, dan database. Dengan RAG, LLM dapat mengakses dan memproses data terbaru, sehingga dapat memberikan jawaban yang lebih akurat dan terkini. Selain itu, pendekatan ini juga memudahkan kita untuk memanfaatkan LLM pada resource dan use case yang lebih spesifik.\n",
    "\n",
    "Pada workshop ini, kita akan mencoba untuk **menggunakan dokumen PDF sebagai konteks tambahan untuk LLM**. Dokumen ini merupakan sebuah paper yang berjudul [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223) dengan tebal 124 halaman. Paper ini menyediakan review yang komprehensif terkait LLM. Paper yang sama juga dapat Anda temui pada folder `data_input` dengan nama file `llm_survey.pdf`.\n",
    "\n",
    "Selanjutnya, kita akan **membuat chain** dengan LangChain yang memungkinkan kita untuk **bertanya seputar konten paper tersebut**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "Untuk membuat sebuah implementasi Q&A dokumen PDF menggunakan LangChain, kita akan mengikuti workflow berikut ini.\n",
    "\n",
    "![](assets/rag_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita perlu memuat dokumen yang menjadi konteks tambahan bagi LLM. Dalam hal ini, kita menggunakan `PyPDFLoader()` untuk memuat dokumen PDF. Selain PDF, LangChain menyediakan fungsionalitas untuk memuat dokumen dengan ekstensi lain, seperti CSV dan JSON.\n",
    "\n",
    "> 📌 [LangChain Document Loaders](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk loading file PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader('data_input/llm_survey.pdf')\n",
    "pdf_data = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan kode di atas, dokumen PDF akan di-load sebagai list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List ini akan memiliki elemen sebanyak halaman dokumen PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setiap elemen pada list berisikan konten pada halaman tertentu. Misalnya, kita ingin melihat konten pada halaman ke-5 (indeks ke-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_data[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita akan melakukan proses splitting. Proses ini bertujuan untuk membuat chunk yang lebih kecil dari konten yang sudah diekstrak sebelumnya. Chunk-chunk yang lebih kecil akan lebih mudah di-maintain, disimpan, dan diproses oleh LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk split konten file PDF menjadi chunk yang lebih kecil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada proses splitting ini, kita menggunakan `RecursiveCharacterTextSplitter()`. Dengan `RecursiveCharacterTextSplitter()`, proses splitting dilakukan dengan tetap menjaga bagian-bagian teks yang terkait tetap berdekatan satu sama lain. Merujuk pada [dokumentasi LangChain](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/), teknik splitting dengan `RecursiveCharacterTextSplitter()` adalah teknik yang direkomendasikan untuk mulai melakukan splitting dari sebuah teks yang cukup besar.\n",
    "\n",
    "> 📌 [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)\n",
    "\n",
    "Pada kode di bawah ini, kita mendefinisikan output dari proses splitting:\n",
    "\n",
    "* Setiap chunk akan memiliki maksimal sebanyak 1000 karakter.\n",
    "* Antara satu chunk dengan chunk yang lain memiliki kesamaan sebanyak 450 karakter. Hal ini didefinisikan untuk mempertahankan konteks antara satu chunk dengan chunk yang lain. \n",
    "* Proses splitting dilakukan berdasarkan daftar karakter yang kita definisikan pada parameter `separators`.\n",
    "\n",
    "Selanjutnya, kita memasukkan list `pdf_data` sebagai input untuk method `.split_documents()`. Method ini akan membuat chunk yang lebih kecil dari `pdf_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 450,\n",
    "    separators=[\n",
    "                \"\\n\\n\",\n",
    "                \"\\n\",\n",
    "                \" \",\n",
    "                \".\",\n",
    "                \",\",\n",
    "                \"\\u200b\",  # zero-width space\n",
    "                \"\\uff0c\",  # full-width comma\n",
    "                \"\\u3001\",  # ideographic comma\n",
    "                \"\\uff0e\",  # full-width full stop\n",
    "                \"\\u3002\",  # ideographic full stop\n",
    "                \"\",\n",
    "            ])\n",
    "\n",
    "splits = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di akhir proses ini, kita mendapatkan 1316 chunk hasil splitting dari 124 elemen pada `pdf_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di balik layar, operasi pada komputer merupakan operasi yang melibatkan angka. Agar teks yang kita miliki dapat dimengerti oleh komputer, kita perlu membuat representasi numerik dari teks tersebut. Proses ini dibut dengan **embedding**.\n",
    "\n",
    "LangChain menyediakan banyak pilihan embedding, tergantung dari LLM yang akan kita gunakan:\n",
    "* `GoogleGenerativeEmbeddings()` untuk model LLM yang dikembangkan oleh Google.\n",
    "* `OpenAIEmbeddings()` untuk model LLM dari OpenAI.\n",
    "* `HuggingFaceEmbeddings()` untuk model LLM dari HuggingFace.\n",
    "\n",
    "> 📌 [LangChain Embedding Models](https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/)\n",
    "\n",
    "Representasi numerik dari teks ini selanjutnya disimpan di dalam **vector database**. \n",
    "> ✨ Sederhananya, kita menyimpan konten file PDF di database dalam bentuk angka.\n",
    "\n",
    "Vector database yang akan kita gunakan adalah **Chroma**. Chroma merupakan vector database yang cukup powerful untuk menyimpan unstructured data, seperti konten pada file PDF. Chroma juga dilengkapi dengan algoritma yang efisien dan akurat untuk pencarian informasi dari embedding yang disimpan. Selain itu, Chroma juga memungkinkan kita untuk menyimpan objek database sehingga dapat digunakan kembali untuk keperluan yang akan datang.\n",
    "\n",
    "> 📌 [LangChain Vector Stores](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk membuat embedding\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # embedding Google Generative AI\n",
    "\n",
    "# untuk menyimpan embedding\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode dibawah ini, kita mendefinisikan sebuah fungsi untuk:\n",
    "\n",
    "* Mengubah dokumen menjadi embedding. \n",
    "* Menyimpan hasil embedding ke dalam Chroma.\n",
    "* Menyimpan file database ke suatu folder agar dapat digunakan kembali.\n",
    "\n",
    "Penjelasan parameter:\n",
    "\n",
    "* `documents `: dokumen yang akan diubah menjadi embedding.\n",
    "* `embbeding`: fungsi yang digunakan untuk membuat embedding, misalnya `GoogleGenerativeAIEmbeddings()` jika menggunakan Gemini sebagai LLM.\n",
    "* `persist_directory`: folder untuk menyimpan vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore_folder(documents, embedding, persist_directory):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents = documents, \n",
    "        embedding = embedding,\n",
    "        persist_directory = persist_directory\n",
    "    )\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_gemini = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
    "#     persist_directory = 'data_input/chroma_gemini'\n",
    "# )\n",
    "\n",
    "# time spent: 40.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses mengubah dokumen menjadi embedding dan menyimpannya ke dalam database dapat berjalan lama karena dapat dipengaruhi oleh berbagai faktor, mulai dari besarnya dokumen sampai dengan koneksi internet. Oleh karena itu, pada workshop ini, proses pembuatan vector database sudah dilakukan terlebih dahulu. Penyimpanan vector database ini terdapat pada folder `data_input`. \n",
    "\n",
    "Untuk me-load kembali vector database dari folder yang sudah ada, Anda dapat menjalankan kode di bawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memanggil embedding dari directory yang sudah disimpan\n",
    "vec_gemini = Chroma(persist_directory = 'data_input/chroma_gemini',\n",
    "                    embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita telah membuat sebuah vector database yang menjadi sumber bagi LLM untuk mencari informasi. Selanjutnya, kita akan membuat sebuah chain yang memungkinkan kita untuk bertanya kepada LLM. LLM akan memberikan jawaban berdasarkan informasi yang terdapat pada vector database.\n",
    "\n",
    "> 🤨 **Bagaimanaca LLM memberikan jawaban yang sesuai?** Pada saat kita memberikan pertanyaan (query) kepada LLM, pertanyaan tersebut akan diubah menjadi representasi numerik (vector). Representasi numerik ini akan digunakan untuk mencari jawaban yang sesuai berdasarkan informasi pada vector database. Proses ini melibatkan perhitungan vector similarity antara vector query dan vector informasi yang terdapat dalam vector database. Informasi dengan tingkat similarity yang tinggi akan di-return sebagai jawaban."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/rag_workflow2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# untuk mempersiapkan prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# untuk memasukkan pertanyaan\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# untuk menampilkan output yang diinginkan\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# untuk menampilkan output secara rapi\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = \"\"\"\n",
    "    You are the great assistant in understanding additional context\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    Use the minimum of three sentences to answer the question. \n",
    "    Try your best to answer as complete as possible with easy style of English.\n",
    "    Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(qa_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(retriever, llm):\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode di atas, kita membuat sebuah fungsi untuk membuat chain untuk Q&A. Chain untuk Q&A di atas terdiri atas beberapa komponen:\n",
    "\n",
    "*  `custom_rag_prompt` yang merupakan prompt untuk mengarahkan output dari LLM.\n",
    "* `llm` yang akan berinteraksi dengan pengguna.\n",
    "* `StrOutputParser()` yang akan menangkap output dari LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan pada saat kita mendefinisikan template prompt, terdapat 2 placeholder: `{context}` dan `{question}`.\n",
    "\n",
    "* Value untuk `{context}` berasal dari vector database.\n",
    "* Value untuk `{question}` berasal dari pertanyaan pengguna pada saat menjalankan chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_chain = create_qa_chain(\n",
    "    retriever = vec_gemini.as_retriever(),\n",
    "    llm = ChatGoogleGenerativeAI(model = 'gemini-pro')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang, kita sudah membuat chain untuk Q&A. Mari kita tes chain tersebut dengan memberi beberapa pertanyaan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        gemini_chain.invoke('Describe about llaMa model family'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        gemini_chain.invoke('How is the technical evolution of GPT-series model?'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebelumnya, kita mendefinisikan sebuah chain untuk Q&A menggunakan `|`. `|` merupakan sintaks LangChain Expression Language (LCEL) yang menjadi dasar dalam pendefinisian chain yang lebih kompleks.\n",
    "\n",
    "> 📌 [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/)\n",
    "\n",
    "Selain digunakan untuk membuat sistem Q&A, LLM juga sering dimanfaatkan untuk task summarization. \n",
    "\n",
    "Pada use case kali ini, kita akan mendefinisikan chain dengan LCEL untuk melakukan summarization. Dokumen yang akan kita buat summary-nya merupakan paper yang berjudul [Transformers in Time Series: A Survey](https://arxiv.org/abs/2202.07125). Pembahasan paper ini adalah seputar penggunaan arsitektur transformer untuk data time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita akan me-load file yang akan kita ringkas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_ts_loader = PyPDFLoader('data_input/transformer_ts.pdf')\n",
    "trans_ts_content = trans_ts_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita mendefinisikan prompt untuk membuat sebuah summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "    You are the great assistant in summarizing the following passage.\n",
    "\n",
    "    Provide a summary of the following passage. \n",
    "    The summary should be general and no longer than five sentences.\n",
    "\n",
    "    Passage:\n",
    "    ```{text}```\n",
    "    \n",
    "    Summary:\n",
    "\"\"\"\n",
    "\n",
    "custom_summary_prompt = PromptTemplate.from_template(summary_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita mendefinisikan sebuah fungsi yang akan me-return chain untuk melakukan summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_chain(llm):\n",
    "    summary_chain = {'text':RunnablePassthrough()} | custom_summary_prompt | llm | StrOutputParser()\n",
    "    return summary_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_summarizer = create_summary_chain(\n",
    "    llm = ChatGoogleGenerativeAI(model = \"gemini-pro\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita sebuah chain untuk melakukan summarizer dengan Gemini sebagai LLM. Selanjutnya, kita dapat menjalankan chain tersebut dan memberikan dokumen yang akan diringkas (`llm_ts_content`) sebagai input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        gemini_summarizer.invoke(trans_ts_content)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A dengan LLM Lainnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain API dari Gemini, Anda bisa juga menggunakan API yang disediakan oleh provider lain, seperti OpenAI dan HuggingFace. Untuk mengakses model dari provider tersebut, Anda juga memerlukan API key:\n",
    "\n",
    "* [Cara mendapatkan API key dari OpenAI](https://platform.openai.com/docs/quickstart).\n",
    "* [Cara mendapatkan API key dari HuggingFace](https://huggingface.co/docs/api-inference/en/quicktour).\n",
    "\n",
    "Selanjutnya, Anda perlu menambahkan API key tersebut di dalam file `.env`.\n",
    "\n",
    "Berikut adalah contoh file `.env()` dengan API key dari Google, OpenAI, dan HuggingFace.\n",
    "```\n",
    "OPENAI_API_KEY = \"sk-proj-**********\"\n",
    "GOOGLE_API_KEY = \"AIza************\"\n",
    "HUGGINGFACEHUB_API_TOKEN = \"hf_*********\"\n",
    "```\n",
    "Selanjutnya, Anda dapat menggunakan fungsi `load_dotenv()` untuk me-load API key secara otomatis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "# LLM\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_open_ai = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "#     persist_directory = 'data_input/chroma_open_ai'\n",
    "# )\n",
    "\n",
    "# time spent: 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memanggil embedding dari directory yang sudah disimpan\n",
    "\n",
    "vec_openai = Chroma(persist_directory = 'data_input/chroma_open_ai',\n",
    "                    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat custom chain untuk Q&A dengan LLM OpenAI\n",
    "\n",
    "openai_chain = create_qa_chain(\n",
    "    retriever = vec_openai.as_retriever(),\n",
    "    llm = ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A implementation\n",
    "\n",
    "print(\n",
    "    textwrap.fill(\n",
    "        openai_chain.invoke('Describe about llaMa model family'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        openai_chain.invoke('How is the technical evolution of GPT-series model?'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤗 HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # embedding HuggingFace \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# LLM\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_hf = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "#     persist_directory = 'data_input/chroma_hugging_face'\n",
    "# )\n",
    "\n",
    "# time spent: 6m 24.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_hf = Chroma(persist_directory = 'data_input/chroma_hugging_face',\n",
    "                    embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat custom chain untuk Q&A dengan LLM dari HuggingFace\n",
    "\n",
    "hf_chain = create_qa_chain(\n",
    "    retriever = vec_hf.as_retriever(),\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=\"declare-lab/flan-alpaca-large\",\n",
    "        task=\"text-generation\",\n",
    "        model_kwargs={\"temperature\":0.3, \"max_length\":1000}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        hf_chain.invoke('Describe about llaMa model family'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        hf_chain.invoke('How is the technical evolution of GPT-series model?'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizer dengan LLM Lainnnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_summarizer = create_summary_chain(\n",
    "    llm =  ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        openai_summarizer.invoke(trans_ts_content)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next\n",
    "\n",
    "* Selain beberapa use case yang sudah kita pelajari di atas, Anda dapat mengeksplor use case lain contoh penggunaan LLM menggunakan LangChain: https://python.langchain.com/v0.1/docs/use_cases.\n",
    "* Anda juga bisa membuat aplikasi berbasis LLM dalam bentuk dashboard yang di-deploy, misalnya melalui Streamlit:\n",
    "    * Contoh dashboard: https://ask-summarize-pdf.streamlit.app/.\n",
    "    * Source code: https://github.com/saskia-dwi-ulfah/ptld5-dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Pada workshop ini, kita telah mempelajari banyak hal tentang Large Language Models (LLM). Kita memulai dengan memahami dasar-dasar Python yang penting untuk bekerja dengan LLM, termasuk berbagai library dan tools yang digunakan. Kemudian, kita mendalami foundational LLM, mulai dari konsep dasar tentang apa itu LLM, bagaimana ia bekerja, hingga berbagai arsitektur yang digunakan serta mekanisme pelatihannya.\n",
    "\n",
    "Selain itu, kita juga mengeksplorasi penggunaan LLM untuk file PDF, seperti fitur Q&A dan summarization. Ini memungkinkan LLM mengekstrak informasi penting dari dokumen PDF dan memberikan ringkasan yang berguna. Penggunaan ini menunjukkan bagaimana LLM dapat diterapkan dalam berbagai konteks untuk meningkatkan efisiensi dan efektivitas pengolahan data.\n",
    "\n",
    "Selanjutnya, kami mendorong Anda untuk mengeksplorasi lebih jauh berbagai use case lain dari LLM, seperti analisis data, chatbot, dan automasi tugas administratif. Anda juga bisa mempelajari cara men-deploy model ini dalam bentuk dashboard atau aplikasi lain yang dapat digunakan dalam konteks bisnis atau personal. \n",
    "\n",
    "Semoga pengetahuan yang telah kita pelajari dalam workshop ini dapat menjadi dasar yang kuat bagi Anda untuk terus berkembang dan berinovasi dengan LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [LangChain Documentation](https://python.langchain.com/v0.1/docs/get_started/introduction).\n",
    "* [Gemini API Documentation](https://ai.google.dev/gemini-api/docs).\n",
    "* [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dss_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
