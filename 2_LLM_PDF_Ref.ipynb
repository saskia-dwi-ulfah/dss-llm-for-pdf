{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSS May 2024: Efficient Information Extraction: Q&A and Summarization over PDF Documents using LLM\n",
    "\n",
    "* Instruktur: [Saskia Dwi Ulfah](https://www.linkedin.com/in/saskia-dwi-ulfah/).\n",
    "* Last updated: May 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìï Dokumen yang disimpan dalam format **Portable Document Format (PDF)** merupakan salah satu bentuk dokumen yang sering digunakan untuk bertukar informasi melalui internet dan perangkat digital seperti handphone, laptop, dan komputer. PDF juga dimanfaatkan di berbagai sektor. Di bidang pendidikan, mahasiswa dapat mengakses jurnal penelitian dengan format PDF melalui website seperti Elsevier dan IEEE. Di bidang finansial, perusahaan menampilkan laporan keuangan tahunan dalam format PDF pada website perusahaan.\n",
    "\n",
    "\n",
    "‚õ≥ Format PDF merupakan format yang universal. Artinya, dokumen yang disimpan dalam format PDF dapat diakses secara mudah pada perangkat yang berbeda. Selain itu, format PDF lebih disukai karena tampilan dokumen yang lebih rapi dibandingkan format dokumen lainnya. PDF juga men-support dokumen yang terdiri lebih dari satu halaman. Hal ini memungkinkan pengguna untuk menuliskan informasi yang lengkap dan komprehensif dalam sebuah dokumen PDF. Akan tetapi, hal ini membuat **dokumen PDF cenderung tebal dan kompleks** sehingga menyebabkan pembaca **kesulitan untuk menemukan informasi yang spesifik**.\n",
    "\n",
    "üí° Dengan perkembangan teknologi artificial intelligence  (AI) dan machine learning (ML), kita dapat menggunakan **Large Language Model (LLM)** untuk pencarian informasi pada dokumen PDF. Pada workshow ini, Anda akan belajar bagaimana kita dapat memperluas kemampuan LLM untuk pencarian informasi secara efisien dari dokumen PDF. Dimulai dengan dokumen PDF biasa, Anda akan belajar cara memproses dokumen ini dan menyajikannya sebagai konteks tambahanuntuk LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Setelah menyelesaikan workshop ini, Anda diharapkan dapat:\n",
    "\n",
    "* Memahami konsep dasar dari LLM.\n",
    "* Mengimplementasikan penggunaan LLM dengan framework LangChain.\n",
    "* Memahami workflow yang digunakan dalam menyediakan additional context untuk LLM.\n",
    "* Mengembangkan skill di bidang AI dan data science dengan menguasai teknik information retrieval dari dokumen PDF menggunakan LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Syllabus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Python Programming Basics** \n",
    "    - Introduction to Python for Data Science.\n",
    "    - Working with Python Environment.\n",
    "    - Working with Notebook.\n",
    "    - Python Fundamental Data Types and Data Structures.\n",
    "    - Understanding Looping Concept in Python.\n",
    "    - Understanding The Creation of Python Function.\n",
    "    - Understanding The Usage of Python Libraries.\n",
    "* **The Fundamentals of LLM**\n",
    "    - The Concept of Generative AI.\n",
    "    - LLM as Generative AI.\n",
    "    - Transformer Architecture in a Nutshell\n",
    "    - LLM Capability, Limitation, and Consideration\n",
    "* **Introduction to LangChain**\n",
    "    - The Big Picture of LangChain Concept and Component\n",
    "    - API Concept and Setting for LangChain Usage\n",
    "    - Demonstration of LLM Usage with LangChain\n",
    "* **Case Study: Q&A and Summarization for PDF Document**\n",
    "    - The Concept of RAG (Retrieval Augmented Generation)\n",
    "    - Loading PDF Documents using LangChain\n",
    "    - The Concept of Embedding for PDF Documents\n",
    "    - Storing The Embedding using a Vector Database\n",
    "    - Prompt Creation for Q&A and Summarization Cases\n",
    "    - Employing LLM for Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fundamentals of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concept of Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/gen_ai_hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Apa itu Generative AI?**\n",
    "\n",
    "**Generative AI** merupakan bagian dari artificial intelligence yang bertujuan untuk **menghasilkan (generate) data**, seperti teks, gambar, video, dan musik. Untuk dapat menghasilkan data, generative AI dilatih/belajar dari data yang jumlahnya sangat besar. Proses belajar ini sering disebut training. Generative AI akan menghasilkan data berdasarkan pola yang sudah dipelari selama proses training.\n",
    "\n",
    "üöÄ **Jenis-Jenis Generative AI dan Aplikasinya**\n",
    "\n",
    "1. **Generative Text Model**\n",
    "    * Input: teks.\n",
    "    * Output ‚Üí teks (text-to-text generator).\n",
    "      * Contoh: translation, **summarization**, **question-answering**, grammar correction.\n",
    "    * Output ‚Üí image (text-to-image generator).\n",
    "      * Contoh: image generation, video generation.\n",
    "    * Output ‚Üí audio (text-to-speech generator).\n",
    "      * Contoh: music generation.\n",
    "2. **Generative Image Model**\n",
    "    * Input: gambar.\n",
    "    * Output: teks (image-to-text generator).\n",
    "      * Contoh: image captioning, visual question-answering, image search.\n",
    "    * Output: gambar (image-to-image generator).\n",
    "      * Contoh: image completion.\n",
    "    * Output: video (image-to-video generator).\n",
    "      * Contoh: animation generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM as Generative AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí¨ **LLM** merupakan generative AI yang telah dilatih menggunakan data teks dalam jumlah yang sangat besar. LLM mampu untuk memahami dan menghasilkan teks dengan gaya bahasa yang mirip dengan manusia. Selain menggambarkan besarnya data yang digunakan untuk melatih LLM, istilah \"large\" mengacu kepada tingkat kompleksitas, ukuran, dan banyaknya parameter pada LLM.\n",
    "\n",
    "> ‚ÑπÔ∏è Sebagai contoh, GPT-4, varian yang paling baru dari LLM yang dirilis oleh OpenAI, memiliki 1,3 triliun parameter. Sumber data yang digunakan untuk melatih model ini meliputi buku, website, jurnal ilmiah, artikel, posting-an media sosial, dan repositori kode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Platforms\n",
    "\n",
    "üí¨ Beberapa contoh platform LLM yang cukup populer: \n",
    "\n",
    "1. ü§ñ**GPT**ü§ñ\n",
    "\n",
    "    ü§ñ **Generative Pre-Trained Transformer (GPT)** merupakan large language model yang dikembangkan oleh Open AI. Sama seperti model generative AI lainnya, GPT di-training dengan data teks dalam jumlah yang besar. GPT dirancang untuk memahami dan menghasilkan teks dengan cara yang mirip dengan manusia, sehingga dapat digunakan dalam berbagai aplikasi seperti chatbot, language translation, dan content creation. Baru-baru ini, OpenAI meluncurkan varian terbaru dari GPT-4, yakni GPT-4o.\n",
    "\n",
    "    Segmentasi produk GPT dapat dibedakan menjadi 2: chat-based platform dan Application Programming Interface (API)-based platform. Produk GPT berbasis chat dikenal sebagai [**ChatGPT**](https://chatgpt.com/). Semenjak dirilis pada November 2022 dengan versi GPT-2, saat ini ChatGPT telah merilis varian GPT yang lebih powerful: ChatGPT dengan GPT-3.5 dan ChatGPT dengan GPT-4. Untuk ChatGPT-3.5, pengguna dapat mengakses secara free. Untuk mendaptkan fitur yang lebih advanced pada ChatGPT dengan GPT-4, pengguna perlu melakukan subscription terlebih dahulu.\n",
    "\n",
    "    Varian produk kedua adalah produk berbasis [**Application Programming Interface (API)**](https://platform.openai.com/). API memungkinkan pengguna untuk meng-embbed kemampuan GPT pada aplikasi yang dapat disesuakaikan dengan kebutuhan mereka. Penggunaan API ini sangat fleksibel karena dapat diintegrasikan dengan berbagai platform dan sistem yang sudah ada. \n",
    "\n",
    "2. ‚ôä**Gemini**‚ôä\n",
    "\n",
    "    ‚ôä **Gemini** merupakan model generative AI yang dikembangkan oleh Google. Gemini dilatih menggunakan data teks, gambar, suara, dan video secara bersamaan dalam jumlah yang sangat besar. Hal ini membuat Gemini mampu untuk menerima berbagai macam input. Kelebihan ini menjadikan Gemini unggul dari segi generalisasinya terhadap data baru. Saat ini, versi terbaru dari Gemini adalah Gemini 1.5. \n",
    "\n",
    "    Sama seperti GPT, secara umum, fitur-fitur pada Gemini dikemas dalam 2 bentuk produk. Produk yang pertama merupakan [aplikasi yang berbasis chat bernama **Gemini app (sebelumnya Bard)**](https://gemini.google.com/app). Penggunaan aplikasi ini ditujukan untuk siapa saja secara umum tanpa perlu menuliskan kode tertentu. Produk lainnya merupakan [**API**](https://ai.google.dev/gemini-api) yang dapat digunakan pada use case yang lebih custom. Penggunaan API ini lebih ditujukan untuk pengguna dengan aspek pekerjaan teknis yang lebih tinggi, seperti software developer.\n",
    "\n",
    "3. ü§ó**HuggingFace**ü§ó\n",
    "\n",
    "    ü§ó**HuggingFace** platform yang dikenal luas dalam komunitas machine learning dan AI, khususnya dalam bidang Natural Language Processing (NLP). Platform ini menyediakan model-model pre-trained yang dapat digunakan untuk berbagai task yang berkaitan dengan NLP, seperti language modeling dan sentiment analysis.\n",
    "\n",
    "    Beberapa contoh LLM open source yang dapat diakses melalui HuggingFace:\n",
    "    \n",
    "    ---add the list---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt pada Language Model\n",
    "\n",
    "Untuk menghasilkan output yang sesuai, LLM memerlukan **prompt**. Prompt dapat dianalogikan sebagai sebuah petunjuk untuk LLM. Secara eksplisit, prompt merujuk kepada instruksi atau detail yang diberikan kepada LLM terkait output seperti apa yang diharapkan. Prompt juga bisa berupa konteks atau latar belakang dari topik yang ingin diketahui.\n",
    "\n",
    "üí° Beberapa tips awal untuk merancang sebuah prompt:\n",
    "\n",
    "1. **Buat Prompt yang Sederhana**\n",
    "\n",
    "    Pembuatan prompt merupakan proses iteratif. Selalu mulai dengan sebuah prompt sederhana. Jika masih belum mendapatkan output yang diinginkan dari LLM, kita bisa me-refine prompt yang sudah dibuat sebelumnya.\n",
    "\n",
    "    Hindari pembuatan prompt untuk task yang kompleks. Kita dapat mem-break down task yang kompleks tersebut menjadi subtask yang lebih kecil dan membuat prompt yang berbeda untuk masing-masing subtask.\n",
    "\n",
    "2. **Berikan Perintah yang Jelas: What to Do and What Not to Do?**\n",
    "\n",
    "    Saat membuat sebuah prompt, pastikan prompt memiliki instruksi yang jelas. Misalnya, kita menginginkan ringkasan dari sebuah kalimat, maka prompt yang efektif mengandung instruksi untuk membuat ringkasan tersebut. Kita juga dapat mendefinisikan detail apa yang tidak perlu dimasukkan dalam ringkasan yang dihasilkan LLM.\n",
    "\n",
    "\n",
    "3. **Be Speficic and Detail**\n",
    "\n",
    "    Selain memberikan perintah yang jelas, kita dapat membuat prompt menjadi lebih spesifik dan detail. Misalnya, kita dapat mendefinisikan detail seperti gaya bahasa dan panjang output yang diharapkan.\n",
    "\n",
    "Contoh prompt:\n",
    "\n",
    "```\n",
    "\n",
    "Summarize the following article into approximately 150 words, highlighting the main points and key information discussed. Do not include minor details, personal opinions, or repetitive information.\n",
    "\n",
    "```\n",
    "\n",
    "> üîë Kunci prompt yang efektif: sederhana, jelas, dan spesifik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñ Arsitektur **Transformer** merupakan salah satu fondasi dibalik keandalan LLM. Arsitektur ini pertama kali diperkenalkan pada paper yang berjudul [Attention is All You Need](https://arxiv.org/abs/1706.03762) oleh Vaswani dkk. pada tahun 2017. \n",
    "\n",
    "Kemunculan arsitektur Transformer bertolak dari keterbatasan dari arsitektur yang sebelumnya sering digunakan untuk memodelkan data berupa sequence (contoh: bahasa), seperti Recurrent Neural Network (RNN) dan Long Short-Term Memory (LSTM). Keterbatasan ini di antaranya: ketidakefisienan dari segi daya komputasi dan waktu serta ketersediaan memori yang terbatas untuk sequence yang cukup panjang. Transformer mengatasi keterbatasan ini dengan fitur spesial yang disebut dengan **self-attention mechanism**. \n",
    "\n",
    "Secara sederhana, Transformer terdiri dari 2 komponen utama:\n",
    "\n",
    "1. **Encoder**\n",
    "2. **Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Capability, Limitation, and Consideration\n",
    "\n",
    "Saat ini, LLM banyak dipilih karena keandalan dan kemampuannya untuk memahami konteks yang rumit. Selain itu, LLM juga mampu untuk menghasilkan teks dan narasi yang hampir menyamai produk bahasa yang ditulis oleh manusia. \n",
    "\n",
    "Berikut ini beberapa contoh task yang dapat diaplikasikan oleh LLM.\n",
    "\n",
    "* **Text Generation**: menghasilkan narasi dengan gaya bahasa yang hampir menyamai kemampuan manusia.\n",
    "* **Translation**: menerjemahkan teks ke dalam bahasa yang berbeda secara akurat.\n",
    "* **Summarization**: membuat ringkasan singkat dari teks yang panjang.\n",
    "* **Question-Answering**: menjawab sebuah pertanyaan berdasarkan konteks yang diberikan.\n",
    "\n",
    "Akan tetapi, perlu diperhatikan bahwa saat ini LLM masih menjadi topik riset yang aktif. Para peneliti masih mengembangkan metode yang lebih advanced untuk mengurangi beberapa limitasi dari LLM, seperti:\n",
    "\n",
    "* **Misleading Output** \n",
    "    \n",
    "    Sebagai produk AI yang sangat powerful, output yang dihasilkan oleh LLM dapat terlihat sangat meyakinkan. Akan tetapi, sebagai pengguna, kita perlu bijak dalam mengonsumsi konten yang dibuat oleh AI. Secara aktif, kita perlu melakukan verifikasi fakta melalui sumber-sumber yang kredibel.\n",
    "\n",
    "* **Technical Limitation**\n",
    "\n",
    "    Di balik kapabilitas LLM dalam task yang melibatkan natural language, masih terdapat concern terkait limitasi teknis. Terdapat situasi di mana AI tidak dapat memahami konteks secara mendalam atau menghasilkan respons yang sesuai dengan harapan pengguna. Selain itu, LLM memerlukan sumber daya komputasi yang sangat tinggi untuk berfungsi secara optimal, yang dapat menjadi hambatan bagi pengguna dengan keterbatasan infrastruktur teknologi.\n",
    "\n",
    "* **Laws and Rights-Related Issue**\n",
    "\n",
    "    Sebagai teknologi yang terus berkembang, penggunaan LLM juga dihadapkan pada isu-isu hukum dan hak. Misalnya, hak cipta, privasi, dan perlindungan data adalah aspek-aspek penting yang perlu dipertimbangkan saat menggunakan AI. \n",
    "    \n",
    "    Pengguna harus memastikan bahwa mereka mematuhi semua regulasi yang relevan dan menghormati hak-hak individu dan entitas lain dalam penggunaan dan penyebaran konten yang dihasilkan oleh AI.\n",
    "\n",
    "Mempertimbangkan kelebihan dan limitasi yang ada, sebagai end-user, kita perlu cermat, selektif, dan aware dalam mempergunakan LLM. Kemampuan LLM yang powerful akan semakin maksimal selama dibersamai dengan konsiderasi berikut:\n",
    "\n",
    "1. **Tidak Menganggap Output LLM sebagai Final Result** \n",
    "    \n",
    "    Output dari LLM tidak dijadikan sebagai hasil akhir, melainkan hanya sebagai pendukung tersier. Kita perlu tetap melakukan verifikasi dan cross-check dengan sumber lain yang terpercaya untuk memastikan akurasi dan relevansi informasi.\n",
    "\n",
    "2. **Keputusan Kompleks dan Terkait Makhluk Hidup**\n",
    "    \n",
    "    Keputusan yang diambil dari output LLM tidak digunakan dalam pertimbangan yang kompleks dan menyangkut keperluan yang berkaitan dengan makhluk hidup, seperti keputusan medis, hukum, atau keuangan, tanpa konsultasi dengan ahli yang kompeten di bidang tersebut.\n",
    "\n",
    "3. **Evaluasi Kritis dan Penyesuaian Konteks**\n",
    "    \n",
    "    Selalu melakukan evaluasi kritis terhadap output yang dihasilkan oleh LLM, termasuk memahami konteks dan bias yang mungkin ada dalam data pelatihannya. Penyesuaian dan interpretasi yang bijak diperlukan untuk memastikan bahwa informasi tersebut sesuai dengan konteks yang dibutuhkan.\n",
    "\n",
    "4. **Keamanan dan Privasi Data**\n",
    "\n",
    "    Memperhatikan aspek keamanan dan privasi data saat menggunakan LLM. Jangan memasukkan informasi pribadi atau sensitif yang dapat disalahgunakan, dan pastikan bahwa penggunaan LLM mematuhi kebijakan privasi dan regulasi yang berlaku.\n",
    "\n",
    "> üìå More info: [Responsible AI](https://ai.google/responsibility/responsible-ai-practices/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChainü¶úüîó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü¶úüîó**LangChain** merupakan framework untuk membuat aplikasi berbasis LLM.\n",
    "\n",
    "> üìå[LangChain Official Documentation](https://python.langchain.com/v0.1/docs/get_started/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Concept and Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Concept and Setting for LangChain Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of LLM Usage with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Q&A and Summarization for PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk loading file PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader('data_input/llm_survey.pdf',\n",
    "                         extract_images = True)\n",
    "pdf_data = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='3\\n(i.e.,following the scaling law [30]). A number of studies\\nhave explored the performance limit by training an ever\\nlarger PLM ( e.g., the 175B-parameter GPT-3 and the 540B-\\nparameter PaLM). Although scaling is mainly conducted\\nin model size (with similar architectures and pre-training\\ntasks), these large-sized PLMs display different behaviors\\nfrom smaller PLMs ( e.g., 330M-parameter BERT and 1.5B-\\nparameter GPT-2) and show surprising abilities (called emer-\\ngent abilities [31]) in solving a series of complex tasks. For\\nexample, GPT-3 can solve few-shot tasks through in-context\\nlearning , whereas GPT-2 cannot do well. Thus, the research\\ncommunity coins the term ‚Äú large language models (LLM) ‚Äù1\\nfor these large-sized PLMs [32‚Äì35], which attract increasing\\nresearch attention (See Figure 1). A remarkable application\\nof LLMs is ChatGPT2that adapts the LLMs from the GPT\\nseries for dialogue, which presents an amazing conversation\\nability with humans. We can observe a sharp increase of the\\narXiv papers that are related to LLMs after the release of\\nChatGPT in Figure 1.\\nAs discussed before, language model is not a new tech-\\nnical concept specially for LLMs, but has evolved with the\\nadvance of artificial intelligence over the decades. Early lan-\\nguage models mainly aim to model and generate text data,\\nwhile latest language models ( e.g., GPT-4) focus on complex\\ntask solving. From language modeling totask solving , it is an\\nimportant leap in scientific thinking, which is the key to\\nunderstand the development of language models in the re-\\nsearch history. From the perspective of task solving, the four\\ngenerations of language models have exhibited different lev-\\nels of model capacities. In Figure 2, we describe the evolu-\\ntion process of language models in terms of the task solving\\ncapacity. At first, statistical language models mainly assisted\\nin some specific tasks ( e.g., retrieval or speech tasks), in\\nwhich the predicted or estimated probabilities can enhance\\nthe performance of task-specific approaches. Subsequently,\\nneural language models focused on learning task-agnostic\\nrepresentations ( e.g., features), aiming to reduce the efforts\\nfor human feature engineering. Furthermore, pre-trained\\nlanguage models learned context-aware representations that\\ncan be optimized according to downstream tasks. For the\\nlatest generation of language model, LLMs are enhanced by\\nexploring the scaling effect on model capacity, which can be\\nconsidered as general-purpose task solvers. To summarize,\\nin the evolution process, the task scope that can be solved\\nby language models have been greatly extended, and the\\ntask performance attained by language models have been\\nsignificantly enhanced.\\nIn the existing literature, PLMs have been widely dis-\\ncussed and surveyed [36‚Äì39], while LLMs are seldom re-\\nviewed in a systematic way. To motivate our survey, we first\\nhighlight three major differences between LLMs and PLMs.\\nFirst, LLMs display some surprising emergent abilities that\\nmay not be observed in previous smaller PLMs. These abili-\\nties are key to the performance of language models on com-\\nplex tasks, making AI algorithms unprecedently powerful\\nand effective. Second, LLMs would revolutionize the way\\nthat humans develop and use AI algorithms. Unlike small\\n1. Note that a LLM is not necessarily more capable than a small PLM,\\nand emergent abilities may not occur in some LLMs.\\n2. https://openai.com/blog/chatgpt/PLMs, the major approach to accessing LLMs is through\\nthe prompting interface ( e.g., GPT-4 API). Humans have to\\nunderstand how LLMs work and format their tasks in a way\\nthat LLMs can follow. Third, the development of LLMs no\\nlonger draws a clear distinction between research and en-\\ngineering. The training of LLMs requires extensive practical\\nexperiences in large-scale data processing and distributed\\nparallel training. To develop capable LLMs, researchers\\nhave to solve complicated engineering issues, working with\\nengineers or being engineers.\\nNowadays, LLMs are posing a significant impact on\\nthe AI community, and the advent of ChatGPT and GPT-4\\nleads to the rethinking of the possibilities of artificial general\\nintelligence (AGI). OpenAI has published a technical article\\nentitled ‚Äú Planning for AGI and beyond ‚Äù, which discusses\\nthe short-term and long-term plans to approach AGI [40],\\nand a more recent paper has argued that GPT-4 might be\\nconsidered as an early version of an AGI system [41]. The\\nresearch areas of AI are being revolutionized by the rapid\\nprogress of LLMs. In the field of NLP , LLMs can serve as a\\ngeneral-purpose language task solver (to some extent), and\\nthe research paradigm has been shifting towards the use\\nof LLMs. In the field of IR, traditional search engines are\\nchallenged by the new information seeking way through AI\\nchatbots ( i.e.,ChatGPT), and New Bing3presents an initial\\nattempt that enhances the search results based on LLMs. In\\nthe field of CV , the researchers try to develop ChatGPT-like\\nvision-language models that can better serve multimodal\\ndialogues [42‚Äì45], and GPT-4 [46] has supported multi-\\nmodal input by integrating the visual information. This new\\nwave of technology would potentially lead to a prosperous\\necosystem of real-world applications based on LLMs. For\\ninstance, Microsoft 365 is being empowered by LLMs ( i.e.,\\nCopilot) to automate the office work, and OpenAI supports\\nthe use of plugins in ChatGPT for implementing special\\nfunctions.\\nDespite the progress and impact, the underlying prin-\\nciples of LLMs are still not well explored. Firstly, it is\\nmysterious why emergent abilities occur in LLMs, instead of\\nsmaller PLMs. As a more general issue, there lacks a deep,\\ndetailed investigation of the key factors that contribute to\\nthe superior abilities of LLMs. It is important to study when\\nand how LLMs obtain such abilities [47]. Although there are\\nsome meaningful discussions about this problem [31, 47],\\nmore principled investigations are needed to uncover the\\n‚Äúsecrets ‚Äú of LLMs. Secondly, it is difficult for the research\\ncommunity to train capable LLMs. Due to the huge de-\\nmand of computation resources, it is very costly to carry\\nout repetitive, ablating studies for investigating the effect\\nof various strategies for training LLMs. Indeed, LLMs are\\nmainly trained by industry, where many important training\\ndetails ( e.g., data collection and cleaning) are not revealed\\nto the public. Thirdly, it is challenging to align LLMs with\\nhuman values or preferences. Despite the capacities, LLMs\\nare also likely to produce toxic, fictitious, or harmful con-\\ntents. It requires effective and efficient control approaches\\nto eliminating the potential risk of the use of LLMs [46].\\nFaced with both opportunities and challenges, it needs\\nmore attention on the research and development of LLMs. In\\n3. https://www.bing.com/new', metadata={'source': 'data_input/llm_survey.pdf', 'page': 2})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk split file PDF menjadi chunk yang lebih kecil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 450,\n",
    "    separators=[\n",
    "                \"\\n\\n\",\n",
    "                \"\\n\",\n",
    "                \" \",\n",
    "                \".\",\n",
    "                \",\",\n",
    "                \"\\u200b\",  # zero-width space\n",
    "                \"\\uff0c\",  # full-width comma\n",
    "                \"\\u3001\",  # ideographic comma\n",
    "                \"\\uff0e\",  # full-width full stop\n",
    "                \"\\u3002\",  # ideographic full stop\n",
    "                \"\",\n",
    "            ])\n",
    "splits = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASANI\\anaconda3\\envs\\dss_may2024\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# untuk membuat embedding\n",
    "from langchain_openai import OpenAIEmbeddings # embedding OpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # embedding Google Generative AI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # embedding HuggingFace \n",
    "\n",
    "# untuk menyimpan embedding\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore_folder(documents, embedding, persist_directory):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents = documents, \n",
    "        embedding = embedding,\n",
    "        persist_directory = persist_directory\n",
    "    )\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_open_ai = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "#     persist_directory = 'data_input/chroma_open_ai'\n",
    "# )\n",
    "\n",
    "# time spent: 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_gemini = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
    "#     persist_directory = 'data_input/chroma_gemini'\n",
    "# )\n",
    "\n",
    "# time spent: 40.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_hf = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "#     persist_directory = 'data_input/chroma_hugging_face'\n",
    "# )\n",
    "\n",
    "# time spent: 6m 24.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memanggil embedding dari directory yang sudah disimpan\n",
    "\n",
    "vec_openai = Chroma(persist_directory = 'data_input/chroma_open_ai',\n",
    "                    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_gemini = Chroma(persist_directory = 'data_input/chroma_gemini',\n",
    "                    embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_hf = Chroma(persist_directory = 'data_input/chroma_hugging_face',\n",
    "                    embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "# mempersiapkan prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# untuk memasukkan pertanyaan\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# untuk menampilkan output yang diinginkan\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# menampilkan output secara rapi\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    You are the great assistant in understanding additional context\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    Use the minimum of three sentences to answer the question. \n",
    "    Try your best to answer as complete as possible with easy style of English.\n",
    "    Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_openai = vec_openai.as_retriever()\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0) # versi terbaru dari GPT\n",
    "\n",
    "openai_chain = (\n",
    "    {\"context\": retriever_openai | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLaMA model family is a collection of models introduced by Meta AI in February 2023.\n",
      "The family consists of four sizes: 7B, 13B, 30B, and 65B. Since its release, LLaMA has\n",
      "gained significant attention from both the research and industry communities due to its\n",
      "excellent performance on various open benchmarks. It has become one of the most popular\n",
      "open language models. Many researchers have extended LLaMA models through instruction\n",
      "tuning or continual pretraining. Stanford Alpaca is one of the extended models, which is\n",
      "the first open instruct-following model fine-tuned based on LLaMA (7B). Other popular\n",
      "LLaMA variants include Vicuna, which is trained upon user-shared conversations. Thanks for\n",
      "asking!\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(openai_chain.invoke('Describe about llaMa model family'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Turing test was proposed by Alan Turing, a British mathematician and computer\n",
      "scientist, in 1950. This test was designed to assess a machine's ability to exhibit\n",
      "intelligent behavior equivalent to, or indistinguishable from, human behavior. It's a\n",
      "significant concept in the field of artificial intelligence. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(openai_chain.invoke('When was the Turing test proposed?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text provided does not specify a single paper title. It mentions several papers by\n",
      "different authors, published in different years, and in different journals or conferences.\n",
      "Some of the mentioned papers include \"Quantifying memorization across neural language\n",
      "models\", \"Extracting training data from large language models\", \"Deduplicating training\n",
      "data mitigates privacy risks in language models\", \"Conditional random fields:\n",
      "Probabilistic models for segmenting and labeling sequence data\", and \"Realtoxicityprompts:\n",
      "Evaluating neural toxic degeneration in language models\". Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(openai_chain.invoke('What is the title of the paper?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The technical evolution of the GPT-series models has been a progressive journey. It\n",
      "started with GPT-1, which was developed based on a generative, decoder-only Transformer\n",
      "architecture. It used a hybrid approach of unsupervised pretraining and supervised fine-\n",
      "tuning. Then came GPT-2, which increased the parameter scale to 1.5B and was trained with\n",
      "a large webpage dataset WebText. It performed tasks via unsupervised language modeling,\n",
      "without explicit fine-tuning using labeled data. GPT-3 was released in 2020, scaling the\n",
      "model parameters to an even larger size of 175B and introduced the concept of in-context\n",
      "learning. The evolution continued with models like GPT-3.5, GPT-4, and GPT-4 Turbo, each\n",
      "introducing new features and improvements. For instance, GPT-4 Turbo introduced a longer\n",
      "context window and multimodal ability. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(openai_chain.invoke('How is the technical evolution of GPT-series model?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(retriever, llm):\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_chain = create_chain(\n",
    "    retriever = vec_gemini.as_retriever(),\n",
    "    llm = ChatGoogleGenerativeAI(model = 'gemini-pro')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLaMA model family consists of a variety of language models, including LLaMA, Alpaca,\n",
      "Vicuna, and PandaGPT. These models are known for their strong language understanding and\n",
      "generation abilities, and have been used in a wide range of applications, including\n",
      "multimodal modeling, instruction following, and dialogue generation. LLaMA is the\n",
      "foundation model for many of these variants, and its release has greatly advanced the\n",
      "research progress of LLMs. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(gemini_chain.invoke('Describe about llaMa model family'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text does not mention when the Turing test was first proposed, so I cannot\n",
      "answer this question from the provided context.  Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(gemini_chain.invoke('When was the Turing test first proposed?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot find the title of the paper within the context provided.  Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(gemini_chain.invoke('What is the title of the paper?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The technical evolution of GPT-series models has seen a significant increase in the number\n",
      "of parameters, from 117 million in GPT-1 to 175 billion in GPT-3. This has led to a\n",
      "corresponding increase in the models' performance on a wide range of natural language\n",
      "processing tasks. In addition, the GPT-series models have also seen the introduction of\n",
      "new features, such as the ability to generate text in response to a prompt, and the\n",
      "ability to translate languages. These features have made the GPT-series models\n",
      "increasingly useful for a variety of applications, including customer service, marketing,\n",
      "and education. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(gemini_chain.invoke('How is the technical evolution of GPT-series model?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_chain = create_chain(\n",
    "    retriever = vec_hf.as_retriever(),\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=\"declare-lab/flan-alpaca-large\",\n",
    "        task=\"text-generation\",\n",
    "        model_kwargs={\"temperature\":0.3, \"max_length\":1000}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLaMA model family is a powerful language model family that is widely used in many\n",
      "applications. It is based on the LLaMA architecture, which is a supervised learning\n",
      "algorithm that is trained on a large dataset of language data. It is able to generate\n",
      "strong language understanding and generation abilities, compared to other variants. It is\n",
      "also able to learn from user-shared conversations, which makes it suitable for multimodal\n",
      "tasks.\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(hf_chain.invoke('Describe about llaMa model family'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Turing test was proposed in 1947.\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(hf_chain.invoke('When was the Turing test is proposed?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the paper is \"Helpful: A Guide to Using Machine Learning to Enhance Your\n",
      "Research\".\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(hf_chain.invoke('What is the title of the paper?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The technical evolution of GPT-series models is a gradual process of improvement. The\n",
      "model has evolved over time, with new features and improvements being added to the model.\n",
      "The model has also been improved by scaling the model parameters, which has allowed for a\n",
      "key capacity leap. The model has also been improved by incorporating more features and\n",
      "incorporating more advanced techniques. Finally, the model has been improved by\n",
      "incorporating more advanced techniques such as in-context learning and reinforcement\n",
      "learning.\n"
     ]
    }
   ],
   "source": [
    "neat_result = textwrap.fill(hf_chain.invoke('How is the technical evolution of GPT-series model?'), width=90)\n",
    "print(neat_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Chatbot-Like Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 4o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dss_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
