{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSS May 2024: Efficient Information Extraction: Q&A and Summarization over PDF Documents using LLM\n",
    "\n",
    "* Instruktur: [Saskia Dwi Ulfah](https://www.linkedin.com/in/saskia-dwi-ulfah/).\n",
    "* Last updated: May 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“• Dokumen yang disimpan dalam format **Portable Document Format (PDF)** merupakan salah satu bentuk dokumen yang sering digunakan untuk bertukar informasi melalui internet dan perangkat digital seperti handphone, laptop, dan komputer. PDF juga dimanfaatkan di berbagai sektor. Di bidang pendidikan, mahasiswa dapat mengakses jurnal penelitian dengan format PDF melalui website seperti Elsevier dan IEEE. Di bidang finansial, perusahaan menampilkan laporan keuangan tahunan dalam format PDF pada website perusahaan.\n",
    "\n",
    "\n",
    "â›³ Format PDF merupakan format yang universal. Artinya, dokumen yang disimpan dalam format PDF dapat diakses secara mudah pada perangkat yang berbeda. Selain itu, format PDF lebih disukai karena tampilan dokumen yang lebih rapi dibandingkan format dokumen lainnya. PDF juga men-support dokumen yang terdiri lebih dari satu halaman. Hal ini memungkinkan pengguna untuk menuliskan informasi yang lengkap dan komprehensif dalam sebuah dokumen PDF. Akan tetapi, hal ini membuat **dokumen PDF cenderung tebal dan kompleks** sehingga menyebabkan pembaca **kesulitan untuk menemukan informasi yang spesifik**.\n",
    "\n",
    "ðŸ’¡ Dengan perkembangan teknologi artificial intelligence  (AI) dan machine learning (ML), kita dapat menggunakan **Large Language Model (LLM)** untuk pencarian informasi pada dokumen PDF. Pada workshow ini, Anda akan belajar bagaimana kita dapat memperluas kemampuan LLM untuk pencarian informasi secara efisien dari dokumen PDF. Dimulai dengan dokumen PDF biasa, Anda akan belajar cara memproses dokumen ini dan menyajikannya sebagai konteks tambahanuntuk LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Setelah menyelesaikan workshop ini, Anda diharapkan dapat:\n",
    "\n",
    "* Memahami konsep dasar dari LLM.\n",
    "* Mengimplementasikan penggunaan LLM dengan framework LangChain.\n",
    "* Memahami workflow yang digunakan dalam menyediakan additional context untuk LLM.\n",
    "* Mengembangkan skill di bidang AI dan data science dengan menguasai teknik information retrieval dari dokumen PDF menggunakan LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Syllabus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Python Programming Basics** \n",
    "    - Introduction to Python for Data Science.\n",
    "    - Working with Python Environment.\n",
    "    - Working with Notebook.\n",
    "    - Python Fundamental Data Types and Data Structures.\n",
    "    - Understanding Looping Concept in Python.\n",
    "    - Understanding The Creation of Python Function.\n",
    "    - Understanding The Usage of Python Libraries.\n",
    "* **The Fundamentals of LLM**\n",
    "    - The Concept of Generative AI.\n",
    "    - LLM as Generative AI.\n",
    "    - Transformer Architecture in a Nutshell\n",
    "    - LLM Capability, Limitation, and Consideration\n",
    "* **Introduction to LangChain**\n",
    "    - The Big Picture of LangChain Concept and Component\n",
    "    - API Concept and Setting for LangChain Usage\n",
    "    - Demonstration of LLM Usage with LangChain\n",
    "* **Case Study: Q&A and Summarization for PDF Document**\n",
    "    - The Concept of RAG (Retrieval Augmented Generation)\n",
    "    - Loading PDF Documents using LangChain\n",
    "    - The Concept of Embedding for PDF Documents\n",
    "    - Storing The Embedding using a Vector Database\n",
    "    - Prompt Creation for Q&A and Summarization Cases\n",
    "    - Employing LLM for Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fundamentals of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concept of Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/gen_ai_hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš€ **Apa itu Generative AI?**\n",
    "\n",
    "**Generative AI** merupakan bagian dari artificial intelligence yang bertujuan untuk **menghasilkan (generate) data**, seperti teks, gambar, video, dan musik. Untuk dapat menghasilkan data, generative AI dilatih/belajar dari data yang jumlahnya sangat besar. Proses belajar ini sering disebut training. Generative AI akan menghasilkan data berdasarkan pola yang sudah dipelari selama proses training.\n",
    "\n",
    "ðŸš€ **Jenis-Jenis Generative AI dan Aplikasinya**\n",
    "\n",
    "* **Generative Text Model**\n",
    "    * Input: teks.\n",
    "    * Output â†’ teks (text-to-text generator).\n",
    "      * Contoh: translation, **summarization**, **question-answering**, grammar correction.\n",
    "    * Output â†’ image (text-to-image generator).\n",
    "      * Contoh: image generation, video generation.\n",
    "    * Output â†’ audio (text-to-speech generator).\n",
    "      * Contoh: music generation.\n",
    "* **Generative Image Model**\n",
    "    * Input: gambar.\n",
    "    * Output: teks (image-to-text generator).\n",
    "      * Contoh: image captioning, visual question-answering, image search.\n",
    "    * Output: gambar (image-to-image generator).\n",
    "      * Contoh: image completion.\n",
    "    * Output: video (image-to-video generator).\n",
    "      * Contoh: animation generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM as Generative AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¬ **LLM** merupakan generative AI yang telah dilatih menggunakan data teks dalam jumlah yang sangat besar. LLM mampu untuk memahami dan menghasilkan teks dengan gaya bahasa yang mirip dengan manusia. Selain menggambarkan besarnya data yang digunakan untuk melatih LLM, istilah \"large\" mengacu kepada tingkat kompleksitas, ukuran, dan banyaknya parameter pada LLM.\n",
    "\n",
    "> â„¹ï¸ Sebagai contoh, GPT-4, varian yang paling baru dari LLM yang dirilis oleh OpenAI, memiliki 1,3 triliun parameter. Sumber data yang digunakan untuk melatih model ini meliputi buku, website, jurnal ilmiah, artikel, posting-an media sosial, dan repositori kode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Platforms\n",
    "\n",
    "ðŸ’¬ Beberapa contoh platform LLM yang cukup populer: \n",
    "\n",
    "* ðŸ¤–**GPT**ðŸ¤–\n",
    "\n",
    "    ðŸ¤– **Generative Pre-Trained Transformer (GPT)** merupakan large language model yang dikembangkan oleh Open AI. Sama seperti model generative AI lainnya, GPT di-training dengan data teks dalam jumlah yang besar. GPT dirancang untuk memahami dan menghasilkan teks dengan cara yang mirip dengan manusia, sehingga dapat digunakan dalam berbagai aplikasi seperti chatbot, language translation, dan content creation. Baru-baru ini, OpenAI meluncurkan varian terbaru dari GPT-4, yakni GPT-4o.\n",
    "\n",
    "    Segmentasi produk GPT dapat dibedakan menjadi 2: chat-based platform dan Application Programming Interface (API)-based platform. Produk GPT berbasis chat dikenal sebagai [**ChatGPT**](https://chatgpt.com/). Semenjak dirilis pada November 2022 dengan versi GPT-2, saat ini ChatGPT telah merilis varian GPT yang lebih powerful: ChatGPT dengan GPT-3.5 dan ChatGPT dengan GPT-4. Untuk ChatGPT-3.5, pengguna dapat mengakses secara free. Untuk mendaptkan fitur yang lebih advanced pada ChatGPT dengan GPT-4, pengguna perlu melakukan subscription terlebih dahulu.\n",
    "\n",
    "    Varian produk kedua adalah produk berbasis [**Application Programming Interface (API)**](https://platform.openai.com/). API memungkinkan pengguna untuk meng-embbed kemampuan GPT pada aplikasi yang dapat disesuakaikan dengan kebutuhan mereka. Penggunaan API ini sangat fleksibel karena dapat diintegrasikan dengan berbagai platform dan sistem yang sudah ada. \n",
    "\n",
    "* â™Š**Gemini**â™Š\n",
    "\n",
    "    â™Š **Gemini** merupakan model generative AI yang dikembangkan oleh Google. Gemini dilatih menggunakan data teks, gambar, suara, dan video secara bersamaan dalam jumlah yang sangat besar. Hal ini membuat Gemini mampu untuk menerima berbagai macam input. Kelebihan ini menjadikan Gemini unggul dari segi generalisasinya terhadap data baru. Saat ini, versi terbaru dari Gemini adalah Gemini 1.5. \n",
    "\n",
    "    Sama seperti GPT, secara umum, fitur-fitur pada Gemini dikemas dalam 2 bentuk produk. Produk yang pertama merupakan [aplikasi yang berbasis chat bernama **Gemini app (sebelumnya Bard)**](https://gemini.google.com/app). Penggunaan aplikasi ini ditujukan untuk siapa saja secara umum tanpa perlu menuliskan kode tertentu. Produk lainnya merupakan [**API**](https://ai.google.dev/gemini-api) yang dapat digunakan pada use case yang lebih custom. Penggunaan API ini lebih ditujukan untuk pengguna dengan aspek pekerjaan teknis yang lebih tinggi, seperti software developer.\n",
    "\n",
    "* ðŸ¤—**HuggingFace**ðŸ¤—\n",
    "\n",
    "    ðŸ¤—**HuggingFace** platform yang dikenal luas dalam komunitas machine learning dan AI, khususnya dalam bidang Natural Language Processing (NLP). Platform ini menyediakan model-model pre-trained yang dapat digunakan untuk berbagai task yang berkaitan dengan NLP, seperti language modeling dan sentiment analysis.\n",
    "\n",
    "    Beberapa contoh LLM open source yang dapat diakses melalui HuggingFace:\n",
    "    \n",
    "    ---add the list---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt pada Language Model\n",
    "\n",
    "Untuk menghasilkan output yang sesuai, LLM memerlukan **prompt**. Prompt dapat dianalogikan sebagai sebuah petunjuk untuk LLM. Secara eksplisit, prompt merujuk kepada instruksi atau detail yang diberikan kepada LLM terkait output seperti apa yang diharapkan. Prompt juga bisa berupa konteks atau latar belakang dari topik yang ingin diketahui.\n",
    "\n",
    "ðŸ’¡ Beberapa tips awal untuk merancang sebuah prompt:\n",
    "\n",
    "* **Buat Prompt yang Sederhana**\n",
    "\n",
    "    Pembuatan prompt merupakan proses iteratif. Selalu mulai dengan sebuah prompt sederhana. Jika masih belum mendapatkan output yang diinginkan dari LLM, kita bisa me-refine prompt yang sudah dibuat sebelumnya.\n",
    "\n",
    "    Hindari pembuatan prompt untuk task yang kompleks. Kita dapat mem-break down task yang kompleks tersebut menjadi subtask yang lebih kecil dan membuat prompt yang berbeda untuk masing-masing subtask.\n",
    "\n",
    "* **Berikan Perintah yang Jelas: What to Do and What Not to Do?**\n",
    "\n",
    "    Saat membuat sebuah prompt, pastikan prompt memiliki instruksi yang jelas. Misalnya, kita menginginkan ringkasan dari sebuah kalimat, maka prompt yang efektif mengandung instruksi untuk membuat ringkasan tersebut. Kita juga dapat mendefinisikan detail apa yang tidak perlu dimasukkan dalam ringkasan yang dihasilkan LLM.\n",
    "\n",
    "\n",
    "* **Be Speficic and Detail**\n",
    "\n",
    "    Selain memberikan perintah yang jelas, kita dapat membuat prompt menjadi lebih spesifik dan detail. Misalnya, kita dapat mendefinisikan detail seperti gaya bahasa dan panjang output yang diharapkan.\n",
    "\n",
    "Contoh prompt:\n",
    "\n",
    "```\n",
    "\n",
    "Summarize the following article into approximately 150 words, highlighting the main points and key information discussed. Do not include minor details, personal opinions, or repetitive information.\n",
    "\n",
    "```\n",
    "\n",
    "> ðŸ”‘ Kunci prompt yang efektif: sederhana, jelas, dan spesifik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤– Arsitektur **transformer** merupakan salah satu fondasi dibalik keandalan LLM. Arsitektur ini pertama kali diperkenalkan pada paper yang berjudul [Attention is All You Need](https://arxiv.org/abs/1706.03762) oleh Vaswani dkk. pada tahun 2017. \n",
    "\n",
    "Kemunculan arsitektur transformer bertolak dari keterbatasan dari arsitektur yang sebelumnya sering digunakan untuk memodelkan data berupa sequence (contoh: bahasa), seperti Recurrent Neural Network (RNN) dan Long Short-Term Memory (LSTM). Keterbatasan ini di antaranya: ketidakefisienan dari segi daya komputasi dan waktu serta ketersediaan memori yang terbatas untuk sequence yang cukup panjang. Transformer mengatasi keterbatasan ini dengan fitur spesial yang disebut dengan **self-attention mechanism**. \n",
    "\n",
    "Secara sederhana, Transformer terdiri dari 2 komponen utama:\n",
    "\n",
    "![](assets/transformer_simplified_3.png)\n",
    "\n",
    "\n",
    "â–¶ï¸ **Encoder**\n",
    "\n",
    "**Encoder** merupakan bagian dari transformer yang bertujuan untuk memproses input sequence ke dalam representasi tertentu. Representasi ini merupakan bentuk numerik dari input sequence berikut dengan informasi urutannya. **Encoder** terdiri dari beberapa lapisan yang disebut **encoder layer**. Setiap encoder layer memiliki dua bagian utama:\n",
    "\n",
    "- **Multi-Head Self-Attention Mechanism**\n",
    "\n",
    "    Bagian ini membantu model untuk memperhatikan berbagai kata dalam input sekaligus, sehingga memahami hubungan antar kata yang berbeda.\n",
    "\n",
    "- **Feed-Forward Neural Network**\n",
    "    \n",
    "    Setelah bagian self-attention, representasi dari input sequence diproses oleh neural network untuk memperkuat pemahaman model tentang konteks.\n",
    "\n",
    "â–¶ï¸ **Decoder**\n",
    "\n",
    "**Decoder** merupakan bagian dari transformer yang bertujuan untuk menghasilkan output. Output ini mempertimbangkan input dari encoder. **Decoder** juga terdiri dari beberapa lapisan yang disebut **decoder layer**. Setiap decoder layer memiliki tiga bagian utama:\n",
    "\n",
    "- **Masked Multi-Head Self-Attention Mechanism**\n",
    "\n",
    "    Bagian ini memastikan model hanya memperhatikan kata-kata yang sudah dihasilkan sebelumnya, bukan kata yang belum dihasilkan. Ini penting untuk menjaga urutan kata yang dihasilkan.\n",
    "- **Multi-Head Attention Mechanism**\n",
    "\n",
    "    Bagian ini membantu decoder memperhatikan informasi dari encoder, sehingga output yang dihasilkan mempertimbangkan seluruh input.\n",
    "- **Feed-Forward Neural Network**\n",
    "\n",
    "    Setelah melalui kedua bagian attention, representasi dari sequence diproses oleh neural network untuk memperkuat pemahaman model tentang konteks.\n",
    "\n",
    "Berikut merupakan ilustrasi sederhana bagaimana transformer bekerja.\n",
    "\n",
    "![](assets/transformer_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Capability, Limitation, and Consideration\n",
    "\n",
    "Saat ini, LLM banyak dipilih karena keandalan dan kemampuannya untuk memahami konteks yang rumit. Selain itu, LLM juga mampu untuk menghasilkan teks dan narasi yang hampir menyamai produk bahasa yang ditulis oleh manusia. \n",
    "\n",
    "Berikut ini beberapa contoh task yang dapat diaplikasikan oleh LLM.\n",
    "\n",
    "* **Text Generation**: menghasilkan narasi dengan gaya bahasa yang hampir menyamai kemampuan manusia.\n",
    "* **Translation**: menerjemahkan teks ke dalam bahasa yang berbeda secara akurat.\n",
    "* **Summarization**: membuat ringkasan singkat dari teks yang panjang.\n",
    "* **Question-Answering**: menjawab sebuah pertanyaan berdasarkan konteks yang diberikan.\n",
    "\n",
    "Akan tetapi, perlu diperhatikan bahwa saat ini LLM masih menjadi topik riset yang aktif. Para peneliti masih mengembangkan metode yang lebih advanced untuk mengurangi beberapa limitasi dari LLM, seperti:\n",
    "\n",
    "* **Misleading Output** \n",
    "    \n",
    "    Sebagai produk AI yang sangat powerful, output yang dihasilkan oleh LLM dapat terlihat sangat meyakinkan. Akan tetapi, sebagai pengguna, kita perlu bijak dalam mengonsumsi konten yang dibuat oleh AI. Secara aktif, kita perlu melakukan verifikasi fakta melalui sumber-sumber yang kredibel.\n",
    "\n",
    "* **Technical Limitation**\n",
    "\n",
    "    Di balik kapabilitas LLM dalam task yang melibatkan natural language, masih terdapat concern terkait limitasi teknis. Terdapat situasi di mana AI tidak dapat memahami konteks secara mendalam atau menghasilkan respons yang sesuai dengan harapan pengguna. Selain itu, LLM memerlukan sumber daya komputasi yang sangat tinggi untuk berfungsi secara optimal, yang dapat menjadi hambatan bagi pengguna dengan keterbatasan infrastruktur teknologi.\n",
    "\n",
    "* **Laws and Rights-Related Issue**\n",
    "\n",
    "    Sebagai teknologi yang terus berkembang, penggunaan LLM juga dihadapkan pada isu-isu hukum dan hak. Misalnya, hak cipta, privasi, dan perlindungan data adalah aspek-aspek penting yang perlu dipertimbangkan saat menggunakan AI. \n",
    "    \n",
    "    Pengguna harus memastikan bahwa mereka mematuhi semua regulasi yang relevan dan menghormati hak-hak individu dan entitas lain dalam penggunaan dan penyebaran konten yang dihasilkan oleh AI.\n",
    "\n",
    "Mempertimbangkan kelebihan dan limitasi yang ada, sebagai end-user, kita perlu cermat, selektif, dan aware dalam mempergunakan LLM. Kemampuan LLM yang powerful akan semakin maksimal selama dibersamai dengan konsiderasi berikut:\n",
    "\n",
    "* **Tidak Menganggap Output LLM sebagai Final Result** \n",
    "    \n",
    "    Output dari LLM tidak dijadikan sebagai hasil akhir, melainkan hanya sebagai pendukung tersier. Kita perlu tetap melakukan verifikasi dan cross-check dengan sumber lain yang terpercaya untuk memastikan akurasi dan relevansi informasi.\n",
    "\n",
    "* **Keputusan Kompleks dan Terkait Makhluk Hidup**\n",
    "    \n",
    "    Keputusan yang diambil dari output LLM tidak digunakan dalam pertimbangan yang kompleks dan menyangkut keperluan yang berkaitan dengan makhluk hidup, seperti keputusan medis, hukum, atau keuangan, tanpa konsultasi dengan ahli yang kompeten di bidang tersebut.\n",
    "\n",
    "* **Evaluasi Kritis dan Penyesuaian Konteks**\n",
    "    \n",
    "    Selalu melakukan evaluasi kritis terhadap output yang dihasilkan oleh LLM, termasuk memahami konteks dan bias yang mungkin ada dalam data pelatihannya. Penyesuaian dan interpretasi yang bijak diperlukan untuk memastikan bahwa informasi tersebut sesuai dengan konteks yang dibutuhkan.\n",
    "\n",
    "* **Keamanan dan Privasi Data**\n",
    "\n",
    "    Memperhatikan aspek keamanan dan privasi data saat menggunakan LLM. Jangan memasukkan informasi pribadi atau sensitif yang dapat disalahgunakan, dan pastikan bahwa penggunaan LLM mematuhi kebijakan privasi dan regulasi yang berlaku.\n",
    "\n",
    "> ðŸ“Œ More info: [Responsible AI](https://ai.google/responsibility/responsible-ai-practices/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChainðŸ¦œðŸ”—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Concept and Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦œðŸ”—**LangChain** merupakan sebuah framework yang dapat digunakan untuk membuat aplikasi berbasis LLM. Dengan LangChain, pengguna dapat mengakses berbagai jenis LLM yang disediakan oleh provider seperti OpenAI, Google, dan HuggingFace. Selain itu, framework ini juga memungkinkan kita untuk membuat aplikasi custom dengan memanfaatkan kemampuan LLM. Fitur- fitur ini menjadikan LangChain sebagai package yang cukup powerful. \n",
    "\n",
    "> ðŸ“Œ[LangChain Official Documentation](https://python.langchain.com/v0.1/docs/get_started/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Seperti namanya, ide dasar dari LangChain adalah men-chaining beberapa komponen menjadi satu kesatuan untuk menjalankan fungsi tertentu. Dengan menggabungkan berbagai komponen ini, kita bisa membuat alur kerja yang kompleks dan dinamis.\n",
    "\n",
    "Secara umum, berikut adalah pengelompokan komponen LangChain berdasarkan fungsinya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ **Model I/O**\n",
    "\n",
    "* Mengatur input dan output dari sebuah LLM.\n",
    "* Komponen:\n",
    "    * **Prompt**: memberikan instruksi terkait output seperti apa yang harus dihasilkan.\n",
    "    * **LLM dan Chat Model**: memberikan output berdasarkan prompt.\n",
    "\n",
    "2ï¸âƒ£ **Retrievers**\n",
    "\n",
    "* Mengambil data yang relevan untuk digunakan dalam LLM (biasanya melibatkan dokumen eksternal).\n",
    "* Komponen:\n",
    "    * **Document Loader**: untuk memuat dokumen eksternal.\n",
    "    * **Text Splitter**: untuk pemrosesan dokumen.\n",
    "    * **Embedding Model**: untuk merepresentasikan dokumen dalam bentuk numerik (embedding).\n",
    "    * **Vectorstore**: untuk menyimpan embedding\n",
    "    * **Retrievers**: mencari dan mengembalikan data yang paling relevan dari vectorstores berdasarkan query pengguna.\n",
    "\n",
    "3ï¸âƒ£ **Composition**\n",
    "\n",
    "* Gabungan dari komponen basic di atas.\n",
    "* Komponen: tools, agents, chains.\n",
    "\n",
    "4ï¸âƒ£ **Additional**\n",
    "\n",
    "* Komponen: memory, callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Concept and Setting for LangChain Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/api_analogy.png)\n",
    "\n",
    "Provider LLM seperti OpenAI dan Google menyediakan fungsionalitas LLM mereka dalam bentuk **Application Programming Interface (API)**. API merupakan sebuah protokol atau program yang memungkinkan beberapa komputer untuk saling berkomunikasi.\n",
    "\n",
    "> ðŸ’¡ Apabila dianalogikan, API merupakan pelayan di sebuah restoran. Kita sebagai pelanggan memesan makanan melalui pelayan. Selanjutnya, pelayan akan menyampaikan pesanan kita kepada chef (provider LLM) di dapur dan chef akan membuatkan pesanan tersebut. Pesanan yang sudah jadi akan diantarkan kembali melalui pelayan.\n",
    "\n",
    "Untuk dapat berinteraksi dengan API dari provider LLM, kita memerlukan sebuah **API key** (ID untuk pemesanan). API key ini digunakan setiap kali kita berinteraksi dengan LLM yang disediakan oleh provider. Pada workshop ini, kita akan menggunakan **Gemini sebagai LLM**.\n",
    "\n",
    "> ðŸ”‘ Find your [Gemini API key here](https://ai.google.dev/gemini-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena API key bersifat rahasia, API key jarang dimasukkan langsung ke dalam kode program. Salah satu cara terbaik untuk memuat API key secara otomatis adalah dengan membuat file `.env` dan menggunakan fungsi `load_dotenv()` dari package `dotenv` untuk memanggil file tersebut. \n",
    "\n",
    "Berikut adalah contoh isi dari file `.env`.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY = \"your_OPENAI_API_key\"\n",
    "GOOGLE_API_KEY = \"your_GOOGLE_API_key\"\n",
    "HUGGINGFACEHUB_API_TOKEN = \"your_HuggingFace_API_key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk meng-import fungsi load_dotenv() dari package dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load file .env secara otomatis\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of LLM Usage with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sejauh ini, kita sudah mengetahui beberapa dasar terkait LLM, konsep dasar LangChain, dan komponen dari LangChain. Selanjutnya, mari kita lihat bagaimana implementasi sederhana penggunaan LLM dengan LangChain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Your First Call to Gemini â™Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan pertama ini, kita akan:\n",
    "\n",
    "* Membuat objek LLM.\n",
    "* Mencoba beberapa varian Gemini yang tersedia.\n",
    "* Menanyakan pertanyaan dengan method `.invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASANI\\anaconda3\\envs\\dss_may2024\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat objek LLM dengan beberapa varian yang ada\n",
    "bison = GoogleGenerativeAI(model=\"models/text-bison-001\")\n",
    "gemini_10_pro = GoogleGenerativeAI(model=\"models/gemini-pro\") # also refer to gemini-1.0-pro\n",
    "gemini_15_pro = GoogleGenerativeAI(model=\"models/gemini-1.5-pro-latest\") # beware of 2 RPM limit\n",
    "# gemini_15_flash = GoogleGenerativeAI(model=\"gemini-1.5-flash\") --> still not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joko Widodo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bison.invoke('Who is the president of Indonesia?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joko Widodo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_10_pro.invoke('Who is the president of Indonesia?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The president of Indonesia is **Joko Widodo**, often referred to as **Jokowi**. \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_15_pro.invoke('Who is the president of Indonesia?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Add Prompt to Boost Gemini's Response ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan kedua ini, kita akan menambahkan prompt untuk membuat implementasi yang lebih dinamis dan custom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita akan membuat sebuah template untuk menerjemahkan sebuah kalimat ke dalam bahasa Inggris dengan gaya bahasa yang informal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template = '''\n",
    "                        Translate the following sentence into informal English:\n",
    "                        {sentence}\n",
    "                        ''',\n",
    "                        input_variables = [\"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita menggunakan operator `|` untuk membuat sebuah chain. Chain tersebut terbentuk dari komponen prompt dan LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain =  prompt | gemini_10_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terakhir, kita akan mencoba untuk memberikan sebuah pertanyaan melalui method `.invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll be back in 10\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    'sentence' : 'Saya akan kembali 10 menit lagi'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Prompt + Gemini via `LLMChain()` â›“ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan sebelumnya, kita menggunakan dictionary `{ }` pada method `.invoke()` untuk memberikan pertanyaan kepada LLM. Pendekatan tersebut masih kurang fleksibel di mana pengguna masih perlu secara manual mendefinisikan dictionary yang berisi pertanyaan.\n",
    "\n",
    "Pada eksperimen ketiga ini, kita akan mengenal `LLMChain()`. `LLMChain()` memudahkan kita untuk bertanya dengan LLM dengan hanya mendifinisikan:\n",
    "\n",
    "* LLM apa yang akan digunakan.\n",
    "* Prompt seperti apa yang diinginkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASANI\\anaconda3\\envs\\dss_may2024\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm = gemini_10_pro,\n",
    "                     prompt = prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan kode di atas, kita tidak perlu mendefinisikan pertanyaan dengan dictionary `{ }`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Di mana terakhir kali kita bertemu?',\n",
       " 'text': 'Where was the last time we bumped into each other?'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke('Di mana terakhir kali kita bertemu?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Q&A and Summarization for PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤” Coba tanyakan pertanyaan berikut kepada LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joko Widodo'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bison.invoke('Who is the winner of presidential election in Indonesia for 2024-2029?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pemilu 2024 di Indonesia belum dilaksanakan, sehingga pemenangnya belum diketahui.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_10_pro.invoke('Siapa pemenang Pemilu 2024 di Indonesia?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maaf, aku tidak bisa memprediksi masa depan, termasuk hasil Pemilu 2024 di Indonesia. Pemilihan masih beberapa tahun lagi, dan banyak faktor yang dapat memengaruhi hasilnya. \\n\\nSemoga informasi ini membantu! \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_15_pro.invoke('Siapa pemenang Pemilu 2024 di Indonesia?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meskipun memiliki kemampuan yang powerful untuk mengerjakan task yang berkaitan dengan bahasa, LLM memiliki **knowledge cutoff**. Knowledge cutoff merupakan batas pengetahuan yang dimiliki oleh LLM karena resource untuk men-training LLM berhenti pada titik waktu tertentu. \n",
    "\n",
    "> â„¹ï¸ Untuk Gemini, knowledge cutoff-nya adalah awal 2023.\n",
    "\n",
    "Dengan demikian, pengetahuan LLM tentang peristiwa dan informasi setelah knowledge cutoff tersebut mungkin terbatas atau tidak ada sama sekali.\n",
    "\n",
    "Untuk mengatasi permasalahan tersebut, kita dapat menggunakan **Retrieval Augmented Generation** (RAG). RAG bekerja dengan cara menyediakan konteks tambahan untuk LLM, seperti dokumen, website, dan database. Dengan RAG, LLM dapat mengakses dan memproses data terbaru, sehingga dapat memberikan jawaban yang lebih akurat dan terkini. Selain itu, pendekatan ini juga memudahkan kita untuk memanfaatkan LLM pada resource dan use case yang lebih spesifik.\n",
    "\n",
    "Pada workshop ini, kita akan mencoba untuk **menggunakan dokumen PDF sebagai konteks tambahan untuk LLM**. Dokumen ini merupakan sebuah paper yang berjudul [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223) dengan tebal 124 halaman. Paper ini menyediakan review yang komprehensif terkait LLM. Paper yang sama juga dapat Anda temui pada folder `data_input` dengan nama file `llm_survey.pdf`.\n",
    "\n",
    "Selanjutnya, kita akan **membuat chain** dengan LangChain yang memungkinkan kita untuk **bertanya seputar konten paper tersebut**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "Untuk membuat sebuah implementasi Q&A dokumen PDF menggunakan LangChain, kita akan mengikuti workflow berikut ini.\n",
    "\n",
    "![](assets/rag_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita perlu memuat dokumen yang menjadi konteks tambahan bagi LLM. Dalam hal ini, kita menggunakan `PyPDFLoader()` untuk memuat dokumen PDF. Selain PDF, LangChain menyediakan fungsionalitas untuk memuat dokumen dengan ekstensi lain, seperti CSV dan JSON.\n",
    "\n",
    "> ðŸ“Œ [LangChain Document Loaders](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk loading file PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader('data_input/llm_survey.pdf',\n",
    "                         extract_images = True)\n",
    "pdf_data = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan kode di atas, dokumen PDF akan di-load sebagai list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List ini akan memiliki elemen sebanyak halaman dokumen PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setiap elemen pada list berisikan konten pada halaman tertentu. Misalnya, kita ingin melihat konten pada halaman ke-5 (indeks ke-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "given an increase in compute budget, the KM scaling law\n",
      "favors a larger budget allocation in model size than the data\n",
      "size, while the Chinchilla scaling law argues that the two\n",
      "sizes should be increased in equal scales, i.e.,having similar\n",
      "values for aandbin Equation (3).\n",
      "Discussion on Scaling Laws . After introducing the formu-\n",
      "lations, we continue to discuss scaling law in the following\n",
      "two aspects, to enhance its understanding:\n",
      "â€¢Predictable scaling . In practice, scaling law can be used\n",
      "to instruct the training of LLMs, and it has been proven\n",
      "feasible to reliably estimate the performance of larger mod-\n",
      "els based on that of smaller models, called predictable scal-\n",
      "ing [46]. The benefits of predictable scaling for training\n",
      "LLMs are mainly twofold. Firstly, for large models, it is\n",
      "infeasible to rigorously examine various training tricks or\n",
      "variants, and it would be very helpful if experiences gained\n",
      "from small models could also apply to large models. For\n",
      "instance, small proxy models can be trained to find the\n",
      "optimal schedule of the data mixture for large models [59].\n",
      "Secondly, the training of large-scale models takes a long\n",
      "time, often suffering from issues such as training loss spike,\n",
      "and scaling law can be employed to monitor the training\n",
      "status of LLMs, e.g.,identifying abnormal performance at an\n",
      "early time. Despite that scaling law characterizes a smooth\n",
      "trend of performance increase (or loss decrease), it also\n",
      "indicates that diminishing returns7might occur as model\n",
      "scaling. An empirical study [58] from the OpenAI team\n",
      "has shown that representation quality or semantic content\n",
      "can still effectively improve even if approaching the point\n",
      "of diminishing returns ( i.e., approaching the irreducible\n",
      "loss) [58]. This finding suggests that training large models\n",
      "are promising for improving the performance of down-\n",
      "stream tasks. To further explore scaling effect, a potential\n",
      "issue is that the amount of available data for training LLMs\n",
      "is actually limited. With the ever-increasing model scale, the\n",
      "public text data would be soon â€œexhaustedâ€ for LLMs [60].\n",
      "Thus, it will be meaningful to study how scaling laws apply\n",
      "to a data-constrained regime [61], where data repetition or\n",
      "augmentation might be useful to alleviate data scarcity.\n",
      "â€¢Task-level predictability . Existing research of scaling laws\n",
      "are mostly conducted in terms of language modeling loss\n",
      "(e.g., per-token cross-entropy loss in nats [30]), while in\n",
      "practice we are more concerned about the performance of\n",
      "LLMs on actual tasks. Thus, a basic problem is that how\n",
      "the decrease of language modeling loss translates into the\n",
      "improvement of task performance [58]. Intuitively, a model\n",
      "with a smaller language modeling loss tends to yield a\n",
      "better performance on downstream tasks, since language\n",
      "modeling loss can be considered as a general measure of\n",
      "the overall model capacity. GPT-4 [46] has reported that\n",
      "some capabilities ( e.g., coding ability) can be accurately\n",
      "predicted via scaling law. Despite that, readers should be\n",
      "aware that a direct decrease in language modeling loss does\n",
      "not always indicate an improvement of model performance\n",
      "on downstream tasks. Specially, the phenomenon of inverse\n",
      "scaling would occur for some tasks, where task performance\n",
      "surprisingly becomes worse as the language modeling loss\n",
      "decreases [62]. Overall, it is more difficult to explore and\n",
      "7. https://en.wikipedia.org/wiki/Diminishing returnscharacterize task-level scaling laws, since it might be also\n",
      "dependent on task-related information (task metric, task\n",
      "difficulty, etc.). Furthermore, some capacities ( e.g.,in-context\n",
      "learning [55]) are unpredictable according to the scaling law,\n",
      "which can be observed only when the model size exceeds a\n",
      "certain level (as discussed below).\n",
      "Emergent Abilities of LLMs . In the literature [31], emergent\n",
      "abilities of LLMs are formally defined as â€œthe abilities that\n",
      "are not present in small models but arise in large modelsâ€,\n",
      "which is one of the most prominent features that distin-\n",
      "guish LLMs from previous PLMs. It further introduces a\n",
      "notable characteristic when emergent abilities occur [31]:\n",
      "performance rises significantly above random when the\n",
      "scale reaches a certain level. By analogy, such an emergent\n",
      "pattern has close connections with the phenomenon of phase\n",
      "transition in physics [31, 63]. In principle, emergent abilities\n",
      "can be defined in relation to some complex tasks [31, 64],\n",
      "while we are more concerned with general abilities that\n",
      "can be applied to solve a variety of tasks. Here, we briefly\n",
      "introduce three typical emergent abilities for LLMs and\n",
      "representative models that possess such an ability8.\n",
      "â€¢In-context learning. The in-context learning (ICL) ability\n",
      "is formally introduced by GPT-3 [55]: assuming that the\n",
      "language model has been provided with a natural language\n",
      "instruction and/or several task demonstrations, it can gen-\n",
      "erate the expected output for the test instances by com-\n",
      "pleting the word sequence of input text, without requiring\n",
      "additional training or gradient update9. Among the GPT-\n",
      "series models, the 175B GPT-3 model exhibited a strong ICL\n",
      "ability in general, but not the GPT-1 and GPT-2 models. Such\n",
      "an ability also depends on the specific downstream task. For\n",
      "example, the ICL ability can emerge on the arithmetic tasks\n",
      "(e.g., the 3-digit addition and subtraction) for the 13B GPT-3,\n",
      "but 175B GPT-3 even cannot work well on the Persian QA\n",
      "task [31].\n",
      "â€¢Instruction following. By fine-tuning with a mixture of\n",
      "multi-task datasets formatted via natural language descrip-\n",
      "tions (called instruction tuning ), LLMs are shown to perform\n",
      "well on unseen tasks that are also described in the form\n",
      "of instructions [28, 66, 67]. With instruction tuning, LLMs\n",
      "are enabled to follow the task instructions for new tasks\n",
      "without using explicit examples, thus having an improved\n",
      "generalization ability. According to the experiments in [67],\n",
      "instruction-tuned LaMDA-PT [68] started to significantly\n",
      "outperform the untuned one on unseen tasks when the\n",
      "model size reached 68B, but not for 8B or smaller model\n",
      "sizes. A recent study [69] found that a model size of 62B is\n",
      "at least required for PaLM to perform well on various tasks\n",
      "in four evaluation benchmarks ( i.e.,MMLU, BBH, TyDiQA\n",
      "and MGSM), though a much smaller size might suffice for\n",
      "some specific tasks ( e.g., MMLU).\n",
      "â€¢Step-by-step reasoning. For small language models, it\n",
      "is usually difficult to solve complex tasks that involve\n",
      "8. It is difficult to accurately examine the critical size for emergent\n",
      "abilities of LLMs ( i.e.,the minimum size to possess an ability), since it\n",
      "might vary for different models or tasks. Also, existing studies often\n",
      "test emergent abilities on very limited model sizes for a specific LLM.\n",
      "For example, PaLM is often tested with three sizes of 8B, 62B and 540B.\n",
      "It is unclear about the model performance of the untested sizes.\n",
      "9. In a recent study [65], it also shows that in-context learning implic-\n",
      "itly performs meta-optimization through the attention mechanism.\n"
     ]
    }
   ],
   "source": [
    "print(pdf_data[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita akan melakukan proses splitting. Proses ini bertujuan untuk membuat chunk yang lebih kecil dari konten yang sudah diekstrak sebelumnya. Chunk-chunk yang lebih kecil akan lebih mudah di-maintain, disimpan, dan diproses oleh LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk split konten file PDF menjadi chunk yang lebih kecil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada proses splitting ini, kita menggunakan `RecursiveCharacterTextSplitter()`. Dengan `RecursiveCharacterTextSplitter()`, proses splitting dilakukan dengan tetap menjaga bagian-bagian teks yang terkait tetap berdekatan satu sama lain. Merujuk pada [dokumentasi LangChain](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/), teknik splitting dengan `RecursiveCharacterTextSplitter()` adalah teknik yang direkomendasikan untuk mulai melakukan splitting dari sebuah teks yang cukup besar.\n",
    "\n",
    "> ðŸ“Œ [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)\n",
    "\n",
    "Pada kode di bawah ini, kita mendefinisikan output dari proses splitting:\n",
    "\n",
    "* Setiap chunk akan memiliki maksimal sebanyak 1000 karakter.\n",
    "* Antara satu chunk dengan chunk yang lain memiliki kesamaan sebanyak 450 karakter. Hal ini didefinisikan untuk mempertahankan konteks antara satu chunk dengan chunk yang lain. \n",
    "* Proses splitting dilakukan berdasarkan daftar karakter yang kita definisikan pada parameter `separators`.\n",
    "\n",
    "Selanjutnya, kita memasukkan list `pdf_data` sebagai input untuk method `.split_documents()`. Method ini akan membuat chunk yang lebih kecil dari `pdf_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 450,\n",
    "    separators=[\n",
    "                \"\\n\\n\",\n",
    "                \"\\n\",\n",
    "                \" \",\n",
    "                \".\",\n",
    "                \",\",\n",
    "                \"\\u200b\",  # zero-width space\n",
    "                \"\\uff0c\",  # full-width comma\n",
    "                \"\\u3001\",  # ideographic comma\n",
    "                \"\\uff0e\",  # full-width full stop\n",
    "                \"\\u3002\",  # ideographic full stop\n",
    "                \"\",\n",
    "            ])\n",
    "\n",
    "splits = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di akhir proses ini, kita mendapatkan 1316 chunk hasil splitting dari 124 elemen pada `pdf_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di balik layar, operasi pada komputer merupakan operasi yang melibatkan angka. Agar teks yang kita miliki dapat dimengerti oleh komputer, kita perlu membuat representasi numerik dari teks tersebut. Proses ini dibut dengan **embedding**.\n",
    "\n",
    "LangChain menyediakan banyak pilihan embedding, tergantung dari LLM yang akan kita gunakan:\n",
    "* `GoogleGenerativeEmbeddings()` untuk model LLM yang dikembangkan oleh Google.\n",
    "* `OpenAIEmbeddings()` untuk model LLM dari OpenAI.\n",
    "* `HuggingFaceEmbeddings()` untuk model LLM dari HuggingFace.\n",
    "\n",
    "> ðŸ“Œ [LangChain Embedding Models](https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/)\n",
    "\n",
    "Representasi numerik dari teks ini selanjutnya disimpan di dalam **vector database**. \n",
    "> âœ¨ Sederhananya, kita menyimpan konten file PDF di database dalam bentuk angka.\n",
    "\n",
    "Vector database yang akan kita gunakan adalah **Chroma**. Chroma merupakan vector database yang cukup powerful untuk menyimpan unstructured data, seperti konten pada file PDF. Chroma juga dilengkapi dengan algoritma yang efisien dan akurat untuk pencarian informasi dari embedding yang disimpan. Selain itu, Chroma juga memungkinkan kita untuk menyimpan objek database sehingga dapat digunakan kembali untuk keperluan yang akan datang.\n",
    "\n",
    "> ðŸ“Œ [LangChain Vector Stores](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk membuat embedding\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # embedding Google Generative AI\n",
    "\n",
    "# untuk menyimpan embedding\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode dibawah ini, kita mendefinisikan sebuah fungsi untuk:\n",
    "\n",
    "* Mengubah dokumen menjadi embedding. \n",
    "* Menyimpan hasil embedding ke dalam Chroma.\n",
    "* Menyimpan file database ke suatu folder agar dapat digunakan kembali.\n",
    "\n",
    "Penjelasan parameter:\n",
    "\n",
    "* `documents `: dokumen yang akan diubah menjadi embedding.\n",
    "* `embbeding`: fungsi yang digunakan untuk membuat embedding, misalnya `GoogleGenerativeAIEmbeddings()` jika menggunakan Gemini sebagai LLM.\n",
    "* `persist_directory`: folder untuk menyimpan vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore_folder(documents, embedding, persist_directory):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents = documents, \n",
    "        embedding = embedding,\n",
    "        persist_directory = persist_directory\n",
    "    )\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_gemini = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
    "#     persist_directory = 'data_input/chroma_gemini'\n",
    "# )\n",
    "\n",
    "# time spent: 40.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses mengubah dokumen menjadi embedding dan menyimpannya ke dalam database dapat berjalan lama karena dapat dipengaruhi oleh berbagai faktor, mulai dari besarnya dokumen sampai dengan koneksi internet. Oleh karena itu, pada workshop ini, proses pembuatan vector database sudah dilakukan terlebih dahulu. Penyimpanan vector database ini terdapat pada folder `data_input`. \n",
    "\n",
    "Untuk me-load kembali vector database dari folder yang sudah ada, Anda dapat menjalankan kode di bawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memanggil embedding dari directory yang sudah disimpan\n",
    "vec_gemini = Chroma(persist_directory = 'data_input/chroma_gemini',\n",
    "                    embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita telah membuat sebuah vector database yang menjadi sumber bagi LLM untuk mencari informasi. Selanjutnya, kita akan membuat sebuah chain yang memungkinkan kita untuk bertanya kepada LLM. LLM akan memberikan jawaban berdasarkan informasi yang terdapat pada vector database.\n",
    "\n",
    "> ðŸ¤¨ **Bagaimanaca LLM memberikan jawaban yang sesuai?** Pada saat kita memberikan pertanyaan (query) kepada LLM, pertanyaan tersebut akan diubah menjadi representasi numerik (vector). Representasi numerik ini akan digunakan untuk mencari jawaban yang sesuai berdasarkan informasi pada vector database. Proses ini melibatkan perhitungan vector similarity antara vector query dan vector informasi yang terdapat dalam vector database. Informasi dengan tingkat similarity yang tinggi akan di-return sebagai jawaban."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/rag_workflow2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# untuk mempersiapkan prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# untuk memasukkan pertanyaan\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# untuk menampilkan output yang diinginkan\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# untuk menampilkan output secara rapi\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    You are the great assistant in understanding additional context\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    Use the minimum of three sentences to answer the question. \n",
    "    Try your best to answer as complete as possible with easy style of English.\n",
    "    Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(retriever, llm):\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode di atas, kita membuat sebuah fungsi untuk membuat chain untuk Q&A. Chain untuk Q&A di atas terdiri atas beberapa komponen:\n",
    "\n",
    "*  `custom_rag_prompt` yang merupakan prompt untuk mengarahkan output dari LLM.\n",
    "* `llm` yang akan berinteraksi dengan pengguna.\n",
    "* `StrOutputParser()` yang akan menangkap output dari LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan pada saat kita mendefinisikan template prompt, terdapat 2 placeholder: `{context}` dan `{question}`.\n",
    "\n",
    "* Value untuk `{context}` berasal dari vector database.\n",
    "* Value untuk `{question}` berasal dari pertanyaan pengguna pada saat menjalankan chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_chain = create_chain(\n",
    "    retriever = vec_gemini.as_retriever(),\n",
    "    llm = ChatGoogleGenerativeAI(model = 'gemini-pro')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang, kita sudah membuat chain untuk Q&A. Mari kita tes chain tersebut dengan memberi beberapa pertanyaan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLaMA model family consists of a variety of popular models, including LLaVA,\n",
      "MiniGPT-4, InstructBLIP, and PandaGPT. The release of LLaMA has greatly advanced the\n",
      "research progress of LLMs. Many multimodal models incorporate LLaMA models as the base\n",
      "language models to achieve strong language understanding and generation abilities. Vicuna\n",
      "is a popular LLaMA variant that is often used in multimodal models. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        gemini_chain.invoke('Describe about llaMa model family'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-1 was released in 2018 and was trained with a hybrid approach of unsupervised\n",
      "pretraining and supervised fine-tuning. It established the core architecture for the GPT-\n",
      "series models and the underlying principle to model natural language text, predicting the\n",
      "next word. GPT-2 increased the parameter scale to 1.5B and was trained with a large\n",
      "webpage dataset WebText. It sought to perform tasks via unsupervised language modeling,\n",
      "without explicit fine-tuning using labeled data and introduced a probabilistic form for\n",
      "multi-task solving.  Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        gemini_chain.invoke('How is the technical evolution of GPT-series model?'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A dengan Model AI Lainnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "# LLM\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_open_ai = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "#     persist_directory = 'data_input/chroma_open_ai'\n",
    "# )\n",
    "\n",
    "# time spent: 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memanggil embedding dari directory yang sudah disimpan\n",
    "\n",
    "vec_openai = Chroma(persist_directory = 'data_input/chroma_open_ai',\n",
    "                    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat custom chain untuk Q&A dengan LLM OpenAI\n",
    "\n",
    "openai_chain = create_chain(\n",
    "    retriever = vec_openai.as_retriever(),\n",
    "    llm = ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLaMA model family was introduced by Meta AI in February 2023, consisting of four\n",
      "sizes: 7B, 13B, 30B, and 65B. These models have garnered significant attention from both\n",
      "research and industry communities due to their exceptional performance on various\n",
      "benchmarks. Many researchers have extended the LLaMA models through instruction tuning or\n",
      "continual pretraining, making them popular for developing customized models with\n",
      "relatively low computational costs. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "# Q&A implementation\n",
    "\n",
    "print(\n",
    "    textwrap.fill(\n",
    "        openai_chain.invoke('Describe about llaMa model family'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The technical evolution of the GPT-series models has been marked by significant\n",
      "advancements in model size, capabilities, and architecture. Starting from GPT-1 with a\n",
      "decoder-only architecture for generative pre-training, the series progressed to GPT-2 with\n",
      "increased parameter scale and unsupervised language modeling. Subsequent models like GPT-3\n",
      "introduced concepts like in-context learning and scaled model parameters to 175B, leading\n",
      "to improved communication with humans and enhanced reasoning abilities. The evolution\n",
      "culminated in models like GPT-4 Turbo with vision and ChatGPT with multimodal abilities,\n",
      "showcasing a continuous progression towards more powerful and versatile language models.\n",
      "Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        openai_chain.invoke('How is the technical evolution of GPT-series model?'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤— HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    " # embedding HuggingFace \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# LLM\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_hf = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "#     persist_directory = 'data_input/chroma_hugging_face'\n",
    "# )\n",
    "\n",
    "# time spent: 6m 24.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_hf = Chroma(persist_directory = 'data_input/chroma_hugging_face',\n",
    "                    embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASANI\\anaconda3\\envs\\dss_may2024\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# membuat custom chain untuk Q&A dengan LLM dari HuggingFace\n",
    "\n",
    "hf_chain = create_chain(\n",
    "    retriever = vec_hf.as_retriever(),\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=\"declare-lab/flan-alpaca-large\",\n",
    "        task=\"text-generation\",\n",
    "        model_kwargs={\"temperature\":0.3, \"max_length\":1000}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLaMA model family is a powerful language model family that is widely used in many\n",
      "applications. It is based on the LLaMA architecture, which is a supervised learning\n",
      "algorithm that is trained on a large dataset of language data. It is able to generate\n",
      "strong language understanding and generation abilities, compared to other variants. It is\n",
      "also able to learn from user-shared conversations, which makes it suitable for multimodal\n",
      "tasks.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        hf_chain.invoke('Describe about llaMa model family'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The technical evolution of GPT-series models is a gradual process of improvement. The\n",
      "model has evolved over time, with new features and improvements being added to the model.\n",
      "The model has also been improved by scaling the model parameters, which has allowed for a\n",
      "key capacity leap. The model has also been improved by incorporating more features and\n",
      "incorporating more advanced techniques. Finally, the model has been improved by\n",
      "incorporating more advanced techniques such as in-context learning and reinforcement\n",
      "learning.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        hf_chain.invoke('How is the technical evolution of GPT-series model?'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dss_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
