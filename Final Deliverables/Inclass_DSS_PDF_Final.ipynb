{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSS May 2024: Efficient Information Extraction: Q&A and Summarization over PDF Documents using LLM\n",
    "\n",
    "* Instruktur: [Saskia Dwi Ulfah](https://www.linkedin.com/in/saskia-dwi-ulfah/).\n",
    "* Last updated: May 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìï Dokumen yang disimpan dalam format **Portable Document Format (PDF)** merupakan salah satu bentuk dokumen yang sering digunakan untuk bertukar informasi melalui internet dan perangkat digital seperti handphone, laptop, dan komputer. PDF juga dimanfaatkan di berbagai sektor. Di bidang pendidikan, mahasiswa dapat mengakses jurnal penelitian dengan format PDF melalui website seperti Elsevier dan IEEE. Di bidang finansial, perusahaan menampilkan laporan keuangan tahunan dalam format PDF pada website perusahaan.\n",
    "\n",
    "\n",
    "‚õ≥ Format PDF merupakan format yang universal. Artinya, dokumen yang disimpan dalam format PDF dapat diakses secara mudah pada perangkat yang berbeda. Selain itu, format PDF lebih disukai karena tampilan dokumen yang lebih rapi dibandingkan format dokumen lainnya. PDF juga men-support dokumen yang terdiri lebih dari satu halaman. Hal ini memungkinkan pengguna untuk menuliskan informasi yang lengkap dan komprehensif dalam sebuah dokumen PDF. Akan tetapi, hal ini membuat **dokumen PDF cenderung tebal dan kompleks** sehingga menyebabkan pembaca **kesulitan untuk menemukan informasi yang spesifik**.\n",
    "\n",
    "üí° Dengan perkembangan teknologi artificial intelligence  (AI) dan machine learning (ML), kita dapat menggunakan **Large Language Model (LLM)** untuk pencarian informasi pada dokumen PDF. Pada workshow ini, Anda akan belajar bagaimana kita dapat memperluas kemampuan LLM untuk pencarian informasi secara efisien dari dokumen PDF. Dimulai dengan dokumen PDF biasa, Anda akan belajar cara memproses dokumen ini dan menyajikannya sebagai konteks tambahan untuk LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Setelah menyelesaikan workshop ini, Anda diharapkan dapat:\n",
    "\n",
    "* Memahami konsep dasar dari LLM.\n",
    "* Mengimplementasikan penggunaan LLM dengan framework LangChain.\n",
    "* Memahami workflow yang digunakan dalam menyediakan additional context untuk LLM.\n",
    "* Mengembangkan skill di bidang AI dan data science dengan menguasai teknik information retrieval dari dokumen PDF menggunakan LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Syllabus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Python Programming Basics** \n",
    "    - Introduction to Python for Data Science.\n",
    "    - Working with Python Environment.\n",
    "    - Working with Notebook.\n",
    "    - Python Fundamental Data Types and Data Structures.\n",
    "    - Understanding Looping Concept in Python.\n",
    "    - Understanding The Creation of Python Function.\n",
    "    - Understanding The Usage of Python Libraries.\n",
    "* **The Fundamentals of LLM**\n",
    "    - The Concept of Generative AI.\n",
    "    - LLM as Generative AI.\n",
    "    - Transformer Architecture in a Nutshell\n",
    "    - LLM Capability, Limitation, and Consideration\n",
    "* **Introduction to LangChain**\n",
    "    - The Big Picture of LangChain Concept and Component\n",
    "    - API Concept and Setting for LangChain Usage\n",
    "    - Demonstration of LLM Usage with LangChain\n",
    "* **Case Study: Q&A and Summarization for PDF Document**\n",
    "    - The Concept of RAG (Retrieval Augmented Generation)\n",
    "    - Loading PDF Documents using LangChain\n",
    "    - The Concept of Embedding for PDF Documents\n",
    "    - Storing The Embedding using a Vector Database\n",
    "    - Prompt Creation for Q&A and Summarization Cases\n",
    "    - Employing LLM for Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**START OF DAY 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Programming Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Python for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üêç **Python** merupakan bahasa pemrograman yang banyak digunakan untuk data science dan artificial intelligence. Bahasa pemrograman ini dirancang oleh Guido van Rossum dan pertama kali dirilis pada tahun 1991.\n",
    "\n",
    "Beberapa fitur yang menjadi keunggulan Python:\n",
    "- Sintaks yang intuitif dan mudah dipelajari.\n",
    "- Ketersediaan library dan framework yang kaya dan informatif. Beberapa library yang sering digunakan di Python.\n",
    "    * Pandas: untuk mengolah data dalam bentuk dataframe.\n",
    "    * Numpy: untuk proses matematika yang melibatkan matriks.\n",
    "    * Matplotlib dan Seaborn: untuk visualisasi data.\n",
    "    * Scikit-learn: untuk membuat model machine learning.\n",
    "    * PyTorch dan Tensorflow: untuk membuat model deep learning.\n",
    "    * **Langchain**: untuk membuat aplikasi berbasis LLM.\n",
    "\n",
    "> üìå [Python 3.10 Official Documentation](https://docs.python.org/3.10/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/venv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üè§ **Virtual environment** adalah environment terisolasi yang memungkinkan setiap environment memiliki instalasi dan versi package yang khusus dan berbeda. Kita bisa menggunakan virtual environment saat memiliki banyak projek di mana setiap projek membutuhkan package dengan versi yang spesifik. Dengan menggunakan virtual environment, kita bisa menjalankan berbagai macam projek pada satu device yang sama tanpa perlu khawatir akan adanya permasalahan yang timbul karena perbedaan versi package (dependency conflict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚öôÔ∏è **Membuat Virtual Environment Baru**\n",
    "\n",
    "1. Buka `Anaconda Prompt`. \n",
    "\n",
    "![](assets/anaconda_pr.png)\n",
    "\n",
    "2. Membuat environmet baru dengan nama `dss_may2024` dengan Python versi 3.10.\n",
    "   * Sintaks: `conda create -n dss_may2024 python=3.10`.\n",
    "   * Tunggu hingga proses pembuatan virtual environment selesai.\n",
    "\n",
    "![](assets/venv_making.png)\n",
    "\n",
    "3. Aktifkan environment yang sudah dibuat.\n",
    "   * Sintaks: `conda activate dss_may2024`.\n",
    "\n",
    "![](assets/venv_activating.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚öôÔ∏è **Meng-install Library yang Diperlukan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah membuat virtual environment, kita akan meng-install semua library yang akan digunakan pada workshop kali ini. \n",
    "\n",
    "* Meng-install LangChain.\n",
    "  * Sintaks: `conda install langchain -c conda-forge`.\n",
    "\n",
    "* Selain LangChain, terdapat dependensi lain yang perlu kita install. Dependensi ini terdapat pada file  `requirements.txt`. \n",
    "  * Arahkan direktori Anda ke tempat `requirements.txt` disimpan dengan command `cd`. \n",
    "    \n",
    "    Sintaks: `cd <DESTINATION DIRECTORY>`.\n",
    "\n",
    "* Meng-install banyak library sekaligus: dengan file `requirements.txt`.\n",
    "  * Sintaks: `pip install -r requirements.txt`.\n",
    "\n",
    "* Meng-install 1 library.\n",
    "  * Sintaks: `pip install <PACKAGE_NAME>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown and Code Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File yang sedang kita gunakan saat ini  disebut dengan **notebook**. Notebook merupakan file Python dengan format `.ipynb` yang memungkinkan kita untuk menulis kode Python dan memberikan penjelasan secara interaktif pada cell. Terdapat 2 jenis cell pada notebook:\n",
    "\n",
    "* **Markdown**\n",
    "    * Untuk menuliskan narasi.\n",
    "    * Kita bisa menulis teks **bold**, *italic*, bahkan formula matematis seperti:\n",
    "\n",
    "    $$f(x) = \\frac{e^{-x}}{(1+e^{-x})}$$\n",
    "\n",
    "* **Code**\n",
    "    * Untuk menuliskan kode Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ini adalah cell markdown untuk menulis narasi.\n",
    "\n",
    "* Kalimat yang bold: **kata**\n",
    "* Kalimat yang italic: *kata*\n",
    "\n",
    "Untuk menjalankan klik Ctrl + Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Model\n"
     ]
    }
   ],
   "source": [
    "# print(\"Ini adalah code cell\") --> tidak akan dieksekusi oleh Python\n",
    "# ini adalah komen\n",
    "\n",
    "print(\"Large Language Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command and Edit Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat 2 mode cell dalam notebook:\n",
    "\n",
    "1. **Command Mode**\n",
    "    - `a` : menambah cell baru di atas.\n",
    "    - `b` : menambah cell baru di bawah.\n",
    "    - `d` + `d` : menghapus cell terpilih.\n",
    "    - `c` : menyalin cell terpilih.\n",
    "    - `v` : paste cell terpilih.\n",
    "    - `m` : mengubah tipe cell ke markdown.\n",
    "    - `y` : mengubah tipe cell ke kode.\n",
    "    - `enter` : enter edit mode.\n",
    "\n",
    "\n",
    "2. **Edit Mode (Cell Terdapat Border Biru Persegi Panjang)**\n",
    "    - `Ctrl + Enter`: eksekusi satu cell.\n",
    "    - `Esc`: mengubah edit mode menjadi command mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Variable and Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saat menggunakan Python, sebagian besar pekerjaan kita melibatkan penyimpanan nilai tertentu ke dalam sebuah variabel. Untuk menyimpan nilai ke sebuah variabel, kita menggunakan assignment operator (`=`). \n",
    "\n",
    "Sebagai contoh, kita mendefinisikan variabel `activity` untuk menyimpan nilai `\"programming\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cara mendefinisikan variabel\n",
    "activity = \"programming\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penting untuk diperhatikan bahwa nama variabel dapat menyertakan angka, tetapi tidak boleh dimulai dengan angka. Memulai nama variabel dengan angka akan menimbulkan pesar error. \n",
    "\n",
    "Pada kode di bawah ini, kita mencoba mendefinisikan variabel `1activity`. \n",
    "\n",
    "Kita menggunakan simbol `#` untuk mengomentari bagian dari kode. Bagian yang dikomentari ini tidak akan dieksekusi. `#` dapat digunakan apabila kita ingin memberi penjelasan kode yang sudah dibuat. \n",
    "\n",
    "Untuk melihat pesan error yang muncul, hapus `#` pada baris pertama kode di bawah ini.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1activity = \"playing\"\n",
    "# menampilkan pesan SyntaxError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setiap nilai yang tersimpan dalam variabel memiliki tipe data tertentu. Terdapat tiga tipe data yang sering dijumpai di Python:\n",
    "\n",
    "* Tipe data untuk menyimpan nilai teks: `str`.\n",
    "* Tipe data untuk menyimpan nilai numerik: `int` dan `float`. \n",
    "  > Penting untuk diperhatikan bahwa tipe `float` disediakan untuk angka floating-point (berkoma).\n",
    "* Tipe data untuk menyimpan nilai kebenaran: `bool`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk memverifikasi tipe sebuah variabel, kita dapat memasukkan variabel tersebut ke dalam fungsi built-in `type()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string dengan kutip 1\n",
    "str1 = 'Studying' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying\n"
     ]
    }
   ],
   "source": [
    "print(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek tipe data\n",
    "type(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string dengan kutip 2\n",
    "str2 = \"Judul DSS bulan ini: 'Efficient Information Extraction: Q&A and Summarization over PDF Documents using LLM'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judul DSS bulan ini: 'Efficient Information Extraction: Q&A and Summarization over PDF Documents using LLM'\n"
     ]
    }
   ],
   "source": [
    "print(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek tipe data\n",
    "type(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string dengan kutip 3: bermanfaat untuk kata yang sangat panjang\n",
    "str3 = ''' Ini adalah contoh string kutip 3. \n",
    "Kita dapat menulis kalimat dengan lebih rapi pada baris baru.\n",
    "Kalimat 1 yang sangat panjang.\n",
    "Kalimat 2 yang sangat panjang\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ini adalah contoh string kutip 3. \n",
      "Kita dapat menulis kalimat dengan lebih rapi pada baris baru.\n",
      "Kalimat 1 yang sangat panjang.\n",
      "Kalimat 2 yang sangat panjang\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek tipe data\n",
    "type(str3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek tipe data\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 10.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operasi pada Angka** \n",
    "\n",
    "- `+`: penjumlahan.\n",
    "- `-`: pengurangan.\n",
    "- `*`: perkalian.\n",
    "- `/`: pembagian.\n",
    "- `//`: pembagian dengan pembulatan.\n",
    "- `%`: sisa pembagian.\n",
    "- `**`: eksponen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contoh operasi penjumlahan antara 15 dan 3\n",
    "15 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hands on: berapakah sisa pembagian 1099 dan 4?\n",
    "\n",
    "1099 % 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek tipe data\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python bersifat **case-sensitive**. `\"Activity\"` dan `\"activity\"` adalah nilai yang berbeda. \n",
    "\n",
    "Pada kode di bawah ini, kita menggunakan operator `==` untuk membandingkan kesetaraan kedua nilai tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'activity' == 'Activity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Output dari kode di atas adalah `False`. Hal ini menandakan kedua nilai tersebut berbeda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operator perbandingan lainnya:\n",
    "\n",
    "- `<`: lebih kecil dari (yaitu : a < b).\n",
    "- `<=`: lebih kecil atau sama dengan (yaitu : a <= b).\n",
    "- `>`: lebih besar dari (yaitu: a > b).\n",
    "- `>=`: lebih besar atau sama dengan (yaitu: a >= b).\n",
    "- `!=`: tidak sama dengan (yaitu: a != b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contoh operasi perbandingan lainnya\n",
    "\n",
    "'activity' != 'Activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hands on: apakah variabel b dan c sebelumnya sama?\n",
    "b == c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beberapa hal yang perlu diperhatikan: `True` dan  `False` termasuk dalam daftar istilah yang disebut sebagai **Python keywords**. Kita tidak dapat menggunakan keywords ini sebagai nama variabel, nama fungsi, atau memberikan nilai kepada mereka (melakukan assignment).\n",
    "\n",
    "Berikut adalah daftar keywords lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['False',\n",
       " 'None',\n",
       " 'True',\n",
       " 'and',\n",
       " 'as',\n",
       " 'assert',\n",
       " 'async',\n",
       " 'await',\n",
       " 'break',\n",
       " 'class',\n",
       " 'continue',\n",
       " 'def',\n",
       " 'del',\n",
       " 'elif',\n",
       " 'else',\n",
       " 'except',\n",
       " 'finally',\n",
       " 'for',\n",
       " 'from',\n",
       " 'global',\n",
       " 'if',\n",
       " 'import',\n",
       " 'in',\n",
       " 'is',\n",
       " 'lambda',\n",
       " 'nonlocal',\n",
       " 'not',\n",
       " 'or',\n",
       " 'pass',\n",
       " 'raise',\n",
       " 'return',\n",
       " 'try',\n",
       " 'while',\n",
       " 'with',\n",
       " 'yield']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek daftar keywords\n",
    "import keyword\n",
    "\n",
    "keyword.kwlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü•Ω Dive Deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Definisikan variabel `s` dengan nilai 5. Nilai `s` menunjukkan panjang sisi sebuah persegi.\n",
    "2. Definisikan variabel `area` yang merupakan luas dari sebuah persegi (`s * s`).\n",
    "3. Bandingkan nilai pada variabel area apakah sama dengan `'25'`? (`area == '25'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code here\n",
    "s = 5\n",
    "area = s * s\n",
    "area == '25'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebelumnya, kita sudah melihat tipe data apa yang dapat dimiliki oleh variabel di Python. Untuk keperluan tingkat lanjut, Python memiliki beberapa **data structure** untuk menyimpan beberapa tipe data secara bersamaan. Terdapat 2 **data structure** yang sering digunakan: list dan dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List** merupakan data structure yang memungkinkan kita untuk menyimpan beberapa nilai dengan tipe data yang berbeda. Untuk membuat list, kita menggunakan kurung siku (`[ ]`). Misalnya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendefinisikan list\n",
    "list_example = [\"Taylor Swift\",\n",
    "                34, \n",
    "                ['Shake It Off','Blank Space','Lover'],\n",
    "                True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taylor Swift', 34, ['Shake It Off', 'Blank Space', 'Lover'], True]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menampilkan isi list\n",
    "list_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek tipe data\n",
    "type(list_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk mengakses nilai yang terdapat di dalam list, kita menggunakan **indeks/posisi** dari nilai tersebut.\n",
    "\n",
    "> ‚ÑπÔ∏è Python menerapkan **zero-indexing**. Posisi pertama memiliki indeks 0, posisi kedua memiliki indeks 1, dan seterusnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mengakses usia Taylor Swift: indeks 1\n",
    "list_example[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika kita ingin mengakses beberapa nilai dalam list sekaligus, kita bisa menggunakan `:`. Misalnya, kita ingin mengakses nama, usia, dan daftar lagu yang populer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taylor Swift', 34, ['Shake It Off', 'Blank Space', 'Lover'], True]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taylor Swift', 34, ['Shake It Off', 'Blank Space', 'Lover']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_example[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan kode di atas, kita hanya menampilkan data dengan indeks 0-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cara lain untuk mengakses nilai di dalam list adalah menggunakan **back indexing**. Dengan back indexing, kita menggunakan nilai negatif untuk mengakses nilai di dalam list dari belakang. Misalnya, jika kita memasukkan indeks -1, kita akan menampilkan item terakhir dari list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item terakhir di list\n",
    "list_example[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shake It Off', 'Blank Space', 'Lover']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mengakses daftar lagu populer\n",
    "list_example[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada list, kita menggunakan indeks untuk mengakses sebuah nilai. Pada **dictionary** kita menggunakan **key** untuk mengakses sebuah nilai/**value**. Untuk membuat sebuah dictionary, kita menggunakan kurung keriting (`{ }`). Misalnya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendefiniskan dictionary\n",
    "dict_example = {\n",
    "    \"name\" : \"Taylor Swift\",\n",
    "    \"age\" : 34,\n",
    "    \"popular_songs\" : ['Shake It Off','Blank Space','Lover']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Taylor Swift',\n",
       " 'age': 34,\n",
       " 'popular_songs': ['Shake It Off', 'Blank Space', 'Lover']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menampilkan isi dictionary\n",
    "dict_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek tipe data\n",
    "type(dict_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan contoh di atas:\n",
    "\n",
    "* Yang merupakan key adalah `\"name\"`, `\"age\"`, dan `\"popular_songs\"`.\n",
    "* Yang merupakan value adalah `\"Taylor Swift\"`, `34`, dan `['Shake It Off', 'Blank Space', 'Lover']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk mengakses usia Taylor Swift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mengakses usia Taylor Swift\n",
    "dict_example['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shake It Off', 'Blank Space', 'Lover']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_example['popular_songs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü•Ω Dive Deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misalkan terdapat list yang berisikan 2 buah dictionary seperti di bawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = [\n",
    "{\n",
    "    'LLM' : 'ChatGPT',\n",
    "    'Developer': 'OpenAI'\n",
    "},\n",
    "{\n",
    "    'LLM': 'Gemini',\n",
    "    'Developer': 'Google'\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagaimana cara kita mengakses nama developer Gemini?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "# Pak Audi\n",
    "\n",
    "dev_name = [item['Developer'] for item in list_of_dicts if item['LLM'] == 'Gemini'][0]\n",
    "print(dev_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dicts[1]['Developer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END OF DAY 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**START OF DAY 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping (`while`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîÅ **Looping** pada Python merupakan eksekusi kode secara berulang sampai suatu kondisi terpenuhi. Salah satu sintaks untuk melakukan looping di Python adalah `while`. Contoh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini adalah iterasi ke 1\n",
      "Ini adalah iterasi ke 2\n",
      "Ini adalah iterasi ke 3\n",
      "Ini adalah iterasi ke 4\n",
      "Ini adalah iterasi ke 5\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "while i <= 5:\n",
    "    print(f\"Ini adalah iterasi ke {i}\")\n",
    "    i += 1 # akan menambahkan i yang sekarang dengan 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ÑπÔ∏è `f` pada saat menampilkan kalimat di atas disebut dengan **F-string**. Sederhananya, F-string memungkinkan kita untuk menampilkan nilai dari sebuah variabel secara dinamis dalam sebuah kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode di atas:\n",
    "\n",
    "* Kita mendefinisikan kondisi awal, `i = 1`.\n",
    "* Pengecekan kondisi, apakah nilai `i` saat ini kecil atau sama dengan 5.\n",
    "  * Jika benar, maka akan ditampilkan tulisan dan nilai `i` akan bertambah 1. \n",
    "  * Jika tidak, proses iterasi akan berakhir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function** pada Python merupakan blok kode yang dapat digunakan secara berulang. Penggunaan function dapat membuat penulisan kode kita menjadi lebih rapi dan tidak redundan. \n",
    "\n",
    "Function pada Python umumnya memiliki 3 komponen utama:\n",
    "\n",
    "* **Parameter**: input untuk function.\n",
    "* **Block of code**: blok kode untuk melakukan operasi pada input.\n",
    "* **Return value**: output hasil operasi function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk mendefinisikan function, kita menggunakan keywords `def`. Misalnya, kita membuat sebuah function untuk melakukan beberapa operasi matematika sekaligus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendefinisikan fungsi\n",
    "def math_ops(num1, num2, num3):\n",
    "    sum = num1 + num2 + num3\n",
    "    sub = num1 - num2 - num3\n",
    "\n",
    "    print(f\"Angka: {num1}, {num2}, {num3}\")\n",
    "    print(f\"Hasil penjumlahan: {sum}\")\n",
    "    print(f\"Hasil pengurangan: {sub}\")\n",
    "\n",
    "    return sum, sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angka: 100, 20, 30\n",
      "Hasil penjumlahan: 150\n",
      "Hasil pengurangan: 50\n"
     ]
    }
   ],
   "source": [
    "# cara menggunakan fungsi \n",
    "hasil_jumlah, hasil_kurang = math_ops(100, 20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(hasil_jumlah)\n",
    "print(hasil_kurang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salah satu kelebihan Python adalah tersedianya banyak **package** atau **library** yang dapat kita gunakan dengan mudah. Kita dapat menganggap library sebagai sekumpulan fungsi/program yang telah ditulis orang lain dan dapat kita gunakan kembali. Untuk dapat menggunakan fungsi yang terdapat dalam suatu library, kita harus meng-import library tersebut. \n",
    "\n",
    "Terdapat 2 cara paling umum untuk meng-import library:\n",
    "\n",
    "* Import library dengan statement `import`. Penggunaan fungsi/class menyertakan nama library.\n",
    "\n",
    "```python\n",
    "# cara import library\n",
    "import langchain_community.document_loaders\n",
    "\n",
    "# cara memanggil function/class\n",
    "pdf_loader = langchain_community.document_loaders.PyPDFLoader()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "# file_csv = pandas.read_csv('path_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import fungsi langsung dengan statement `from`. Penggunaan fungsi/class tidak menyertakan nama library.\n",
    "\n",
    "```python\n",
    "# cara import library\n",
    "from langchain_community.document_loaders import PyPDFLoader()\n",
    "\n",
    "# cara memanggil function/class\n",
    "pdf_loader = PyPDFLoader()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import read_csv\n",
    "# file_csv = read_csv('path_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fundamentals of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concept of Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/gen_ai_hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Apa itu Generative AI?**\n",
    "\n",
    "**Generative AI** merupakan bagian dari artificial intelligence yang bertujuan untuk **menghasilkan (generate) data**, seperti teks, gambar, video, dan musik. Untuk dapat menghasilkan data, generative AI dilatih/belajar dari data yang jumlahnya sangat besar. Proses belajar ini sering disebut training. Generative AI akan menghasilkan data berdasarkan pola yang sudah dipelari selama proses training.\n",
    "\n",
    "üöÄ **Jenis-Jenis Generative AI dan Aplikasinya**\n",
    "\n",
    "* **Generative Text Model**\n",
    "    * Input: teks.\n",
    "    * Output ‚Üí teks (text-to-text generator).\n",
    "      * Contoh: translation, **summarization**, **question-answering**, grammar correction.\n",
    "    * Output ‚Üí image (text-to-image generator).\n",
    "      * Contoh: image generation, video generation.\n",
    "    * Output ‚Üí audio (text-to-speech generator).\n",
    "      * Contoh: music generation.\n",
    "* **Generative Image Model**\n",
    "    * Input: gambar.\n",
    "    * Output: teks (image-to-text generator).\n",
    "      * Contoh: image captioning, visual question-answering, image search.\n",
    "    * Output: gambar (image-to-image generator).\n",
    "      * Contoh: image completion.\n",
    "    * Output: video (image-to-video generator).\n",
    "      * Contoh: animation generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM as Generative AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí¨ **LLM** merupakan generative AI yang telah dilatih menggunakan data teks dalam jumlah yang sangat besar. LLM mampu untuk memahami dan menghasilkan teks dengan gaya bahasa yang mirip dengan manusia. Selain menggambarkan besarnya data yang digunakan untuk melatih LLM, istilah \"large\" mengacu kepada tingkat kompleksitas, ukuran, dan banyaknya parameter pada LLM.\n",
    "\n",
    "> ‚ÑπÔ∏è Sebagai contoh, GPT-4, varian yang paling baru dari LLM yang dirilis oleh OpenAI, memiliki 1,7 triliun parameter. Sumber data yang digunakan untuk melatih model ini meliputi buku, website, jurnal ilmiah, artikel, posting-an media sosial, dan repositori kode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Platforms\n",
    "\n",
    "üí¨ Beberapa contoh platform LLM yang cukup populer: \n",
    "\n",
    "* ü§ñ**GPT**ü§ñ\n",
    "\n",
    "    ü§ñ **Generative Pre-Trained Transformer (GPT)** merupakan large language model yang dikembangkan oleh Open AI. Sama seperti model generative AI lainnya, GPT di-training dengan data teks dalam jumlah yang besar. GPT dirancang untuk memahami dan menghasilkan teks dengan cara yang mirip dengan manusia, sehingga dapat digunakan dalam berbagai aplikasi seperti chatbot, language translation, dan content creation. Baru-baru ini, OpenAI meluncurkan varian terbaru dari GPT-4, yakni GPT-4o.\n",
    "\n",
    "    Segmentasi produk GPT dapat dibedakan menjadi 2: chat-based platform dan Application Programming Interface (API)-based platform. Produk GPT berbasis chat dikenal sebagai [**ChatGPT**](https://chatgpt.com/). Semenjak dirilis pada November 2022 dengan versi GPT-2, saat ini ChatGPT telah merilis varian GPT yang lebih powerful: ChatGPT dengan GPT-3.5 dan ChatGPT dengan GPT-4. Untuk ChatGPT-3.5, pengguna dapat mengakses secara free. Untuk mendaptkan fitur yang lebih advanced pada ChatGPT dengan GPT-4, pengguna perlu melakukan subscription terlebih dahulu.\n",
    "\n",
    "    Varian produk kedua adalah produk berbasis [**Application Programming Interface (API)**](https://platform.openai.com/). API memungkinkan pengguna untuk meng-embbed kemampuan GPT pada aplikasi yang dapat disesuaikan dengan kebutuhan mereka. Penggunaan API ini sangat fleksibel karena dapat diintegrasikan dengan berbagai platform dan sistem yang sudah ada. \n",
    "\n",
    "* ‚ôä**Gemini**‚ôä\n",
    "\n",
    "    ‚ôä **Gemini** merupakan model generative AI yang dikembangkan oleh Google. Gemini dilatih menggunakan data teks, gambar, suara, dan video secara bersamaan dalam jumlah yang sangat besar. Hal ini membuat Gemini mampu untuk menerima berbagai macam input. Kelebihan ini menjadikan Gemini unggul dari segi generalisasinya terhadap data baru. Saat ini, versi terbaru dari Gemini adalah Gemini 1.5. \n",
    "\n",
    "    Sama seperti GPT, secara umum, fitur-fitur pada Gemini dikemas dalam 2 bentuk produk. Produk yang pertama merupakan [aplikasi yang berbasis chat bernama **Gemini app (sebelumnya Bard)**](https://gemini.google.com/app). Penggunaan aplikasi ini ditujukan untuk siapa saja secara umum tanpa perlu menuliskan kode tertentu. Produk lainnya merupakan [**API**](https://ai.google.dev/gemini-api) yang dapat digunakan pada use case yang lebih custom. Penggunaan API ini lebih ditujukan untuk pengguna dengan aspek pekerjaan teknis yang lebih tinggi, seperti software developer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt pada Language Model\n",
    "\n",
    "Untuk menghasilkan output yang sesuai, LLM memerlukan **prompt**. Prompt dapat dianalogikan sebagai sebuah petunjuk untuk LLM. Secara eksplisit, prompt merujuk kepada instruksi atau detail yang diberikan kepada LLM terkait output seperti apa yang diharapkan. Prompt juga bisa berupa konteks atau latar belakang dari topik yang ingin diketahui.\n",
    "\n",
    "üí° Beberapa tips awal untuk merancang sebuah prompt:\n",
    "\n",
    "* **Buat Prompt yang Sederhana**\n",
    "\n",
    "    Pembuatan prompt merupakan proses iteratif. Selalu mulai dengan sebuah prompt sederhana. Jika masih belum mendapatkan output yang diinginkan dari LLM, kita bisa me-refine prompt yang sudah dibuat sebelumnya.\n",
    "\n",
    "    Hindari pembuatan prompt untuk task yang kompleks. Kita dapat mem-break down task yang kompleks tersebut menjadi subtask yang lebih kecil dan membuat prompt yang berbeda untuk masing-masing subtask.\n",
    "\n",
    "* **Berikan Perintah yang Jelas: What to Do and What Not to Do?**\n",
    "\n",
    "    Saat membuat sebuah prompt, pastikan prompt memiliki instruksi yang jelas. Misalnya, kita menginginkan ringkasan dari sebuah kalimat, maka prompt yang efektif mengandung instruksi untuk membuat ringkasan tersebut. Kita juga dapat mendefinisikan detail apa yang tidak perlu dimasukkan dalam ringkasan yang dihasilkan LLM.\n",
    "\n",
    "\n",
    "* **Be Speficic and Detail**\n",
    "\n",
    "    Selain memberikan perintah yang jelas, kita dapat membuat prompt menjadi lebih spesifik dan detail. Misalnya, kita dapat mendefinisikan detail seperti gaya bahasa dan panjang output yang diharapkan.\n",
    "\n",
    "Contoh prompt:\n",
    "\n",
    "```\n",
    "\n",
    "Summarize the following article into approximately 150 words, highlighting the main points and key information discussed. Do not include minor details, personal opinions, or repetitive information.\n",
    "\n",
    "```\n",
    "\n",
    "> üîë Kunci prompt yang efektif: sederhana, jelas, dan spesifik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñ Arsitektur **transformer** merupakan salah satu fondasi dibalik keandalan LLM. Arsitektur ini pertama kali diperkenalkan pada paper yang berjudul [Attention is All You Need](https://arxiv.org/abs/1706.03762) oleh Vaswani dkk. pada tahun 2017. \n",
    "\n",
    "Kemunculan arsitektur transformer bertolak dari keterbatasan dari arsitektur yang sebelumnya sering digunakan untuk memodelkan data berupa sequence (contoh: bahasa), seperti Recurrent Neural Network (RNN) dan Long Short-Term Memory (LSTM). Keterbatasan ini di antaranya: ketidakefisienan dari segi daya komputasi dan waktu serta ketersediaan memori yang terbatas untuk sequence yang cukup panjang. Transformer mengatasi keterbatasan ini dengan fitur spesial yang disebut dengan **self-attention mechanism**. \n",
    "\n",
    "Secara sederhana, Transformer terdiri dari 2 komponen utama:\n",
    "\n",
    "![](assets/transformer_simplified_3.png)\n",
    "\n",
    "\n",
    "‚ñ∂Ô∏è **Encoder**\n",
    "\n",
    "**Encoder** merupakan bagian dari transformer yang bertujuan untuk memproses input sequence ke dalam representasi tertentu. Representasi ini merupakan bentuk numerik dari input sequence berikut dengan informasi urutannya. **Encoder** terdiri dari beberapa lapisan yang disebut **encoder layer**. Setiap encoder layer memiliki dua bagian utama:\n",
    "\n",
    "- **Multi-Head Self-Attention Mechanism**\n",
    "\n",
    "    Bagian ini membantu model untuk memperhatikan berbagai kata dalam input sekaligus, sehingga memahami hubungan antar kata yang berbeda.\n",
    "\n",
    "- **Feed-Forward Neural Network**\n",
    "    \n",
    "    Setelah bagian self-attention, representasi dari input sequence diproses oleh neural network untuk memperkuat pemahaman model tentang konteks.\n",
    "\n",
    "‚ñ∂Ô∏è **Decoder**\n",
    "\n",
    "**Decoder** merupakan bagian dari transformer yang bertujuan untuk menghasilkan output. Output ini mempertimbangkan input dari encoder. **Decoder** juga terdiri dari beberapa lapisan yang disebut **decoder layer**. Setiap decoder layer memiliki tiga bagian utama:\n",
    "\n",
    "- **Masked Multi-Head Self-Attention Mechanism**\n",
    "\n",
    "    Bagian ini memastikan model hanya memperhatikan kata-kata yang sudah dihasilkan sebelumnya, bukan kata yang belum dihasilkan. Ini penting untuk menjaga urutan kata yang dihasilkan.\n",
    "- **Multi-Head Attention Mechanism**\n",
    "\n",
    "    Bagian ini membantu decoder memperhatikan informasi dari encoder, sehingga output yang dihasilkan mempertimbangkan seluruh input.\n",
    "- **Feed-Forward Neural Network**\n",
    "\n",
    "    Setelah melalui kedua bagian attention, representasi dari sequence diproses oleh neural network untuk memperkuat pemahaman model tentang konteks.\n",
    "\n",
    "Berikut merupakan ilustrasi sederhana bagaimana transformer bekerja.\n",
    "\n",
    "![](assets/transformer_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Capability, Limitation, and Consideration\n",
    "\n",
    "Saat ini, LLM banyak dipilih karena keandalan dan kemampuannya untuk memahami konteks yang rumit. Selain itu, LLM juga mampu untuk menghasilkan teks dan narasi yang hampir menyamai produk bahasa yang ditulis oleh manusia. \n",
    "\n",
    "Berikut ini beberapa contoh task yang dapat diaplikasikan oleh LLM.\n",
    "\n",
    "* **Text Generation**: menghasilkan narasi dengan gaya bahasa yang hampir menyamai kemampuan manusia.\n",
    "* **Translation**: menerjemahkan teks ke dalam bahasa yang berbeda secara akurat.\n",
    "* **Summarization**: membuat ringkasan singkat dari teks yang panjang.\n",
    "* **Question-Answering**: menjawab sebuah pertanyaan berdasarkan konteks yang diberikan.\n",
    "\n",
    "Akan tetapi, perlu diperhatikan bahwa saat ini LLM masih menjadi topik riset yang aktif. Para peneliti masih mengembangkan metode yang lebih advanced untuk mengurangi beberapa limitasi dari LLM, seperti:\n",
    "\n",
    "* **Misleading Output** \n",
    "    \n",
    "    Sebagai produk AI yang sangat powerful, output yang dihasilkan oleh LLM dapat terlihat sangat meyakinkan. Akan tetapi, sebagai pengguna, kita perlu bijak dalam mengonsumsi konten yang dibuat oleh AI. Secara aktif, kita perlu melakukan verifikasi fakta melalui sumber-sumber yang kredibel.\n",
    "\n",
    "* **Technical Limitation**\n",
    "\n",
    "    Di balik kapabilitas LLM dalam task yang melibatkan natural language, masih terdapat concern terkait limitasi teknis. Terdapat situasi di mana AI tidak dapat memahami konteks secara mendalam atau menghasilkan respons yang sesuai dengan harapan pengguna. Selain itu, LLM memerlukan sumber daya komputasi yang sangat tinggi untuk berfungsi secara optimal, yang dapat menjadi hambatan bagi pengguna dengan keterbatasan infrastruktur teknologi.\n",
    "\n",
    "* **Laws and Rights-Related Issue**\n",
    "\n",
    "    Sebagai teknologi yang terus berkembang, penggunaan LLM juga dihadapkan pada isu-isu hukum dan hak. Misalnya, hak cipta, privasi, dan perlindungan data adalah aspek-aspek penting yang perlu dipertimbangkan saat menggunakan AI. \n",
    "    \n",
    "    Pengguna harus memastikan bahwa mereka mematuhi semua regulasi yang relevan dan menghormati hak-hak individu dan entitas lain dalam penggunaan dan penyebaran konten yang dihasilkan oleh AI.\n",
    "\n",
    "Mempertimbangkan kelebihan dan limitasi yang ada, sebagai end-user, kita perlu cermat, selektif, dan aware dalam mempergunakan LLM. Kemampuan LLM yang powerful akan semakin maksimal selama dibersamai dengan konsiderasi berikut:\n",
    "\n",
    "* **Tidak Menganggap Output LLM sebagai Final Result** \n",
    "    \n",
    "    Output dari LLM tidak dijadikan sebagai hasil akhir, melainkan hanya sebagai pendukung tersier. Kita perlu tetap melakukan verifikasi dan cross-check dengan sumber lain yang terpercaya untuk memastikan akurasi dan relevansi informasi.\n",
    "\n",
    "* **Keputusan Kompleks dan Terkait Makhluk Hidup**\n",
    "    \n",
    "    Keputusan yang diambil dari output LLM tidak digunakan dalam pertimbangan yang kompleks dan menyangkut keperluan yang berkaitan dengan makhluk hidup, seperti keputusan medis, hukum, atau keuangan, tanpa konsultasi dengan ahli yang kompeten di bidang tersebut.\n",
    "\n",
    "* **Evaluasi Kritis dan Penyesuaian Konteks**\n",
    "    \n",
    "    Selalu melakukan evaluasi kritis terhadap output yang dihasilkan oleh LLM, termasuk memahami konteks dan bias yang mungkin ada dalam data pelatihannya. Penyesuaian dan interpretasi yang bijak diperlukan untuk memastikan bahwa informasi tersebut sesuai dengan konteks yang dibutuhkan.\n",
    "\n",
    "* **Keamanan dan Privasi Data**\n",
    "\n",
    "    Memperhatikan aspek keamanan dan privasi data saat menggunakan LLM. Jangan memasukkan informasi pribadi atau sensitif yang dapat disalahgunakan, dan pastikan bahwa penggunaan LLM mematuhi kebijakan privasi dan regulasi yang berlaku.\n",
    "\n",
    "> üìå More info: [Responsible AI](https://ai.google/responsibility/responsible-ai-practices/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChainü¶úüîó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Concept and Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü¶úüîó**LangChain** merupakan sebuah framework yang dapat digunakan untuk membuat aplikasi berbasis LLM. Dengan LangChain, pengguna dapat mengakses berbagai jenis LLM yang disediakan oleh provider seperti OpenAI dan Google. Selain itu, framework ini juga memungkinkan kita untuk membuat aplikasi custom dengan memanfaatkan kemampuan LLM. Fitur- fitur ini menjadikan LangChain sebagai package yang cukup powerful. \n",
    "\n",
    "> üìå[LangChain Official Documentation](https://python.langchain.com/v0.1/docs/get_started/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Seperti namanya, ide dasar dari LangChain adalah men-chaining beberapa komponen menjadi satu kesatuan untuk menjalankan fungsi tertentu. Dengan menggabungkan berbagai komponen ini, kita bisa membuat alur kerja yang kompleks dan dinamis.\n",
    "\n",
    "Secara umum, berikut adalah pengelompokan komponen LangChain berdasarkan fungsinya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ **Model I/O**\n",
    "\n",
    "* Mengatur input dan output dari sebuah LLM.\n",
    "* Komponen:\n",
    "    * **Prompt**: memberikan instruksi terkait output seperti apa yang harus dihasilkan.\n",
    "    * **LLM dan Chat Model**: memberikan output berdasarkan prompt.\n",
    "\n",
    "2Ô∏è‚É£ **Retrievers**\n",
    "\n",
    "* Mengambil data yang relevan untuk digunakan dalam LLM (biasanya melibatkan dokumen eksternal).\n",
    "* Komponen:\n",
    "    * **Document Loader**: untuk memuat dokumen eksternal.\n",
    "    * **Text Splitter**: untuk pemrosesan dokumen.\n",
    "    * **Embedding Model**: untuk merepresentasikan dokumen dalam bentuk numerik (embedding).\n",
    "    * **Vectorstore**: untuk menyimpan embedding\n",
    "    * **Retrievers**: mencari dan mengembalikan data yang paling relevan dari vectorstores berdasarkan query pengguna.\n",
    "\n",
    "3Ô∏è‚É£ **Composition**\n",
    "\n",
    "* Gabungan dari komponen basic di atas.\n",
    "* Komponen: tools, agents, chains.\n",
    "\n",
    "4Ô∏è‚É£ **Additional**\n",
    "\n",
    "* Komponen: memory, callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Concept and Setting for LangChain Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/api_analogy.png)\n",
    "\n",
    "Provider LLM seperti OpenAI dan Google menyediakan fungsionalitas LLM mereka dalam bentuk **Application Programming Interface (API)**. API merupakan sebuah protokol atau program yang memungkinkan beberapa komputer untuk saling berkomunikasi.\n",
    "\n",
    "> üí° Apabila dianalogikan, API merupakan pelayan di sebuah restoran. Kita sebagai pelanggan memesan makanan melalui pelayan. Selanjutnya, pelayan akan menyampaikan pesanan kita kepada chef (provider LLM) di dapur dan chef akan membuatkan pesanan tersebut. Pesanan yang sudah jadi akan diantarkan kembali melalui pelayan.\n",
    "\n",
    "Untuk dapat berinteraksi dengan API dari provider LLM, kita memerlukan sebuah **API key** (ID untuk pemesanan). API key ini digunakan setiap kali kita berinteraksi dengan LLM yang disediakan oleh provider. Pada workshop ini, kita akan menggunakan **Gemini sebagai LLM**.\n",
    "\n",
    "> üîë Find your [Gemini API key here](https://ai.google.dev/gemini-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena API key bersifat rahasia, API key jarang dimasukkan langsung ke dalam kode program. Salah satu cara terbaik untuk memuat API key secara otomatis adalah dengan membuat file `.env` dan menggunakan fungsi `load_dotenv()` dari package `dotenv` untuk memanggil file tersebut. \n",
    "\n",
    "Berikut adalah contoh isi dari file `.env`.\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY = \"your_GOOGLE_API_key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk meng-import fungsi load_dotenv() dari package dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load file .env secara otomatis\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of LLM Usage with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sejauh ini, kita sudah mengetahui beberapa dasar terkait LLM, konsep dasar LangChain, dan komponen dari LangChain. Selanjutnya, mari kita lihat bagaimana implementasi sederhana penggunaan LLM dengan LangChain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Your First Call to Gemini ‚ôä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan pertama ini, kita akan:\n",
    "\n",
    "* Membuat objek LLM.\n",
    "* Mencoba varian Gemini yang tersedia.\n",
    "* Menanyakan pertanyaan dengan method `.invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASANI\\anaconda3\\envs\\dss_pdf_llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat objek LLM \n",
    "gemini_10_pro = GoogleGenerativeAI(model = \"gemini-pro\") # gemini versi 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joko Widodo'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memberi pertanyaan dengan method .invoke()\n",
    "gemini_10_pro.invoke('Who is the president of Indonesia?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Add Prompt to Boost Gemini's Response üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan kedua ini, kita akan menambahkan prompt untuk membuat implementasi yang lebih dinamis dan custom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita akan membuat sebuah template untuk menerjemahkan sebuah kalimat ke dalam bahasa Inggris dengan gaya bahasa yang informal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendefinisikan prompt untuk translasi\n",
    "prompt = PromptTemplate(template = '''\n",
    "                        Translate the following sentence into informal English:\n",
    "                        {sentence}\n",
    "                        ''',\n",
    "                        input_variables = [\"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita menggunakan operator `|` untuk membuat sebuah chain. Chain tersebut terbentuk dari komponen prompt dan LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat chain untuk translasi \n",
    "llm_chain = prompt | gemini_10_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll be back in 10\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menerjemahkan kalimat dengan method .invoke()\n",
    "llm_chain.invoke({\n",
    "    'sentence': 'Saya akan kembali 10 menit lagi'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü•Ω Dive Deeper: Chain for  Grammar Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buatlah sebuah chain untuk melakukan grammar correction!\n",
    "\n",
    "**Step**:\n",
    "\n",
    "1. Anda dapat memodifikasi prompt dari contoh sebelumnya untuk kasus grammar correction.\n",
    "2. Definisikan chain dengan step pada `Experiment 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "# Pak Johan\n",
    "# step 1\n",
    "prompt_grammar = PromptTemplate(template = '''\n",
    "                        Correct the grammar of following sentence into :\n",
    "                        {sentence}\n",
    "                        ''',\n",
    "                        input_variables = [\"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "llm_chain = prompt_grammar | gemini_10_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like him'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\n",
    "    'sentence':'I like he'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END OF DAY 2**\n",
    "\n",
    "**START OF DAY 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Q&A and Summarization for PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Coba tanyakan pertanyaan berikut kepada LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pemilu 2024 di Indonesia belum dilaksanakan, sehingga belum ada pemenang yang diketahui.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_10_pro.invoke('Siapa pemenang Pemilu 2024 di Indonesia?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meskipun memiliki kemampuan yang powerful untuk mengerjakan task yang berkaitan dengan bahasa, LLM memiliki **knowledge cutoff**. Knowledge cutoff merupakan batas pengetahuan yang dimiliki oleh LLM karena resource untuk men-training LLM berhenti pada titik waktu tertentu. \n",
    "\n",
    "> ‚ÑπÔ∏è Untuk Gemini, knowledge cutoff-nya adalah November 2023.\n",
    "\n",
    "Dengan demikian, pengetahuan LLM tentang peristiwa dan informasi setelah knowledge cutoff tersebut mungkin terbatas atau tidak ada sama sekali.\n",
    "\n",
    "Untuk mengatasi permasalahan tersebut, kita dapat menggunakan **Retrieval Augmented Generation** (RAG). RAG bekerja dengan cara menyediakan konteks tambahan untuk LLM, seperti dokumen, website, dan database. Dengan RAG, LLM dapat mengakses dan memproses data terbaru, sehingga dapat memberikan jawaban yang lebih akurat dan terkini. Selain itu, pendekatan ini juga memudahkan kita untuk memanfaatkan LLM pada resource dan use case yang lebih spesifik.\n",
    "\n",
    "Pada workshop ini, kita akan mencoba untuk **menggunakan dokumen PDF sebagai konteks tambahan untuk LLM**. Dokumen ini merupakan sebuah paper yang berjudul [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223) dengan tebal 124 halaman. Paper ini menyediakan review yang komprehensif terkait LLM. Paper yang sama juga dapat Anda temui pada folder `data_input` dengan nama file `llm_survey.pdf`.\n",
    "\n",
    "Selanjutnya, kita akan **membuat chain** dengan LangChain yang memungkinkan kita untuk **bertanya seputar konten paper tersebut**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "Untuk membuat sebuah implementasi Q&A dokumen PDF menggunakan LangChain, kita akan mengikuti workflow berikut ini.\n",
    "\n",
    "![](assets/rag_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita perlu memuat dokumen yang menjadi konteks tambahan bagi LLM. Dalam hal ini, kita menggunakan `PyPDFLoader()` untuk memuat dokumen PDF. Selain PDF, LangChain menyediakan fungsionalitas untuk memuat dokumen dengan ekstensi lain, seperti CSV dan JSON.\n",
    "\n",
    "> üìå [LangChain Document Loaders](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk loading file PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader('data_input/llm_survey.pdf')\n",
    "pdf_data = pdf_loader.load()\n",
    "\n",
    "# ada parameter extract_images untuk ingin ekstrak info dari gambar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan kode di atas, dokumen PDF akan di-load sebagai list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List ini akan memiliki elemen sebanyak halaman dokumen PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setiap elemen pada list berisikan konten pada halaman tertentu. Misalnya, kita ingin melihat konten pada halaman ke-5 (indeks ke-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "given an increase in compute budget, the KM scaling law\n",
      "favors a larger budget allocation in model size than the data\n",
      "size, while the Chinchilla scaling law argues that the two\n",
      "sizes should be increased in equal scales, i.e.,having similar\n",
      "values for aandbin Equation (3).\n",
      "Discussion on Scaling Laws . After introducing the formu-\n",
      "lations, we continue to discuss scaling law in the following\n",
      "two aspects, to enhance its understanding:\n",
      "‚Ä¢Predictable scaling . In practice, scaling law can be used\n",
      "to instruct the training of LLMs, and it has been proven\n",
      "feasible to reliably estimate the performance of larger mod-\n",
      "els based on that of smaller models, called predictable scal-\n",
      "ing [46]. The benefits of predictable scaling for training\n",
      "LLMs are mainly twofold. Firstly, for large models, it is\n",
      "infeasible to rigorously examine various training tricks or\n",
      "variants, and it would be very helpful if experiences gained\n",
      "from small models could also apply to large models. For\n",
      "instance, small proxy models can be trained to find the\n",
      "optimal schedule of the data mixture for large models [59].\n",
      "Secondly, the training of large-scale models takes a long\n",
      "time, often suffering from issues such as training loss spike,\n",
      "and scaling law can be employed to monitor the training\n",
      "status of LLMs, e.g.,identifying abnormal performance at an\n",
      "early time. Despite that scaling law characterizes a smooth\n",
      "trend of performance increase (or loss decrease), it also\n",
      "indicates that diminishing returns7might occur as model\n",
      "scaling. An empirical study [58] from the OpenAI team\n",
      "has shown that representation quality or semantic content\n",
      "can still effectively improve even if approaching the point\n",
      "of diminishing returns ( i.e., approaching the irreducible\n",
      "loss) [58]. This finding suggests that training large models\n",
      "are promising for improving the performance of down-\n",
      "stream tasks. To further explore scaling effect, a potential\n",
      "issue is that the amount of available data for training LLMs\n",
      "is actually limited. With the ever-increasing model scale, the\n",
      "public text data would be soon ‚Äúexhausted‚Äù for LLMs [60].\n",
      "Thus, it will be meaningful to study how scaling laws apply\n",
      "to a data-constrained regime [61], where data repetition or\n",
      "augmentation might be useful to alleviate data scarcity.\n",
      "‚Ä¢Task-level predictability . Existing research of scaling laws\n",
      "are mostly conducted in terms of language modeling loss\n",
      "(e.g., per-token cross-entropy loss in nats [30]), while in\n",
      "practice we are more concerned about the performance of\n",
      "LLMs on actual tasks. Thus, a basic problem is that how\n",
      "the decrease of language modeling loss translates into the\n",
      "improvement of task performance [58]. Intuitively, a model\n",
      "with a smaller language modeling loss tends to yield a\n",
      "better performance on downstream tasks, since language\n",
      "modeling loss can be considered as a general measure of\n",
      "the overall model capacity. GPT-4 [46] has reported that\n",
      "some capabilities ( e.g., coding ability) can be accurately\n",
      "predicted via scaling law. Despite that, readers should be\n",
      "aware that a direct decrease in language modeling loss does\n",
      "not always indicate an improvement of model performance\n",
      "on downstream tasks. Specially, the phenomenon of inverse\n",
      "scaling would occur for some tasks, where task performance\n",
      "surprisingly becomes worse as the language modeling loss\n",
      "decreases [62]. Overall, it is more difficult to explore and\n",
      "7. https://en.wikipedia.org/wiki/Diminishing returnscharacterize task-level scaling laws, since it might be also\n",
      "dependent on task-related information (task metric, task\n",
      "difficulty, etc.). Furthermore, some capacities ( e.g.,in-context\n",
      "learning [55]) are unpredictable according to the scaling law,\n",
      "which can be observed only when the model size exceeds a\n",
      "certain level (as discussed below).\n",
      "Emergent Abilities of LLMs . In the literature [31], emergent\n",
      "abilities of LLMs are formally defined as ‚Äúthe abilities that\n",
      "are not present in small models but arise in large models‚Äù,\n",
      "which is one of the most prominent features that distin-\n",
      "guish LLMs from previous PLMs. It further introduces a\n",
      "notable characteristic when emergent abilities occur [31]:\n",
      "performance rises significantly above random when the\n",
      "scale reaches a certain level. By analogy, such an emergent\n",
      "pattern has close connections with the phenomenon of phase\n",
      "transition in physics [31, 63]. In principle, emergent abilities\n",
      "can be defined in relation to some complex tasks [31, 64],\n",
      "while we are more concerned with general abilities that\n",
      "can be applied to solve a variety of tasks. Here, we briefly\n",
      "introduce three typical emergent abilities for LLMs and\n",
      "representative models that possess such an ability8.\n",
      "‚Ä¢In-context learning. The in-context learning (ICL) ability\n",
      "is formally introduced by GPT-3 [55]: assuming that the\n",
      "language model has been provided with a natural language\n",
      "instruction and/or several task demonstrations, it can gen-\n",
      "erate the expected output for the test instances by com-\n",
      "pleting the word sequence of input text, without requiring\n",
      "additional training or gradient update9. Among the GPT-\n",
      "series models, the 175B GPT-3 model exhibited a strong ICL\n",
      "ability in general, but not the GPT-1 and GPT-2 models. Such\n",
      "an ability also depends on the specific downstream task. For\n",
      "example, the ICL ability can emerge on the arithmetic tasks\n",
      "(e.g., the 3-digit addition and subtraction) for the 13B GPT-3,\n",
      "but 175B GPT-3 even cannot work well on the Persian QA\n",
      "task [31].\n",
      "‚Ä¢Instruction following. By fine-tuning with a mixture of\n",
      "multi-task datasets formatted via natural language descrip-\n",
      "tions (called instruction tuning ), LLMs are shown to perform\n",
      "well on unseen tasks that are also described in the form\n",
      "of instructions [28, 66, 67]. With instruction tuning, LLMs\n",
      "are enabled to follow the task instructions for new tasks\n",
      "without using explicit examples, thus having an improved\n",
      "generalization ability. According to the experiments in [67],\n",
      "instruction-tuned LaMDA-PT [68] started to significantly\n",
      "outperform the untuned one on unseen tasks when the\n",
      "model size reached 68B, but not for 8B or smaller model\n",
      "sizes. A recent study [69] found that a model size of 62B is\n",
      "at least required for PaLM to perform well on various tasks\n",
      "in four evaluation benchmarks ( i.e.,MMLU, BBH, TyDiQA\n",
      "and MGSM), though a much smaller size might suffice for\n",
      "some specific tasks ( e.g., MMLU).\n",
      "‚Ä¢Step-by-step reasoning. For small language models, it\n",
      "is usually difficult to solve complex tasks that involve\n",
      "8. It is difficult to accurately examine the critical size for emergent\n",
      "abilities of LLMs ( i.e.,the minimum size to possess an ability), since it\n",
      "might vary for different models or tasks. Also, existing studies often\n",
      "test emergent abilities on very limited model sizes for a specific LLM.\n",
      "For example, PaLM is often tested with three sizes of 8B, 62B and 540B.\n",
      "It is unclear about the model performance of the untested sizes.\n",
      "9. In a recent study [65], it also shows that in-context learning implic-\n",
      "itly performs meta-optimization through the attention mechanism.\n"
     ]
    }
   ],
   "source": [
    "print(pdf_data[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita akan melakukan proses splitting. Proses ini bertujuan untuk membuat chunk yang lebih kecil dari konten yang sudah diekstrak sebelumnya. Chunk-chunk yang lebih kecil akan lebih mudah di-maintain, disimpan, dan diproses oleh LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk split konten file PDF menjadi chunk yang lebih kecil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada proses splitting ini, kita menggunakan `RecursiveCharacterTextSplitter()`. Dengan `RecursiveCharacterTextSplitter()`, proses splitting dilakukan dengan tetap menjaga bagian-bagian teks yang terkait tetap berdekatan satu sama lain. Merujuk pada [dokumentasi LangChain](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/), teknik splitting dengan `RecursiveCharacterTextSplitter()` adalah teknik yang direkomendasikan untuk mulai melakukan splitting dari sebuah teks yang cukup besar.\n",
    "\n",
    "> üìå [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)\n",
    "\n",
    "Pada kode di bawah ini, kita mendefinisikan output dari proses splitting:\n",
    "\n",
    "* Setiap chunk akan memiliki maksimal sebanyak 1000 karakter.\n",
    "* Antara satu chunk dengan chunk yang lain memiliki kesamaan sebanyak 450 karakter. Hal ini didefinisikan untuk mempertahankan konteks antara satu chunk dengan chunk yang lain. \n",
    "* Proses splitting dilakukan berdasarkan daftar karakter yang kita definisikan pada parameter `separators`.\n",
    "\n",
    "Selanjutnya, kita memasukkan list `pdf_data` sebagai input untuk method `.split_documents()`. Method ini akan membuat chunk yang lebih kecil dari `pdf_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 450,\n",
    "    separators=[\n",
    "                \"\\n\\n\",\n",
    "                \"\\n\",\n",
    "                \" \",\n",
    "                \".\",\n",
    "                \",\",\n",
    "                \"\\u200b\",  # zero-width space\n",
    "                \"\\uff0c\",  # full-width comma\n",
    "                \"\\u3001\",  # ideographic comma\n",
    "                \"\\uff0e\",  # full-width full stop\n",
    "                \"\\u3002\",  # ideographic full stop\n",
    "                \"\",\n",
    "            ])\n",
    "\n",
    "splits = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di akhir proses ini, kita mendapatkan 1316 chunk hasil splitting dari 124 elemen pada `pdf_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di balik layar, operasi pada komputer merupakan operasi yang melibatkan angka. Agar teks yang kita miliki dapat dimengerti oleh komputer, kita perlu membuat representasi numerik dari teks tersebut. Proses ini dibut dengan **embedding**.\n",
    "\n",
    "LangChain menyediakan banyak pilihan embedding, tergantung dari LLM yang akan kita gunakan:\n",
    "* `GoogleGenerativeEmbeddings()` untuk model LLM yang dikembangkan oleh Google.\n",
    "* `OpenAIEmbeddings()` untuk model LLM dari OpenAI.\n",
    "\n",
    "> üìå [LangChain Embedding Models](https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/)\n",
    "\n",
    "Representasi numerik dari teks ini selanjutnya disimpan di dalam **vector database**. \n",
    "> ‚ú® Sederhananya, kita menyimpan konten file PDF di database dalam bentuk angka.\n",
    "\n",
    "Vector database yang akan kita gunakan adalah **Chroma**. Chroma merupakan vector database yang cukup powerful untuk menyimpan unstructured data, seperti konten pada file PDF. Chroma juga dilengkapi dengan algoritma yang efisien dan akurat untuk pencarian informasi dari embedding yang disimpan. Selain itu, Chroma juga memungkinkan kita untuk menyimpan objek database sehingga dapat digunakan kembali untuk keperluan yang akan datang.\n",
    "\n",
    "> üìå [LangChain Vector Stores](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk membuat embedding\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # embedding Google Generative AI\n",
    "\n",
    "# untuk menyimpan embedding\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode dibawah ini, kita mendefinisikan sebuah fungsi untuk:\n",
    "\n",
    "* Mengubah dokumen menjadi embedding. \n",
    "* Menyimpan hasil embedding ke dalam Chroma.\n",
    "* Menyimpan file database ke suatu folder agar dapat digunakan kembali.\n",
    "\n",
    "Penjelasan parameter:\n",
    "\n",
    "* `documents `: dokumen yang akan diubah menjadi embedding.\n",
    "* `embbeding`: fungsi yang digunakan untuk membuat embedding, misalnya `GoogleGenerativeAIEmbeddings()` jika menggunakan Gemini sebagai LLM.\n",
    "* `persist_directory`: folder untuk menyimpan vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore_folder(documents, embedding, persist_directory):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents = documents, \n",
    "        embedding = embedding,\n",
    "        persist_directory = persist_directory\n",
    "    )\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_gemini = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
    "#     persist_directory = 'data_input/chroma_gemini'\n",
    "# )\n",
    "\n",
    "# time spent: 40.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses mengubah dokumen menjadi embedding dan menyimpannya ke dalam database dapat berjalan lama karena dapat dipengaruhi oleh berbagai faktor, mulai dari besarnya dokumen sampai dengan koneksi internet. Oleh karena itu, pada workshop ini, proses pembuatan vector database sudah dilakukan terlebih dahulu. Penyimpanan vector database ini terdapat pada folder `data_input`. \n",
    "\n",
    "Untuk me-load kembali vector database dari folder yang sudah ada, Anda dapat menjalankan kode di bawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memanggil embedding dari directory yang sudah disimpan\n",
    "vec_gemini = Chroma(persist_directory = 'data_input/chroma_gemini',\n",
    "                    embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita telah membuat sebuah vector database yang menjadi sumber bagi LLM untuk mencari informasi. Selanjutnya, kita akan membuat sebuah chain yang memungkinkan kita untuk bertanya kepada LLM. LLM akan memberikan jawaban berdasarkan informasi yang terdapat pada vector database.\n",
    "\n",
    "> ü§® **Bagaimana cara LLM memberikan jawaban yang sesuai?** Pada saat kita memberikan pertanyaan (query) kepada LLM, pertanyaan tersebut akan diubah menjadi representasi numerik (vector). Representasi numerik ini akan digunakan untuk mencari jawaban yang sesuai berdasarkan informasi pada vector database. Proses ini melibatkan perhitungan vector similarity antara vector query dan vector informasi yang terdapat dalam vector database. Informasi dengan tingkat similarity yang tinggi akan di-return sebagai jawaban."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/rag_workflow2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# untuk mempersiapkan prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# untuk memasukkan pertanyaan\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# untuk menampilkan output yang diinginkan\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# untuk menampilkan output secara rapi\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = \"\"\"\n",
    "    You are the great assistant in understanding additional context\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    Use the minimum of three sentences to answer the question. \n",
    "    Try your best to answer as complete as possible with easy style of English.\n",
    "    Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(qa_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(retriever, llm):\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode di atas, kita membuat sebuah fungsi untuk membuat chain untuk Q&A. Chain untuk Q&A di atas terdiri atas beberapa komponen:\n",
    "\n",
    "*  `custom_rag_prompt` yang merupakan prompt untuk mengarahkan output dari LLM.\n",
    "* `llm` yang akan berinteraksi dengan pengguna.\n",
    "* `StrOutputParser()` yang akan menangkap output dari LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan pada saat kita mendefinisikan template prompt, terdapat 2 placeholder: `{context}` dan `{question}`.\n",
    "\n",
    "* Value untuk `{context}` berasal dari vector database.\n",
    "* Value untuk `{question}` berasal dari pertanyaan pengguna pada saat menjalankan chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_chain = create_qa_chain(retriever = vec_gemini.as_retriever(),\n",
    "                               llm = ChatGoogleGenerativeAI(model = \"gemini-pro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang, kita sudah membuat chain untuk Q&A. Mari kita tes chain tersebut dengan memberi beberapa pertanyaan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model, a type of artificial intelligence (AI) that has been\n",
      "trained on a massive amount of text data. LLMs are known for their ability to generate\n",
      "human-like text, translate languages, answer questions, and perform various other\n",
      "language-related tasks.  Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        gemini_chain.invoke('What is LLM?'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-1, the initial model in the GPT series, was released in 2018 and used a generative,\n",
      "decoder-only Transformer architecture. GPT-2, released later, increased the parameter\n",
      "scale and was trained on a large webpage dataset. Further advancements led to GPT-3, which\n",
      "expanded the parameter scale significantly and introduced few-shot learning capabilities.\n",
      "The latest iteration, GPT-4, is still under development but is expected to bring even\n",
      "greater capabilities. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        gemini_chain.invoke('How is the evolution of GPT series model?'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Chatbot-Like Interaction with `while` Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAsk The PDF!\u001b[0m\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m\n",
      "What is LLM?\n",
      "\u001b[1mResponse:\u001b[0m\n",
      "LLM, standing for Large Language Model, is a type of neural network model that has been\n",
      "trained on a massive amount of text data. LLMs are known for their ability to generate\n",
      "human-like text, translate languages, and answer questions. They have also been used in a\n",
      "variety of other applications, such as summarizing text, extracting information, and\n",
      "writing creative content. Thanks for asking!\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m\n",
      "q\n",
      "Exiting\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASANI\\anaconda3\\envs\\dss_pdf_llm\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"\\033[1mAsk The PDF!\\033[0m\")\n",
    "print('')\n",
    "\n",
    "while True:\n",
    "    \n",
    "    print('\\033[1mQuestion:\\033[0m')\n",
    "\n",
    "    query = input('')\n",
    "    print(query)\n",
    "\n",
    "    #To exit: use 'exit', 'quit', 'q', or Ctrl-D.\",\n",
    "    if query.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print('Exiting')\n",
    "        sys.exit()\n",
    "\n",
    "    print('\\033[1mResponse:\\033[0m')\n",
    "    response = textwrap.fill(gemini_chain.invoke(query), width=90)\n",
    "    print(response)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebelumnya, kita mendefinisikan sebuah chain untuk Q&A menggunakan `|`. `|` merupakan sintaks LangChain Expression Language (LCEL) yang menjadi dasar dalam pendefinisian chain yang lebih kompleks.\n",
    "\n",
    "> üìå [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/)\n",
    "\n",
    "Selain digunakan untuk membuat sistem Q&A, LLM juga sering dimanfaatkan untuk task summarization. \n",
    "\n",
    "Pada use case kali ini, kita akan mendefinisikan chain dengan LCEL untuk melakukan summarization. Dokumen yang akan kita buat summary-nya merupakan paper yang berjudul [Transformers in Time Series: A Survey](https://arxiv.org/abs/2202.07125). Pembahasan paper ini adalah seputar penggunaan arsitektur transformer untuk data time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita akan me-load file yang akan kita ringkas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_ts_loader = PyPDFLoader('data_input/transformer_ts.pdf')\n",
    "trans_ts_content = trans_ts_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita mendefinisikan prompt untuk membuat sebuah summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "    You are the great assistant in summarizing the following passage.\n",
    "\n",
    "    Provide a summary of the following passage. \n",
    "    The summary should be general and no longer than five sentences.\n",
    "\n",
    "    Passage:\n",
    "    ```{text}```\n",
    "    \n",
    "    Summary:\n",
    "\"\"\"\n",
    "\n",
    "custom_summary_prompt = PromptTemplate.from_template(summary_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita mendefinisikan sebuah fungsi yang akan me-return chain untuk melakukan summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_chain(llm):\n",
    "    summary_chain = {'text':RunnablePassthrough()} | custom_summary_prompt | llm | StrOutputParser()\n",
    "    return summary_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_summarizer = create_summary_chain(llm = ChatGoogleGenerativeAI(model=\"gemini-pro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita sebuah chain untuk melakukan summarizer dengan Gemini sebagai LLM. Selanjutnya, kita dapat menjalankan chain tersebut dan memberikan dokumen yang akan diringkas (`llm_ts_content`) sebagai input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper provides a comprehensive and systematic review of\n",
      "Transformer schemes for time series modeling. It highlights strengths,\n",
      "limitations, and future research directions for Transformers in this\n",
      "domain. The authors categorize Transformers based on network\n",
      "modifications and application domains, providing a taxonomy that helps\n",
      "organize and understand the existing literature. Empirical studies are\n",
      "conducted to analyze robustness, model size, and seasonal-trend\n",
      "decomposition, offering practical guidance on how to effectively use\n",
      "Transformers for time series modeling. The authors conclude by\n",
      "discussing potential future research directions, including inductive\n",
      "biases for time series Transformers, Transformers and GNNs for time\n",
      "series, pre-trained Transformers for time series, Transformers with\n",
      "architecture level variants, and Transformers with NAS for time\n",
      "series.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        gemini_summarizer.invoke(trans_ts_content)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Implementasi dengan OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain API dari Gemini, Anda bisa juga menggunakan API yang disediakan oleh provider lain, seperti OpenAI. Untuk mengakses model dari provider tersebut, Anda juga memerlukan API key:\n",
    "\n",
    "* [Cara mendapatkan API key dari OpenAI](https://platform.openai.com/docs/quickstart).\n",
    "\n",
    "Selanjutnya, Anda perlu menambahkan API key tersebut di dalam file `.env`.\n",
    "\n",
    "Berikut adalah contoh file `.env` dengan API key dari Google dan OpenAI.\n",
    "```\n",
    "OPENAI_API_KEY = \"sk-proj-**********\"\n",
    "GOOGLE_API_KEY = \"AIza************\"\n",
    "```\n",
    "Selanjutnya, Anda dapat menggunakan fungsi `load_dotenv()` untuk me-load API key secara otomatis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "# LLM\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanya perlu dijalankan pada saat pertama kali membuat embedding\n",
    "\n",
    "# vectorstore_open_ai = create_vectorstore_folder(\n",
    "#     documents = splits,\n",
    "#     embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "#     persist_directory = 'data_input/chroma_open_ai'\n",
    "# )\n",
    "\n",
    "# time spent: 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memanggil embedding dari directory yang sudah disimpan\n",
    "\n",
    "vec_openai = Chroma(persist_directory = 'data_input/chroma_open_ai',\n",
    "                    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat custom chain untuk Q&A dengan LLM OpenAI\n",
    "\n",
    "openai_chain = create_qa_chain(\n",
    "    retriever = vec_openai.as_retriever(),\n",
    "    llm = ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLaMA model family was introduced by Meta AI in February 2023, consisting of four\n",
      "sizes - 7B, 13B, 30B, and 65B. These models have gained significant attention from both\n",
      "research and industry communities for their excellent performance on various open\n",
      "benchmarks. Researchers have extended LLaMA models through instruction tuning or continual\n",
      "pretraining, making them popular for developing customized models with relatively low\n",
      "computational costs. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "# Q&A implementation\n",
    "\n",
    "print(\n",
    "    textwrap.fill(\n",
    "        openai_chain.invoke('Describe about llaMa model family'), \n",
    "        width=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_summarizer = create_summary_chain(\n",
    "    llm =  ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The passage provides a comprehensive overview of the use of\n",
      "Transformers in time series modeling. It discusses the advantages of\n",
      "Transformers in capturing long-range dependencies and interactions,\n",
      "reviews various Transformer schemes for time series modeling, and\n",
      "categorizes applications such as forecasting, anomaly detection, and\n",
      "classification. The passage also highlights the need for inductive\n",
      "biases, the combination of Transformers and Graph Neural Networks\n",
      "(GNNs), pre-trained Transformers, architecture-level variant designs,\n",
      "and the use of Neural Architecture Search (NAS) for optimizing\n",
      "Transformer architectures. Additionally, the passage includes\n",
      "experimental evaluations and future research opportunities in the\n",
      "field of time series Transformers.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    textwrap.fill(\n",
    "        openai_summarizer.invoke(trans_ts_content)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßê Be Critical of LLM Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return Source of QnA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian sebelumnya, kita berhasil membuat sebuah chain untuk task Q&A. Akan tetapi, chain tersebut hanya memberikan sebuah jawaban. Kita akan mencoba membuat sebuah chain di mana selain memberikan jawaban, chain juga akan memberikan resource yang digunakan untuk menyimpulkan jawaban tersebut. Dengan demikian, diharapkan dapat memverifikasi kebenaran jawaban LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain_with_source(retriever, llm):\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    ).assign(answer=rag_chain_from_docs)\n",
    "\n",
    "    return rag_chain_with_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_openai_source = create_qa_chain_with_source(retriever = vec_openai.as_retriever(),\n",
    "                                               llm = ChatOpenAI(model = 'gpt-3.5-turbo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='the capacities of LLMs to follow complex instructions.\\nOther Practical Tricks. In practice, there are also several\\nuseful strategies and tricks that are helpful to improve the\\nfine-tuning performance of LLMs. We list several represen-\\ntative ones as follows:\\n‚Ä¢Efficient training for multi-turn chat data. Given a multi-\\nturn chat example (the conversation between a user and\\nchatbot), a straightforward fine-tuning way is to split it into\\nmultiple context-response pairs for training: a LLM is fine-\\ntuned to generate the response based on the correspond-\\ning context for all splits ( i.e., at each utterance from the\\nuser). In such a fine-tuning way, it is apparent that there\\nexist overlapping utterances in the split examples from a\\nconversation. To save the training cost, Vicuna [138] has\\nadopted an efficient way that feeds the whole conversation\\ninto the LLM, but relies on a loss mask that only computes\\nthe loss on the responses of the chatbot for training. It can', metadata={'page': 33, 'source': 'data_input/llm_survey.pdf'}),\n",
       "  Document(page_content='compute-efficient allocation of the compute resources. For\\nexample, Chinchilla (with more training tokens) outper-\\nforms its counterpart model Gopher (with a larger model\\nsize) by increasing the data scale with the same compute\\nbudget [34]. In addition, data scaling should be with careful\\ncleaning process, since the quality of pre-training data plays\\na key role in the model capacity.\\n‚Ä¢Training . Due to the huge model size, it is very chal-\\nlenging to successfully train a capable LLM. Distributed\\ntraining algorithms are needed to learn the network param-\\neters of LLMs, in which various parallel strategies are of-\\nten jointly utilized. To support distributed training, several\\noptimization frameworks have been released to facilitate\\nthe implementation and deployment of parallel algorithms,\\nsuch as DeepSpeed [74] and Megatron-LM [75‚Äì77]. Also, op-\\ntimization tricks are also important for training stability and\\nmodel performance, e.g., restart to overcome training loss', metadata={'page': 5, 'source': 'data_input/llm_survey.pdf'}),\n",
       "  Document(page_content='respectively.\\nIn addition to the above practical strategies and tricks,\\nexisting work has also used other tricks, e.g., concatenating\\nmultiple examples into a single sequence to approach the\\nmax length [355].\\n5.1.3 The Effect of Instruction Tuning\\nIn this part, we discuss the effect of instruction tuning on\\nLLMs in three major aspects.\\nPerformance Improvement. Despite being tuned on a mod-\\nerate number of instances, instruction tuning has become\\nan important way to improve or unlock the abilities of\\nLLMs [69]. Recent studies have experimented with language\\nmodels in multiple scales (ranging from 77M to 540B),\\nshowing that the models of different scales can all benefit\\nfrom instruction tuning [69, 345], yielding improved perfor-\\nmance as the parameter scale increases [94]. Further, smaller\\nmodels with instruction tuning can even perform better\\nthan larger models without fine-tuning [28, 69]. Besides\\nthe model scale, instruction tuning demonstrates consistent', metadata={'page': 33, 'source': 'data_input/llm_survey.pdf'}),\n",
       "  Document(page_content='instruction tuning strategy [352], where LLMs are first fine-\\ntuned with large-scale task-formatted instructions and sub-\\nsequently fine-tuned on daily chat ones. To avoid the capac-\\nity forgetting issue, it is also useful to add an amount of task-\\nformatted instructions at the second stage. Actually, such\\na multi-stage tuning strategy can be also applied to other\\nsettings for instruction tuning. For example, we can sched-\\nule different fine-tuning stages with progressively increased\\nlevels on difficulty and complexity, and gradually improve\\nthe capacities of LLMs to follow complex instructions.\\nOther Practical Tricks. In practice, there are also several\\nuseful strategies and tricks that are helpful to improve the\\nfine-tuning performance of LLMs. We list several represen-\\ntative ones as follows:\\n‚Ä¢Efficient training for multi-turn chat data. Given a multi-\\nturn chat example (the conversation between a user and\\nchatbot), a straightforward fine-tuning way is to split it into', metadata={'page': 33, 'source': 'data_input/llm_survey.pdf'})],\n",
       " 'question': 'What are the tricks to improve the performance of LLM fine-tuning',\n",
       " 'answer': 'Some tricks to improve the performance of LLM fine-tuning include efficient training for multi-turn chat data by splitting conversations into context-response pairs, using compute-efficient allocation of compute resources by increasing data scale with the same budget, and employing distributed training algorithms with optimization frameworks like DeepSpeed and Megatron-LM. These strategies help enhance model capacity, stability, and performance during fine-tuning. Thanks for asking!'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_openai_source.invoke('What are the tricks to improve the performance of LLM fine-tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Summarization using ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada case machine learning yang sederhana, seperti klasifikasi, proses evaluasi model dapat dilakukan dengan mudah dengan membandingkan hasil prediksi dengan label yang asli. Dalam konteks LLM, evaluasi output LLM merupakan tahapan yang challenging. Saat ini, para peneliti masih terus mengembangkan metode untuk bisa mengevaluasi output dari LLM.\n",
    "\n",
    "Salah satu metrics yang bisa kita gunakan untuk mengevaluasi output LLM adalah **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**. Secara sederhana, ROUGE bekerja dengan cara membandingkan kemiripan dari 2 buah teks. Dalam hal ini teks yang dihasilkan oleh AI dan teks referensi yang menjadi standar kita untuk menilai kebaikan output LLM. \n",
    "\n",
    "Misalnya, untuk paper [Transformers in Time Series: A Survey](https://arxiv.org/abs/2202.07125), kita akan coba bandingkan abstract paper dengan summary hasil LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in c:\\users\\asani\\anaconda3\\envs\\dss_pdf_llm\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\asani\\anaconda3\\envs\\dss_pdf_llm\\lib\\site-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# install package untuk ROUGE\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_text = '''\n",
    "Transformers have achieved superior performances\n",
    "in many tasks in natural language processing and\n",
    "computer vision, which also triggered great interest in the time series community. \n",
    "Among multiple advantages of Transformers, the ability to capture\n",
    "long-range dependencies and interactions is especially attractive for time series modeling, leading\n",
    "to exciting progress in various time series applications. \n",
    "In this paper, we systematically review Transformer schemes for time series modeling by\n",
    "highlighting their strengths as well as limitations.\n",
    "In particular, we examine the development of time\n",
    "series Transformers in two perspectives. From the\n",
    "perspective of network structure, we summarize the\n",
    "adaptations and modifcations that have been made\n",
    "to Transformers in order to accommodate the challenges in time series analysis. From the perspective\n",
    "of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classifcation. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to\n",
    "study how Transformers perform in time series. Finally, we discuss and suggest future directions to\n",
    "provide useful research guidance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_generated_summary = '''\n",
    "Transformers have shown significant advancements in various domains,\n",
    "including natural language processing and computer vision. They are\n",
    "now being applied to time series modeling due to their ability to\n",
    "capture long-range dependencies. Various adaptations and modifications\n",
    "have been made to Transformers to address challenges in time series\n",
    "analysis. These modifications include positional encodings, attention\n",
    "modules, and architecture-level innovations. Transformers have been\n",
    "successfully applied to tasks such as forecasting, anomaly detection,\n",
    "and classification in time series data. Future research opportunities\n",
    "include exploring inductive biases, combining Transformers with graph\n",
    "neural networks, developing pre-trained models for time series,\n",
    "designing architecture-level variants, and utilizing neural\n",
    "architecture search for optimal Transformer design.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.30275229357798167, 'p': 0.4125, 'f': 0.3492063443240671},\n",
       "  'rouge-2': {'r': 0.1568627450980392,\n",
       "   'p': 0.2376237623762376,\n",
       "   'f': 0.1889763731623164},\n",
       "  'rouge-l': {'r': 0.28440366972477066,\n",
       "   'p': 0.3875,\n",
       "   'f': 0.32804232316004595}}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mengevaluasi output dengan ROUGE\n",
    "evaluator = Rouge()\n",
    "evaluator.get_scores(llm_generated_summary, abstract_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Catatan tentang Output**:\n",
    "\n",
    "* `rouge-1` menghitung kemiripan berdasarkan kata tunggal (unigram).\n",
    "* `rouge-2` menghitung kemiripan berdasarkan dua kata berurutan (bigram).\n",
    "* `rouge-l` menghitung kemiripan berdasarkan bagian yang lebih panjang yang terdapat pada kedua teks yang dibandingkan.\n",
    "* `r` berarti recall.\n",
    "* `p` berarti precision.\n",
    "* `f` berarti F1-score.\n",
    "* Range nilai untuk recall, precision, dan F1-score adalah 0-1. Semakin mendekati 1, semakin mirip kedua teks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources**:\n",
    "\n",
    "* [How to evaluate a summarization task](https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization).\n",
    "* [LLM evaluation with Rouge](https://medium.com/@MUmarAmanat/llm-evaluation-with-rouge-0ebf6cf2aed4).\n",
    "* [rouge 1.0.1](https://pypi.org/project/rouge/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next\n",
    "\n",
    "* Selain beberapa use case yang sudah kita pelajari di atas, Anda dapat mengeksplor use case lain contoh penggunaan LLM menggunakan LangChain: https://python.langchain.com/v0.1/docs/use_cases.\n",
    "* Anda juga bisa membuat aplikasi berbasis LLM dalam bentuk dashboard yang di-deploy, misalnya melalui Streamlit:\n",
    "    * Contoh dashboard: https://ask-summarize-pdf.streamlit.app/.\n",
    "    * Source code: https://github.com/saskia-dwi-ulfah/ptld5-dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada workshop ini, kita telah mempelajari banyak hal tentang Large Language Models (LLM). Kita memulai dengan memahami dasar-dasar Python yang penting untuk bekerja dengan LLM, termasuk berbagai library dan tools yang digunakan. Kemudian, kita mendalami foundational LLM, mulai dari konsep dasar tentang apa itu LLM, bagaimana ia bekerja, hingga berbagai arsitektur yang digunakan serta mekanisme pelatihannya.\n",
    "\n",
    "Selain itu, kita juga mengeksplorasi penggunaan LLM untuk file PDF, seperti fitur Q&A dan summarization. Ini memungkinkan LLM mengekstrak informasi penting dari dokumen PDF dan memberikan ringkasan yang berguna. Penggunaan ini menunjukkan bagaimana LLM dapat diterapkan dalam berbagai konteks untuk meningkatkan efisiensi dan efektivitas pengolahan data.\n",
    "\n",
    "Selanjutnya, kami mendorong Anda untuk mengeksplorasi lebih jauh berbagai use case lain dari LLM, seperti analisis data, chatbot, dan automasi tugas administratif. Anda juga bisa mempelajari cara men-deploy model ini dalam bentuk dashboard atau aplikasi lain yang dapat digunakan dalam konteks bisnis atau personal. \n",
    "\n",
    "Semoga pengetahuan yang telah kita pelajari dalam workshop ini dapat menjadi dasar yang kuat bagi Anda untuk terus berkembang dan berinovasi dengan LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END OF DAY 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* [Python Tutorial](https://www.w3schools.com/python/).\n",
    "* [The Python Tutorial](https://docs.python.org/3.10/tutorial/index.html).\n",
    "* [LangChain Documentation](https://python.langchain.com/v0.1/docs/get_started/introduction).\n",
    "* [Gemini API Documentation](https://ai.google.dev/gemini-api/docs).\n",
    "* [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
